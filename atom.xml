<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-02-02T07:35:19.847Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>3D点云概念及处理方法</title>
    <link href="http://a-kali.github.io/2020/02/02/3D%E7%82%B9%E4%BA%91%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <id>http://a-kali.github.io/2020/02/02/3D点云概念及处理方法/</id>
    <published>2020-02-02T06:03:11.000Z</published>
    <updated>2020-02-02T07:35:19.847Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p><strong>点云与三维图像的关系：</strong>三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以实现天然的物体-背景解耦。点云数据是最为常见也是最基础的三维模型。点云模型往往由测量直接得到，每个点对应一个测量点。</p><p><strong>点云的概念</strong>：点云是在同一空间参考系下表达目标空间分布和目标表面特性的海量点集合，在获取物体表面每个采样点的空间坐标后，得到的是点的集合，称之为“点云”（Point Cloud）。</p><p><strong>点云的内容：</strong>根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。<br>根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。<br>结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）。</p><p><strong>点云存储格式：*</strong>.pts; <em>.asc ; </em>.dat; .stl ; .imw；.xyz；.las。LAS格式文件已成为LiDAR数据的工业标准格式，LAS文件按每条扫描线排列方式存放数据,包括激光点的三维坐标、多次回波信息、强度信息、扫描角度、分类信息、飞行航带信息、飞行姿态信息、项目信息、GPS信息、数据点颜色信息等。</p><h1 id="点云性质"><a href="#点云性质" class="headerlink" title="点云性质"></a>点云性质</h1><p>点云数据是在欧式空间下的点的一个子集，它具有以下三个特征：</p><ul><li>无序。点云数据是一个集合，对数据的顺序是不敏感的。这就意味这处理点云数据的模型需要对数据的不同排列保持不变性。目前文献中使用的方法包括将无序的数据重排序、用数据的所有排列进行数据增强然后使用RNN模型、用对称函数来保证排列不变性。由于第三种方式的简洁性且容易在模型中实现，论文作者选择使用第三种方式，既使用maxpooling这个对称函数来提取点云数据的特征。</li><li>点与点之间的空间关系。一个物体通常由特定空间内的一定数量的点云构成，也就是说这些点云之间存在空间关系。为了能有效利用这种空间关系，论文作者提出了将局部特征和全局特征进行串联的方式来聚合信息。</li><li>不变性。点云数据所代表的目标对某些空间转换应该具有不变性，如旋转和平移。论文作者提出了在进行特征提取之前，先对点云数据进行对齐的方式来保证不变性。对齐操作是通过训练一个小型的网络来得到转换矩阵，并将之和输入点云数据相乘来实现。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>[1]<a href="https://blog.csdn.net/hongju_tang/article/details/85008888" target="_blank" rel="noopener">点云概念与点云处理</a></li><li>[2]<a href="https://www.jiqizhixin.com/articles/2019-05-10-13" target="_blank" rel="noopener">PointNet系列论文解读</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;点云与三维图像的关系：&lt;/strong&gt;三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以
      
    
    </summary>
    
    
      <category term="3D" scheme="http://a-kali.github.io/tags/3D/"/>
    
      <category term="点云" scheme="http://a-kali.github.io/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（2）—— Figure and table</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94-Figure-and-table/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（2）——-Figure-and-table/</id>
    <published>2020-02-01T12:13:32.000Z</published>
    <updated>2020-02-01T14:40:45.755Z</updated>
    
    <content type="html"><![CDATA[<p>图表可以用来表示</p><ol><li>结果：得出的数据、统计图、实验对比等</li><li>方法：流程图、示意图</li></ol><p>图表的分类：</p><ul><li>投稿时：一般要求，在尺寸、分辨率、色彩等方面有一定要求，能让审稿人看清就行；</li><li>发表时：对图片质量要求高，包括图片文字、数字、字号大小、线条粗细等都有具体的要求</li></ul><h1 id="文字、图片和表格的合理使用"><a href="#文字、图片和表格的合理使用" class="headerlink" title="文字、图片和表格的合理使用"></a>文字、图片和表格的合理使用</h1><p>文字表述：</p><ol><li>描述定量化的结果</li><li>用来描述定性数据之间的关系（e.g. 同比上升/下降）</li></ol><p>图片表述：</p><ol><li>用来展示数据组之间的趋势</li><li>描述指标随时间改变的情况</li><li>定量化因素之间的复杂关系</li><li>实验方法介绍（流程图）</li><li>复杂概念介绍</li><li>可视化数据结果（e.g. 细胞图片）</li></ol><p>表格表述：</p><ol><li>展示大量数值型数据</li><li>不同项目之间的细节比较</li><li>复杂定性结果之间比较</li></ol><h1 id="常见图片类型"><a href="#常见图片类型" class="headerlink" title="常见图片类型"></a>常见图片类型</h1><p>照片、线型图、柱状图、流程图、示意图等</p><h1 id="图片制作过程注意事项"><a href="#图片制作过程注意事项" class="headerlink" title="图片制作过程注意事项"></a>图片制作过程注意事项</h1><ol><li>图标：一个图片对应一个图标。比如说图标a只能对应一个大图中的一个小图。</li><li>标注：箭头、星号；</li><li>缩写：图片标识的时候可以使用缩写，但一定要定义；</li><li>颜色：根据杂志要求选择色彩或灰度模式。</li></ol><h1 id="表格的要素"><a href="#表格的要素" class="headerlink" title="表格的要素"></a>表格的要素</h1><ol><li>数值；</li><li>列标题和列；</li><li>行标题和行；</li><li>注释和说明。</li></ol><h1 id="论文表格的一般要求"><a href="#论文表格的一般要求" class="headerlink" title="论文表格的一般要求"></a>论文表格的一般要求</h1><ul><li>单位：一般情况下，单位标识在行标题的后面；</li><li>对比：通常使用横向比较而不是纵向比较；</li><li>顺序：时间顺序、大小顺序、字母顺序；</li><li>空白数据：可以使用”-“表示，或者相应缩写（e.g. ND, Not Done)。</li></ul><h1 id="表格中的线条"><a href="#表格中的线条" class="headerlink" title="表格中的线条"></a>表格中的线条</h1><ol><li>在<strong>列标题上下</strong>需要一条线条；</li><li><strong>最后一行数据下面</strong>需要一条线条；</li><li>列的<strong>副标题下面</strong>需要线条；</li><li>除了以上三种情况以外，其它的都不能使用线条！</li></ol><p><img src="https://i.loli.net/2020/02/01/YlAVc1ryZEhmwz9.png" alt="90U_Y9_G7_QE_SF_U~_413D.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;图表可以用来表示&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;结果：得出的数据、统计图、实验对比等&lt;/li&gt;
&lt;li&gt;方法：流程图、示意图&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;图表的分类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;投稿时：一般要求，在尺寸、分辨率、色彩等方面有一定要求，能让审稿人看清就行；&lt;/li
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（1）——SCI简介及写作顺序</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94SCI%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%86%99%E4%BD%9C%E9%A1%BA%E5%BA%8F/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（1）——SCI简介及写作顺序/</id>
    <published>2020-02-01T10:30:13.000Z</published>
    <updated>2020-02-01T12:07:17.937Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SCI论文组成"><a href="#1-SCI论文组成" class="headerlink" title="1    SCI论文组成"></a>1    SCI论文组成</h1><ul><li>Title：文章标题</li><li>Abstract：摘要</li><li>Introduction：引言</li><li>Methods：实验所使用的材料和方法</li><li>Results：实验结果</li><li>Discussion：对实验结果的讨论和分析</li><li>Reference：参考文献</li><li>Figures and Tables：图片、图表</li><li>Figure legends：图片、图表说明</li><li>Acknowledgements：致谢</li></ul><h2 id="1-1-Title"><a href="#1-1-Title" class="headerlink" title="1.1    Title"></a>1.1    Title</h2><p>简明、准确地总结文章内容，包含关键词，使文章更容易被所需要的人检索到。</p><h2 id="1-2-Abstract"><a href="#1-2-Abstract" class="headerlink" title="1.2    Abstract"></a>1.2    Abstract</h2><p>通过一段话将你的<strong>研究背景、内容、目的、结果以及研究意义</strong>告诉读者，使读者仅仅通过读Abstract就能明白这篇文献是否是其感兴趣的（或者使读者通过读Abstract就能对你的研究产生兴趣）。是用最少的词表述最重要的内容。</p><h2 id="1-3-Introduction"><a href="#1-3-Introduction" class="headerlink" title="1.3    Introduction"></a>1.3    Introduction</h2><p>Introduction是比较难写的一部分。在Introduction中，作者需要比Abstract更为详细地回答以下五个问题：</p><ol><li><p>为什么——为什么要做这个研究？</p></li><li><p>是什么——你的<strong>科学假说</strong>是什么？</p></li><li><p>做什么——你的<strong>研究方法</strong>是什么？</p></li><li><p>什么结果——你的<strong>研究发现</strong>是什么？</p></li><li><p>什么意义——为什么你的研究很重要？</p></li></ol><h2 id="1-4-Methods"><a href="#1-4-Methods" class="headerlink" title="1.4    Methods"></a>1.4    Methods</h2><p>详细叙述研究采用的方法，能够让读者参考该文献复现出实验研究。</p><h2 id="1-5-Results"><a href="#1-5-Results" class="headerlink" title="1.5     Results"></a>1.5     Results</h2><p>开门见山地告诉读者你发现了什么现象、得出了什么数据和结论。同时在这一部分，作者需要灵活地使用图片、表格等方式更加形象地展示研究成果。</p><h2 id="1-6-Discussion"><a href="#1-6-Discussion" class="headerlink" title="1.6    Discussion"></a>1.6    Discussion</h2><p>Discussion同样是比较难写的一部分。作者需要对自己的研究进行总结和归纳；同时需要讨论该研究与相关研究的关系，其结果是否具有一致性，如何去解释不一致性的产生；最后还要说明自己研究的局限性以及如何去改进这种局限性。提出研究和价值。<strong>Discussion直接决定了整篇文章的质量，对文章能否顺利发表有很大的影响</strong>。</p><h2 id="1-7-Reference"><a href="#1-7-Reference" class="headerlink" title="1.7    Reference"></a>1.7    Reference</h2><ul><li>列举出参考文献，为问题提供背景，为方法提供出处，为结论提供证据。 </li><li>对前人工作和观点的一种尊重和赞同。</li><li>为读者提供更多的信息来源。</li></ul><h1 id="2-SCI论文合理写作顺序"><a href="#2-SCI论文合理写作顺序" class="headerlink" title="2    SCI论文合理写作顺序"></a>2    SCI论文合理写作顺序</h1><ol><li>建议首先制作论文图片和表格，表格和图片就是论文的骨架！完成了图片和表格后，整个文章的基本框架便在心里有数。</li><li>完成表格和图片的说明（Figure Legend），趁热打铁。</li><li>完成论文的Results。一个图片/表格对应一个小结，合起来就是整个Results。</li><li>完成论文的Introduction。对于一件你已经完成的事情，写Introduction来解释你为什么要做这件事情。</li><li>完成Discussion。相当于Introduction的后续，对提出的工作进行总结。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-SCI论文组成&quot;&gt;&lt;a href=&quot;#1-SCI论文组成&quot; class=&quot;headerlink&quot; title=&quot;1    SCI论文组成&quot;&gt;&lt;/a&gt;1    SCI论文组成&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Title：文章标题&lt;/li&gt;
&lt;li&gt;Abstract：摘
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Detecting Depression with AI (AVEC2019)</title>
    <link href="http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/"/>
    <id>http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/</id>
    <published>2020-02-01T02:45:56.000Z</published>
    <updated>2020-02-01T08:17:25.251Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1907.11510v1" target="_blank" rel="noopener">AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression with AI, and Cross-Cultural Affect Recognition</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>视听情感挑战与工作坊(AVEC 2019)“心智状态、人工智能检测抑郁、跨文化情感识别”是第九届比赛，旨在将多媒体处理和机器学习方法用于自动视听健康和情感分析，所有参与者在相同的条件下严格竞争。该挑战的目标是为多模态信息处理提供一个通用的基准测试集，并将健康和情绪识别社区以及视听处理社区集合在一起，以比较现实生活数据中各种健康和情绪识别方法的相对优点。本文介绍了今年的主要创新点、挑战指南、使用的数据和基线系统在三个拟议任务上的表现:心理状态识别、人工智能抑郁评估和跨文化情感感知。</p><h1 id="Depression-Detection-with-AI"><a href="#Depression-Detection-with-AI" class="headerlink" title="Depression Detection with AI"></a>Depression Detection with AI</h1><p>抑郁症，尤其是重度抑郁症( major depressive disorder, MDD)，是一种常见的心理健康问题，对人的思维、感觉和行为方式有负面影响。它会导致各种情绪和身体问题，影响工作和个人生活的许多方面。世界卫生组织(WHO)在2015年宣布抑郁症是全球范围内导致疾病和残疾的主要原因:超过3亿人患有抑郁症。鉴于抑郁症的高患病率及其自杀风险，寻找新的诊断和治疗方法变得越来越重要。由于有令人信服的证据表明抑郁症和相关的精神健康障碍与行为模式的改变有关，人们越来越有兴趣使用自动人类行为分析来基于行为线索(如面部表情和说话韵律)进行计算机辅助抑郁症诊断。面部活动、手势、头部运动和表达能力等行为信号都与抑郁症密切相关。</p><p>计算机视觉可以追踪的面部表情和头部动作也是预测抑郁的好方法。据报道，更向下的凝视角度、不那么强烈的微笑、更短的平均微笑持续时间是抑郁症最显著的面部特征。此外，身体表情、手势、头部动作和语言线索也被报道为抑郁检测提供相关线索。综合所有这些证据，有人提议将情感计算技术集成到一个计算机代理中，该代理可以访问人们并识别精神疾病的语言和非语言指标。对创伤后应激障碍患者收集的数据表明，当代理人由充当WoZ的人驱动时，对其抑郁严重程度的自动评估(PHQ-8问卷)可以实现RMSE小于5；PHQ-8 range∈[0,24] 的cutpoint分别定义为轻度、中度、中度和重度抑郁症。这些结果需要进一步研究，因为代理完全由人工智能驱动，因为向导可能会将虚拟代理驱动到一种情况，从而减轻与抑郁症相关的模式的观察，或者自主代理可能在适当地进行访谈方面存在问题。</p><h1 id="Distress-Analysis-Interview-Corpus"><a href="#Distress-Analysis-Interview-Corpus" class="headerlink" title="Distress Analysis Interview Corpus"></a>Distress Analysis Interview Corpus</h1><p>扩展遇险分析访谈语料库(E-DAIC)是WOZ-DAIC的扩展版本，包含半临床访谈，旨在支持诊断焦虑、抑郁和创伤后应激障碍等心理困扰状况。收集这些访谈是为了创建一个计算机代理来采访人们，并识别精神疾病的语言和非语言指标。收集的数据包括音频和视频记录、使用谷歌云语音识别服务自动转录的文本以及广泛的问卷回答。这些面试是由一个叫做Ellie的动画虚拟面试官进行的。在theWoZ的面试中，虚拟代理由另一个房间的人类面试官(巫师)控制，而在Al的面试中，代理以完全自主的方式使用不同的自动感知和行为生成模块。</p><p>为了达到挑战的目的，E-DAIC数据集被划分为培训、开发和测试集，同时保留了演讲者的整体多样性——在年龄、性别分布和8项患者健康问卷(pho8)评分方面——在这些划分内。训练和开发集包括WoZ和人工智能场景的混合，而测试集仅由自主人工智能收集的数据构成。关于扬声器在分区上的分布的详细信息见表2。</p><p><img src="https://i.loli.net/2020/02/01/teQ2Za9kod7L4M1.png" alt="K_UZ`K8@_ADCD0_THOI_~35.png"></p><h1 id="Baseline-System"><a href="#Baseline-System" class="headerlink" title="Baseline System"></a>Baseline System</h1><p>对于抑郁检测基线，我们使用单层64-d GRU作为我们的递归网络，其失步正规化率为20%，然后使用64-d全连通层获得单值回归评分。为了处理偏差，我们将PHQ-8分数标签转换为浮点数，方法是在培训之前按25的倍数缩小比例。使用CCC损失函数和评价分数对网络进行训练和评价，使用原始的PHQ量表报告RMSE结果。批处理大小为15的方法得到了一致的使用，并且在不同的特性集之间优化了学习率。为了使数据适合GPU内存，为会话分配了最大的序列长度。对于MFCCs和eGeMAPS LLDs，以及诸如DeepSpectrum、ResNet和VGG等高维深表示，使用的最大序列长度为20分钟。另外，对于ResNet、VGG和DEEP SPECTRUM表示帧，根据维数的不同，将保留两帧中的一帧或四帧中的一帧，以便将数据加载到内存中。融合不同的视听表现是通过平均他们的分数来实现的。</p><p>DDS的基线结果见表6。结果表明，在开发集上，利用深度谱(DS-VGG)特征获取音频特征的最佳CCC评分，利用ResNet特征获取视觉特征的最佳CCC评分。这些结果表明表达的力量深层神经网络学习的大量数据时在不同的上下文中使用他们最初的设计,这是证实与ResNet视觉模型在测试集上实现最好的结果,尽管相对较低的CCC。</p><p>不同表现形式的融合在开发集上获得了最好的结果，测试集上返回的RMSE比使用AVEC 2017基线系统在DAIC-WoZ数据集上获得的RMSE稍好一些;AVEC2019年的RMSE=6.37，而AVEC 2017年的RMSE=6.97。然而，为今年的挑战开发的基线系统更加复杂，与今年的GRU-RNNs相比，它是一个简单的线性回归模型，因此，根据2017年AVEC抑郁亚挑战的最佳结果(RMSE=4.99)，应该最好地考虑相应的分数。</p><p>根据从与虚拟代理的交互中获得的抑郁程度自动感知的结果，当代理仅由人工智能驱动时，识别似乎比由人作为WoZ驱动时更具挑战性。这一观察结果为设计抑郁症诱因的设计带来了有趣的研究问题。，通过强化学习，根据agent的交互方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1907.11510v1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AVEC 2019 Workshop and Challenge: State-of-Mind, Detecti
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>胶囊网络（Capsule Network）</title>
    <link href="http://a-kali.github.io/2020/01/11/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%EF%BC%88Capsule-Network%EF%BC%89/"/>
    <id>http://a-kali.github.io/2020/01/11/胶囊网络（Capsule-Network）/</id>
    <published>2020-01-11T12:03:27.000Z</published>
    <updated>2020-01-11T12:03:27.803Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>随机权值平均(Stochastic Weight Averaging,  SWA)</title>
    <link href="http://a-kali.github.io/2020/01/11/%E9%9A%8F%E6%9C%BA%E6%9D%83%E5%80%BC%E5%B9%B3%E5%9D%87-Stochastic-Weight-Averaging-SWA/"/>
    <id>http://a-kali.github.io/2020/01/11/随机权值平均-Stochastic-Weight-Averaging-SWA/</id>
    <published>2020-01-11T01:15:59.000Z</published>
    <updated>2020-01-23T04:57:31.951Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1803.05407" target="_blank" rel="noopener">Averaging Weights Leads to Wider Optima and Better Generalization</a></p><p>SWA是一种较为先进的模型融合方法。传统的模型融合通常使用多个模型进行预测，再使用某种方法来对预测结果取平均值得到最终的预测值。而SWA仅需要训练单个模型即可进行融合。</p><p>SWA一定程度上参考了传统的<strong>快照集成（snapshot ensembling）</strong>，即每训练 20-40 epochs，对局部最优模型保存一个权重快照，然后利用<strong>余弦退火</strong>的特性跳出局部最优，寻找另一个局部最优解；最后使用多个保存的模型权重进行预测并对预测结果进行融合。这种集成方法仅需要训练单个模型就能得出接近多模型融合的效果。</p><p>但快照集成也保留了传统模型融合的部分缺点：需要保存多个模型，较为占用存储空间；训练周期过长，每收敛一个新模型需要经过较多epochs。</p><p>而SWA的作者发现 2-4 epochs <strong>循环学习率</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Averaging Weights Leads to Wider Optima and Better General
      
    
    </summary>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模型融合" scheme="http://a-kali.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>2020年上半年论文阅读计划</title>
    <link href="http://a-kali.github.io/2020/01/06/2020%E5%B9%B4%E4%B8%8A%E5%8D%8A%E5%B9%B4%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/"/>
    <id>http://a-kali.github.io/2020/01/06/2020年上半年论文阅读计划/</id>
    <published>2020-01-06T10:58:49.000Z</published>
    <updated>2020-02-02T07:41:44.376Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。</p><p>目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标跟踪、实例分割、全景分割等），顺便再补一下去年没填的坑（比如LSTM、FPN、YOLOv3、知识蒸馏等）。</p><p><a href="https://arxiv.org/abs/1811.00116" target="_blank" rel="noopener">Face Recognition: From Traditional to Deep Learning Methods</a>：人脸识别综述</p><p><a href="https://arxiv.org/abs/1912.04977" target="_blank" rel="noopener">Advances and Open Problems in Federated Learning</a>：联邦学习综述</p><p><a href="https://arxiv.org/abs/1912.12033" target="_blank" rel="noopener">Deep Learning for 3D Point Clouds: A Survey</a>：3D点云综述</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。&lt;/p&gt;
&lt;p&gt;目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标
      
    
    </summary>
    
      <category term="计划" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="计划" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Bengali.AI Handwritten Grapheme Classification 比赛记录</title>
    <link href="http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/"/>
    <id>http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/</id>
    <published>2020-01-03T14:33:32.000Z</published>
    <updated>2020-01-16T07:02:22.977Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Please enter the password to read the blog." />    <label for="pass">Please enter the password to read the blog.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      The article has been encrypted, please enter your password to view.&lt;br&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="Kaggle" scheme="http://a-kali.github.io/tags/Kaggle/"/>
    
      <category term="比赛记录" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab全家桶（From v1 to v3+）</title>
    <link href="http://a-kali.github.io/2019/12/13/DeepLab%E5%85%A8%E5%AE%B6%E6%A1%B6/"/>
    <id>http://a-kali.github.io/2019/12/13/DeepLab全家桶/</id>
    <published>2019-12-13T15:19:07.000Z</published>
    <updated>2019-12-17T08:59:29.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DeepLabv1"><a href="#DeepLabv1" class="headerlink" title="DeepLabv1"></a>DeepLabv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1412.7062v3" target="_blank" rel="noopener">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>其实挺烦看这种远古论文的，引用的算法现在都不太常见，使用的措辞也和现在不太一样。该论文主要引入了<strong>空洞卷积(Astrous/Dilated Convolution)</strong>和<strong>条件随机场(Conditional Random Field, CRF)</strong>。</p><p>空洞卷积，顾名思义，即是在卷积核权重之间注入空洞，<strong>使用小卷积核的计算量获得大卷积核的感受野</strong>。（如理解有误请邮件指正）</p><p>空洞卷积比传统卷积多一个参数为<strong>采样率(dilation rate)</strong>，表示一个卷积核中采样的间隔。</p><p><img src="https://s2.ax1x.com/2019/12/14/Q2DWex.gif" alt="Q2DWex.gif"></p><p>条件随机场涉及到很多机器学习的知识，学起来比较耗时间，而且在后来的DeepLab版本中被取代，所以此处暂略，有机会再补上。</p><h1 id="DeepLabv2"><a href="#DeepLabv2" class="headerlink" title="DeepLabv2"></a>DeepLabv2</h1><p>论文地址：<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>比起v1，v2的主要改动是增加了<strong>带孔空间金字塔池化(ASPP)</strong>模块，其思想来源于SPPnet。但是文中对ASPP的阐述非常少，完全没有讲清楚ASPP的机制，只能通过论文中的图片和网上的博客来猜。</p><p><img src="https://i.loli.net/2019/12/15/MgRpErQ4utse9ND.png" alt="YM_ISX2ECM53ZW3_4T7HNYJ.png"></p><p><img src="https://i.loli.net/2019/12/15/6S7hpAo3ZiefQBN.png" alt="_H7RQ3P@__TYL_4_87Z0H05.png"></p><p>可以看出，ASPP使用了几种不同采样率的空洞卷积，对一张特征图得出多个分支后，最终concat到一起。我到现在也没搞明白为啥叫“空间金字塔池化”而不是“空间金字塔卷积”。</p><h1 id="DeepLabv3"><a href="#DeepLabv3" class="headerlink" title="DeepLabv3"></a>DeepLabv3</h1><p>论文地址：<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>提出了串联(cascade)和并联(parallel)两种格式，并指出并联效果更好。</p><p><img src="https://i.loli.net/2019/12/16/3PZxML24biRdpGj.png" alt="YF@__L_0M@_0RUKBG_N_O6C.png"></p><p><img src="https://i.loli.net/2019/12/15/tqhbdGpKZIwgf8A.png" alt="_C1PITC8~_S3@U_48_2_L5M.png"></p><p>网络去除了CRF，修改了一些参数，应用了一些新技术（比如批归一化）使模型更加精简。</p><p>虽然从文中看不出做了多少修改，但作者说性能得到了很大的提升。科科。</p><h1 id="DeepLabv3-1"><a href="#DeepLabv3-1" class="headerlink" title="DeepLabv3+"></a>DeepLabv3+</h1><p>论文地址：<a href="https://arxiv.org/abs/1802.02611v1" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>8102年，deeplab终于将Encoder-Decoder结构加进网络里了，之前一直用的双线性插值做上采样。</p><p><img src="https://i.loli.net/2019/12/16/bHvnI59LjUo3JcQ.png" alt="V7U32G_QEI_GOMGI97N6LAG.png"></p><p><img src="https://i.loli.net/2019/12/16/1UiclraR5NuOVvz.png" alt="__ZDD_GOD~3NX9_P_L@HSWA.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://blog.csdn.net/qq_31622015/article/details/90551107" target="_blank" rel="noopener">【语义分割系列：一】DeepLab v1 / v2 论文阅读翻译笔记</a></li><li><a href="https://blog.csdn.net/qq_21997625/article/details/87080576" target="_blank" rel="noopener">语义分割(semantic segmentation)—DeepLabV3之ASPP(Atrous Spatial Pyramid Pooling)代码详解</a></li><li><a href="https://blog.csdn.net/guo_rongxin/article/details/79842895" target="_blank" rel="noopener">deeplab v3论文翻译 Rethinking Atrous Convolution for Semantic Image Segmentation</a></li><li><a href="https://blog.csdn.net/fish_like_apple/article/details/82787705" target="_blank" rel="noopener">Deeplab相关改进的阅读记录（Deeplab V3和Deeplab V3+）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DeepLabv1&quot;&gt;&lt;a href=&quot;#DeepLabv1&quot; class=&quot;headerlink&quot; title=&quot;DeepLabv1&quot;&gt;&lt;/a&gt;DeepLabv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1412.7
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DeepLab" scheme="http://a-kali.github.io/tags/DeepLab/"/>
    
  </entry>
  
  <entry>
    <title>SENet: Squeeze-and-Excitation Networks</title>
    <link href="http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/"/>
    <id>http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/</id>
    <published>2019-12-08T11:24:08.000Z</published>
    <updated>2019-12-17T09:00:21.295Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SENet 是北京 Momenta 公司研发团队提出的网络结构，该团队凭借SENet以极大的优势获得了 ImageNet 2017 竞赛的图像分类任务冠军。该网络至今(2019.12.12)仍然是最强力的分类网络之一。</p><p>我们从卷积网络开始说起。近些年来，卷积神经网络在很多领域上都取得了巨大的突破。而卷积核作为卷积神经网络的核心，通常被看做是在局部感受野上，将<strong>空间上（spatial）的信息和特征维度上（channel-wise）的信息</strong>进行聚合的信息聚合体。卷积神经网络由一系列卷积层、非线性层和下采样层构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述。</p><p>Inception 结构即是从空间上提取特征的典型案例，其使用不同大小的卷积核，聚合多种不同感受野的特征来提升性能。而 SENet 则是从通道特征之间的关系来考虑，采用了一种<strong>特征重标定(feature recalibration)</strong>的策略，即<strong>通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><h2 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h2><p><img src="https://i.loli.net/2019/12/12/SmgX8AJWzBUvR9s.png" alt="I8_M4J____T1XB_FTW_P_H5.png"></p><p>上图是一个 SE 模块的示意图。</p><ul><li><p><strong>Squeeze</strong>：$F_{sq}(·)$ 将特征图进行<strong>全局平均池化(global average pooling)</strong>，使每个二维的特征通道对应生成一个实数，这个实数某种程度上具有全局的感受野。这意味着当 SE 模块用于靠近输入的层时，也能获得全局的感受野，这一点在很多任务中都是非常有用的。</p></li><li><p><strong>Excitation</strong>：$F_{ex}(·,W)$ 是一些<strong>全连接层和激活函数</strong>，其输入为squeeze的结果，<strong>输出为每个特征通道的重要性</strong>。具体分为4层网络：</p><ul><li>FC1：输入长度为 C 的向量，输出长度为 C/16 的向量。</li><li>ReLU：增加非线性</li><li>FC2：输入长度为 C/16 的向量，输出长度为 C 的向量。</li><li>Sigmoid：将输出限制到(0,1)之间，作为每个通道的权重，表示通道的重要性。</li></ul><p>使用两个而不是一个全连接层的好处在于：</p><ol><li>多加了一个ReLU层，具有更多的非线性，可以更好地拟合通道间复杂的相关性</li><li>极大地减少了参数量和计算量（约为原计算量的1/8）</li></ol></li><li><p><strong>Reweight</strong>：通过乘法将 Excitation 得到的通道权重加权到先前的特征上，完成在通道维度上对原始特征的重标定。</p></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者将 SE 模块拟合到了 Inception 和 ResNet 中，对网络性能产生了较大的增益。</p><p><img src="https://i.loli.net/2019/12/12/myFb9gUAhtNZSxV.png" alt="_LW`_DHP74CK_LED0COG_1P.png"></p><p>在理论上 SE 模块仅增加了网络 1% 的计算量；在实验中，由于 GPU 的架构原因，在 GPU 上的运算时间增加了 10%，而在 CPU 上仅增加了 2%。</p><p><img src="https://i.loli.net/2019/12/12/lzBcfFuq27vDGjw.png" alt="28R__T_T_RPEF6@_EIL4_8F.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/wfei101/article/details/79672944" target="_blank" rel="noopener">Face Paper：SeNet论文详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SENet" scheme="http://a-kali.github.io/tags/SENet/"/>
    
  </entry>
  
  <entry>
    <title>轻量级卷积神经网络综述：从SqueezeNet到MixNet</title>
    <link href="http://a-kali.github.io/2019/12/05/%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%EF%BC%9A%E4%BB%8ESqueezeNet%E5%88%B0EfficientNet/"/>
    <id>http://a-kali.github.io/2019/12/05/轻量级卷积神经网络综述：从SqueezeNet到EfficientNet/</id>
    <published>2019-12-05T10:40:16.000Z</published>
    <updated>2020-01-06T11:02:30.316Z</updated>
    
    <content type="html"><![CDATA[<p>在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗电量高则会导致汽车、手机等移动端续航能力变差，而只有轻量级的神经网络能解决这个问题。下面我将介绍近年来轻量级卷积神经网络的发展。</p><h1 id="1-SqueezeNet"><a href="#1-SqueezeNet" class="headerlink" title="1    SqueezeNet"></a>1    SqueezeNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1602.07360v2" target="_blank" rel="noopener">SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</a></p><p>SqueezeNet网络的主要亮点在于提出了<strong>Fire Module</strong>来减少参数量。Fire Module 分为两部分：<strong>Squeeze 和 Expand</strong>。Squeeze层通过 1×1 卷积对特征图进行降维，减少参数量，Expand层分别使用 1×1 和 3×3 卷积对降维后的特征图进行处理后concat到一起。比起直接用3×3卷积，这种方法减少了一定的运算量。</p><p><img src="https://i.loli.net/2019/12/05/8TYQwPWMnCU3ok4.png" alt="(R7V7PD.png"></p><p>整个网络由多个Fire Module堆叠而成，很像GoogLeNet。右边两个网络结构参考了ResNet。</p><p><img src="https://i.loli.net/2019/12/05/rW4ugsYPjLUxXVh.png" alt="`YX7Opng"></p><h1 id="2-MobileNet-v1"><a href="#2-MobileNet-v1" class="headerlink" title="2    MobileNet v1"></a>2    MobileNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p><p>MobileNet由谷歌公司提出，主要用于移动和嵌入式视觉应用，其亮点在于采用<strong>深度可分离卷积(Depth-wise Separable Convolution)</strong> 代替传统卷积。</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>深度可分离卷积分为两步：</p><ol><li><strong>Depthwise convolution</strong>：对特征图各个通道进行卷积，每个卷积核只有一个通道且只负责特征图的一个通道。</li><li><strong>Pointwise convolution</strong>：使用1×1卷积将特征图串起来，得到和普通卷积一样的输出。</li></ol><p><img src="https://i.loli.net/2019/12/05/9bdIvHnKpY3XOG8.png" alt="W8I.png"></p><h2 id="运算量对比"><a href="#运算量对比" class="headerlink" title="运算量对比"></a>运算量对比</h2><p>假设输入图像为12×12×3，输出图像为8×8×256。</p><ul><li>Convolution：<ul><li>卷积核大小 5×5×3，卷积核数量 256</li><li>数据量：5×5×3×256 = 19200</li><li>计算量：仅考虑乘法运算，每产生一个输出值就要进行5×5×3次运算，一共要产生8×8×256个输出值，故 5×5×3×256×8×8 = 1228800。</li></ul></li><li>Depthwise Separable Convolution：<ul><li>Depthwise convolution：卷积核大小 5×5×1，卷积核数量 3</li><li>Pointwise convolution：卷积核大小 1×1×3，卷积核数量 256</li><li>数据量：5×5×1×3+1×1×3×256 = 843</li><li>计算量：5×5×1×3×8×8+1×1×3×8×8×256 = 53952</li></ul></li></ul><h2 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h2><p>左边是传统卷积，右边是深度可分离卷积。</p><p><img src="https://i.loli.net/2019/12/05/K2pm9axhdokJOiR.png" alt="FT.png"></p><h2 id="实验结果对比"><a href="#实验结果对比" class="headerlink" title="实验结果对比"></a>实验结果对比</h2><p>可以看到MobileNet在只牺牲了少量精确度的情况下节约了大量的运算量和网络参数。</p><p><img src="https://i.loli.net/2019/12/05/YwsgB2EQ7cxT1XC.png" alt="O.png"></p><h1 id="3-Xception"><a href="#3-Xception" class="headerlink" title="3    Xception"></a>3    Xception</h1><p>论文地址：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolutions</a></p><p>Xception 借鉴了深度可分离卷积的思想并以此改进了Inception V3。</p><p><img src="https://i.loli.net/2019/12/05/Ht6YxQXpTvRbVA2.png" alt="Fpng"></p><p>图中是一个Xception模块，先用 1×1 卷积改变特征图的通道数，再对输出的每个通道分别进行 3×3 卷积，最后将 3×3 卷积的输出concat到一起。</p><h1 id="4-ShuffleNet-v1"><a href="#4-ShuffleNet-v1" class="headerlink" title="4    ShuffleNet v1"></a>4    ShuffleNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p><p>ShuffleNet是由旷视公司提出的轻量级网络，该网络结构主要使用了<strong>分组卷积(group convolution)</strong>和<strong>通道洗牌(channel shuffle)</strong>。</p><p><img src="https://i.loli.net/2019/12/05/tsFGvMo17V6QuIg.png" alt="MBng"></p><p>图a展示了分组卷积，即将通道均等分为多组，分别进行卷积操作（类似于深度可分离卷积）。但这样会导致组之间的信息不流通，对精度造成影响。于是使用通道洗牌的方式，对各组的通道进行交换。</p><p>下图是两种ShuffleNet单元：</p><p><img src="https://i.loli.net/2019/12/05/iNdkjWleXIDqsFK.png" alt="7EYQpng"></p><h1 id="5-MobileNet-v2"><a href="#5-MobileNet-v2" class="headerlink" title="5    MobileNet v2"></a>5    MobileNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p><h2 id="MobileNet-v1-存在问题"><a href="#MobileNet-v1-存在问题" class="headerlink" title="MobileNet v1 存在问题"></a>MobileNet v1 存在问题</h2><p>MobileNet v1 虽然很好地降低了模型运算量，但依然存在如下两个问题：</p><ol><li>MobileNet v1 的结构是类似于 VGG 的堆叠结构，而这种结构比起后来的 ResNet、GoogLeNet 来说性能不高。</li><li>Depthwise Convolution 的潜在问题：论文作者发现，由于<strong>深度残差卷积产生的特征图通道数较少，在 ReLU 的影响下很容易产生较大的信息损耗</strong>（这个故事告诉我们不要在压缩通道后用ReLU）。</li></ol><h2 id="MobileNet-v2-的创新点"><a href="#MobileNet-v2-的创新点" class="headerlink" title="MobileNet v2 的创新点"></a>MobileNet v2 的创新点</h2><p>为了解决 v1 存在的问题，v2 提出了以下改进方法：</p><ol><li><p><strong>Inverted Residual Block</strong>：首先从名字可以看出，这是从传统残差块演化而来的<strong>逆残差</strong>，两者主要的不同在于对 1×1 卷积的运用方式不同。传统的残差块使用 1×1 卷积降低特征图的通道数，减少 3×3 卷积的运算量；而逆残差则是用 1×1 卷积来提升维度，以便提升网络的准确度。可能作者觉得反正 Depthwise Convolution 运算量也不大，不如就牺牲一丢丢速度来提高一下精度吧。</p><p><img src="https://i.loli.net/2019/12/08/cBeWJVdTSU9t5Eb.png" alt="Q_P`KE8N_V3JY_L``4O6CXS.png"></p></li><li><p><strong>Linear Bottlenecks</strong>：对比 v1 和 v2 的结构可以看出，v2 使用线性函数替换了 v1 模块最后的ReLU6：</p></li></ol><p><img src="https://i.loli.net/2019/12/11/aSftLEykhVcC19I.png" alt="H_T83__0YR5Q6K_Y~V8_M_S.png"></p><h1 id="6-ShuffleNet-v2"><a href="#6-ShuffleNet-v2" class="headerlink" title="6    ShuffleNet v2"></a>6    ShuffleNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1807.11164" target="_blank" rel="noopener">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常用于神经网络的设计指导指标使用的是计算复杂度衡量指标：<strong>FLOPs</strong>，而不是更直接的评价指标：<strong>运行速度</strong>(speed)。而作者发现相同FLOPs的网络速度可能差别很大，认为FLOPs并不能作为网络性能的唯一衡量指标。</p><p>造成FLOPs和速度不成比例的原因：</p><ol><li>部分影响速度的原因没有被FLOPs包含在内：<ul><li><strong>内存访问成本</strong>(memory access cost, <strong>MAC</strong>)：这会使得强大的GPU算力受到限制。</li><li><strong>并行度</strong>(degree of parallelism)：在相同FLOPs的情况下，并行度高的网络模型速度远高于低并行度模型。</li></ul></li><li>不同的运行平台会影响FLOPs。比如说新版的CUDNN专门对 3 × 3 卷积运算进行了优化。</li></ol><p>出于这点考虑，作者提出了两点高效结构设计的指导性原则：</p><ol><li>应当使用直接的评价指标（e.g., 速度）而不是间接的（e.g., FLOPs）。</li><li>应当在规定的平台上进行评估。</li></ol><h2 id="高效卷积网络设计准测"><a href="#高效卷积网络设计准测" class="headerlink" title="高效卷积网络设计准测"></a>高效卷积网络设计准测</h2><ul><li><strong>G1: 当输入、输出channels数目相同时，conv计算所需的MAC最低。</strong>以深度可分离卷积(Depth-wise Separable Convolution)为例，其 pointwise convlution (i.e., 1×1 conv) 部分占用了其大部分复杂度。设$c_1$，$c_2$为 1 × 1 卷积的输入、输出通道数，$h$和$w$为特征图的高和宽，则 FLOPs 计算为 $B=hwc_1c_2$。内存访问操作次数为 $MAC=hw(c_1+c_2)+c_1c_2$。得出下面的不等式，仅当输入输出通道数相同时，MAC最小：<script type="math/tex; mode=display">MAC\geq 2\sqrt{hwB}+\frac{B}{hw}</script></li><li><strong>G2: 过多的分组卷积(Group Convolution)会增大 MAC 开销。</strong>设分组数量为 $g$，从下面公式可以看出随着 $g$ 增加，MAC增加。  ：</li></ul><script type="math/tex; mode=display">MAC=hwc_1+\frac{Bg}{c_1}+\frac{B}{hx}</script><ul><li><strong>G3: 网络碎片化(fragmentation)会减少并发度。</strong>这里的碎片化大概指的是模型的分支数量。比如说 NASNET-A 的分支数就高达13，而 ResNet 的分支数为2或3。作者通过实验证明，分支数量的提升会提高网络的准确率，但也会因降低GPU并行计算能力而影响效率。</li><li><strong>G4: Element-wise 操作的计算量不容忽视。</strong>element-wise包括激活、张量相加、添加偏置等，它们的共同特征就是FLOPS较小但是MAC相对较大。同时作者将 depthwise convolution 操作也算入了element-wise，因为其有着同样高的 MAC/FLOP 比率。</li></ul><p>目前的轻量级网络结构主要是是以FLOPS作为度量标准设计的，而没有考虑以上的几点属性。比如说，ShuffleNet v1使用了过多的分组卷积(与G2违背)、bottleneck-like块(与G1违背)；MobileNet v2使用倒置的bottleneck结构(与G1违背)，同时使用了深度卷积和ReLU在”thick”特征图上(与G4违背)；自动生成结构过多的使用了碎片化结构(与G3违背)</p><h2 id="ShuffleNet-V2-网络结构"><a href="#ShuffleNet-V2-网络结构" class="headerlink" title="ShuffleNet V2 网络结构"></a>ShuffleNet V2 网络结构</h2><p>为了使ShuffleNet更加高效，关键在于保持等宽的出入输出通道，以及使用密集卷积操作而不是过多的分组卷积。</p><p><img src="https://i.loli.net/2019/12/11/3YuOe1yB8zalwjL.png" alt="S9YD9ZB_OHNUS`GHFVS@9D0.png"></p><p>如图，左边两个是 ShuffleNet v1 的模块，右边两个是 ShuffleNet v2 的模块。</p><p>图c是 ShuffleNet v2 的基本模块，其首先将输入的通道随机split成两部分（这是一种变相的分组卷积，不过只分了两个组，遵守了G2和G3），一部分恒等映射到模块尾部，另一部分通过三个输入输出通道数相同的卷积前向传播（遵守了G1），之后使用concat操作（而不是add操作，遵守了G4）将两个分支结合在一起，最后进行通道洗牌(channel shuffle)。</p><p>图d为下采样模块，原理类似，stride=2缩小特征图，没有使用channel split操作，最后两个分支concat到一起使通道数翻倍。</p><p>恒等映射后concat到模块尾部，能使特征得到复用，提高准确度。这种思想来源于DenseNet。</p><h1 id="7-MnasNet（待更新）"><a href="#7-MnasNet（待更新）" class="headerlink" title="7    MnasNet（待更新）"></a>7    MnasNet（待更新）</h1><p>太复杂了，回头再看</p><h1 id="8-MobileNet-v3（待更新）"><a href="#8-MobileNet-v3（待更新）" class="headerlink" title="8    MobileNet v3（待更新）"></a>8    MobileNet v3（待更新）</h1><p>基于MnasNet</p><h1 id="9-MixNet（待更新）"><a href="#9-MixNet（待更新）" class="headerlink" title="9    MixNet（待更新）"></a>9    MixNet（待更新）</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://baijiahao.baidu.com/s?id=1589005428414488177&amp;wfr=spider" target="_blank" rel="noopener">纵览轻量化卷积神经网络：SqueezeNet、MobileNet、ShuffleNet、Xception</a></li><li><a href="https://www.jianshu.com/p/fdd7d7353c55" target="_blank" rel="noopener">SqueezeNet | 轻量级深层神经网络</a></li><li><a href="https://www.greedyai.com/" target="_blank" rel="noopener">贪心学院</a></li><li><a href="https://blog.csdn.net/lk3030/article/details/84847879" target="_blank" rel="noopener">Xception</a></li><li><a href="https://blog.csdn.net/kangdi7547/article/details/81431572" target="_blank" rel="noopener">轻量级模型：MobileNet V2</a></li><li><a href="https://www.jianshu.com/p/71e32918ea0a?utm_source=oschina-app" target="_blank" rel="noopener">精简CNN模型系列之六：ShuffleNet v2</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/81322175" target="_blank" rel="noopener">ShuffleNet v2算法笔记</a></li><li><a href="https://blog.csdn.net/h__ang/article/details/88618089" target="_blank" rel="noopener">ShuffleNet_v2模型解读</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="轻量级网络" scheme="http://a-kali.github.io/tags/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SqueezeNet" scheme="http://a-kali.github.io/tags/SqueezeNet/"/>
    
      <category term="MobileNet" scheme="http://a-kali.github.io/tags/MobileNet/"/>
    
      <category term="Xception" scheme="http://a-kali.github.io/tags/Xception/"/>
    
      <category term="ShuffleNet" scheme="http://a-kali.github.io/tags/ShuffleNet/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
      <category term="深度可分离卷积" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>SSD: Single-shot detectors</title>
    <link href="http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/"/>
    <id>http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/</id>
    <published>2019-12-04T08:49:35.000Z</published>
    <updated>2019-12-17T09:01:49.867Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文提出了一个基于深度神经网络的<strong>单步(single shot)目标检测器SSD</strong>，其在继承了YOLO单步预测高检测速度的同时，拥有不弱于Faster R-CNN的准确度。</p><h1 id="SSD-网络结构"><a href="#SSD-网络结构" class="headerlink" title="SSD 网络结构"></a>SSD 网络结构</h1><p><img src="https://i.loli.net/2019/12/04/qIhrwjU6uRMV4Nn.png" alt="png"></p><p>从图中可以看出：</p><ul><li>不同于YOLOv1和Faster R-CNN，SSD是一个全卷积网络。</li><li>SSD的预测结果并不完全由最后一层输出，而是由其5个<strong>额外特征层(Extra Feature Layers)</strong>和 VGG16中的一层的输出综合而来。</li><li>由于SSD是个全卷积网络，所以其分类操作也由卷积层进行。上图中横向的直线即是<strong>卷积分类器</strong>，卷积核大小为3×3，channel数量为anchors×(Classes+4)。此处anchors指anchor的数量；classes为类别数，预测值为每个类置信度，这点应该会给后面的NMS作为评判标准；+4就是(x,y,w,h)。</li><li>SSD的输出特征图平均每个像素都有一组anchor，整个网络共生成8732个anchor，远多于YOLO和Faster R-CNN。（这里有个问题，根据上面一条，使用3×3卷积核作为滑动窗口是没法做到每个像素都有anchor的，所以此处应该有padding）</li></ul><p><img src="https://i.loli.net/2019/12/04/To4hNmlKez1CrqV.png" alt="2Bng"></p><h1 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h1><p>检测结果中，未被选为最终结果的样本都是负样本。这导致负样本数量远大于正样本，样本不均衡。作者采用<strong>Hard negative mining</strong>的方式，仅选用被误认为是正样本可能性更大的负样本。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>论文中还提到了损失函数和anchor的选择，但跟其它的目标检测网络差不多，就不再赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="SSD" scheme="http://a-kali.github.io/tags/SSD/"/>
    
  </entry>
  
  <entry>
    <title>盘点那些在github上找到的宝藏</title>
    <link href="http://a-kali.github.io/2019/12/04/%E7%9B%98%E7%82%B9%E9%82%A3%E4%BA%9B%E5%9C%A8github%E4%B8%8A%E6%89%BE%E5%88%B0%E7%9A%84%E5%AE%9D%E8%97%8F/"/>
    <id>http://a-kali.github.io/2019/12/04/盘点那些在github上找到的宝藏/</id>
    <published>2019-12-03T16:14:40.000Z</published>
    <updated>2020-01-06T10:33:28.524Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>计算机视觉&amp;深度学习超级Road Map！</strong>：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没有SLAM、自动驾驶、行人检测、对抗样本）的学习路线和各个模型的概述。CVer必备！项目地址：<a href="https://github.com/weslynn/AlphaTree-graphic-deep-neural-network" target="_blank" rel="noopener">https://github.com/weslynn/AlphaTree-graphic-deep-neural-network</a></p></li><li><p><strong>使用Python实现各种算法</strong>：想学算法又不想学C++？重度Python患者的福音，项目地址：<a href="https://github.com/TheAlgorithms/Python" target="_blank" rel="noopener">https://github.com/TheAlgorithms/Python</a></p></li><li><p><strong>深度学习500问</strong>：以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，面试党必备。项目地址：<a href="https://github.com/scutan90/DeepLearning-500-questions" target="_blank" rel="noopener">https://github.com/scutan90/DeepLearning-500-questions</a></p></li><li><p><strong>3DCNN-PyTorch</strong>：PyTorch 3D卷积预训练模型。项目地址：<a href="https://github.com/kenshohara/3D-ResNets-PyTorch" target="_blank" rel="noopener">https://github.com/kenshohara/3D-ResNets-PyTorch</a> 。附腾讯优图的3D医疗影像预训练模型MedicalNet（未来会出2D），亲测效果不错，项目地址：<a href="https://github.com/Tencent/MedicalNet" target="_blank" rel="noopener">https://github.com/Tencent/MedicalNet</a></p></li><li><p><strong>南瓜书</strong>：西瓜书公式推导解析，节约2w根头发。项目地址：<a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">https://github.com/datawhalechina/pumpkin-book</a></p></li><li><p><strong>深度学习面试宝典</strong>：涵盖各大公司ML、CV、NLP、数学、算法、强化学习、SLAM等方向的面试题&amp;解答集合！外加面试技巧和经验！面试党吐血墙裂推荐！项目地址：<a href="https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs" target="_blank" rel="noopener">https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs</a></p></li><li><p><strong>PyTorch预训练模型</strong>：</p><ul><li>主要backbone汇总：<a href="https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet" target="_blank" rel="noopener">https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet</a> 。</li><li>EfficientNet：<a href="https://github.com/lukemelas/EfficientNet-PyTorch" target="_blank" rel="noopener">https://github.com/lukemelas/EfficientNet-PyTorch</a> 。这个预训练模型有个缺点，只能单GPU运行，但这不妨碍EfficientNet牛逼。</li><li>语义分割模型：<a href="https://github.com/qubvel/segmentation_models.pytorch" target="_blank" rel="noopener">https://github.com/qubvel/segmentation_models.pytorch</a> 。主流语义分割模型，可惜没有DeepLab系列。</li><li>DeepLab：<a href="https://github.com/jfzhang95/pytorch-deeplab-xception" target="_blank" rel="noopener">https://github.com/jfzhang95/pytorch-deeplab-xception</a></li></ul></li><li><p><strong>数据增广大全</strong>：涵盖了CV、自然语言、音频方向的各种数据增广图例、调用库和论文。项目地址：<a href="https://github.com/AgaMiko/data-augmentation-review" target="_blank" rel="noopener">https://github.com/AgaMiko/data-augmentation-review</a></p></li><li><p><strong>Chrome实用插件大全</strong>：Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类。项目地址：<a href="https://github.com/zhaoolee/ChromeAppHeroes" target="_blank" rel="noopener">https://github.com/zhaoolee/ChromeAppHeroes</a></p></li><li><p><strong>绘制炫酷的神经网络图</strong>：</p><ul><li><p>使用图形界面绘制：<a href="https://github.com/zfrenchee" target="_blank" rel="noopener">https://github.com/zfrenchee</a></p><p>画图工具体验地址：<a href="http://alexlenail.me/NN-SVG/" target="_blank" rel="noopener">http://alexlenail.me/NN-SVG/</a></p></li><li><p>使用LaTeX绘制：<a href="https://github.com/HarisIqbal88/PlotNeuralNet" target="_blank" rel="noopener">https://github.com/HarisIqbal88/PlotNeuralNet</a></p></li><li><p>使用参数绘制：<a href="https://cbovar.github.io/ConvNetDraw/" target="_blank" rel="noopener">https://cbovar.github.io/ConvNetDraw/</a></p><p><img src="https://i.loli.net/2019/12/15/lY4JZtPcwC6fvUu.png" alt="6KJ9MJ75ZKFMJCB9_Z8_DPQ.png"></p></li><li><p>使用Python绘制：<a href="https://github.com/gwding/draw_convnet" target="_blank" rel="noopener">https://github.com/gwding/draw_convnet</a></p></li><li><p>方便&amp;好看&amp;著名的NetScope：<a href="https://github.com/ethereon/netscope" target="_blank" rel="noopener">https://github.com/ethereon/netscope</a></p></li></ul></li></ul><p>找到其它宝藏的小伙伴可以通过邮箱发给我鸭！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;计算机视觉&amp;amp;深度学习超级Road Map！&lt;/strong&gt;：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="PyTorch" scheme="http://a-kali.github.io/tags/PyTorch/"/>
    
      <category term="github" scheme="http://a-kali.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>常见的聚类算法(K-Means\GMM\DBSCAN)</title>
    <link href="http://a-kali.github.io/2019/12/02/k-means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://a-kali.github.io/2019/12/02/k-means-聚类算法/</id>
    <published>2019-12-02T15:59:48.000Z</published>
    <updated>2019-12-17T09:00:44.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>在<strong>无监督学习(unsupervised learning)</strong>中，训练样本的标记信息是未知的，需要·通过一些算法来揭示这些样本数据中的内在性质和规律。无监督学习通常解决的是<strong>聚类(clustering)</strong>问题。</p><p>聚类算法通常将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个”簇(cluster)”。而每个簇都包含一定的内在关系或者性质。</p><p>如下图是一个为做标记的样本集，通过它们的分布，我们很容易对上图中的样本做出以下几种划分：</p><p>当需要将其划分为两个簇时，即 k=2 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKF1eO.png" alt="QKF1eO.png"></p><p>当需要将其划分为四个簇时，即 k=4 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKFalt.png" alt="QKFalt.png"></p><p>而对这些样本进行划分的就是聚类算法。下面我们将介绍几种常见的聚类算法中的算法。</p><h1 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means 算法"></a>K-means 算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>k-means算法又名 k 均值算法。其算法思想大致为：<strong>先从样本集中随机选取 k 个样本作为簇中心，并计算所有样本与这 k 个簇中心的距离，对于每一个样本，将其划分到与其距离最近的簇中心所在的簇中，对于新的簇计算各个簇的新的簇中心，根据新的簇中心来重新划分簇。重复上述过程，直到所有样本到其簇中心距离之和达到最小。</strong>在普通K-means算法中，k 值通常凭经验和需求、通过多次尝试来选择；度量距离的方法通常采用<strong>欧氏距离</strong>，其它距离测量方法有曼哈顿距离、切比雪夫距离等，在这里不一一赘述。</p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>（待更新）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;headerlink&quot; title=&quot;聚类&quot;&gt;&lt;/a&gt;聚类&lt;/h1&gt;&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt;中，训练样本的标记信息是未知的，需要·通过一些算
      
    
    </summary>
    
      <category term="机器学习" scheme="http://a-kali.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="k-means" scheme="http://a-kali.github.io/tags/k-means/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://a-kali.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类" scheme="http://a-kali.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>部分计算机视觉算法面试题解答</title>
    <link href="http://a-kali.github.io/2019/12/02/%E9%83%A8%E5%88%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98%E8%A7%A3%E7%AD%94/"/>
    <id>http://a-kali.github.io/2019/12/02/部分计算机视觉算法面试题解答/</id>
    <published>2019-12-01T16:59:03.000Z</published>
    <updated>2019-12-01T17:15:59.062Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout是失活神经元还是失活连接</strong></p><p>A：失活神经元并清除失活神经元周围的连接</p><p><a href="https://imgse.com/i/Qm4Aht" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qm4Aht.md.png" alt="Qm4Aht.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout是失活神经元还是失活连接&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A：失活神经元并清除失活神经元周围的连接&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://imgse.com/i/Qm4Aht&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>常见激活函数汇总</title>
    <link href="http://a-kali.github.io/2019/12/01/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/"/>
    <id>http://a-kali.github.io/2019/12/01/激活函数汇总/</id>
    <published>2019-12-01T15:05:09.000Z</published>
    <updated>2019-12-01T16:38:44.814Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h1><p>Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。</p><p>函数式：</p><script type="math/tex; mode=display">σ(x) = \frac{1}{1+e^{-x}}</script><p>导数式：</p><script type="math/tex; mode=display">σ'(x) = σ(x)×(1-σ(x))</script><p>函数图像：</p><p><a href="https://imgse.com/i/QmcKiT" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcKiT.md.png" alt="QmcKiT.md.png"></a></p><h1 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h1><p>ReLU是神经网络激活层最常用的一种函数，因为其运算简单、易于求导，能用最简单的方式实现非线性运算的性质。</p><p>函数式：</p><script type="math/tex; mode=display">f(x)=max(0,x)</script><p>函数图像：</p><p><a href="https://imgse.com/i/QmcLkV" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcLkV.md.png" alt="QmcLkV.md.png"></a></p><h1 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h1><p>常见于递归神经网络。</p><p>函数式：</p><script type="math/tex; mode=display">f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/QmgaBn.png" alt="QmgaBn.png"></p><h1 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h1><p>函数式：</p><script type="math/tex; mode=display">f(x)=\left\{\begin{aligned}x, \quad x\geq0\\ax, \quad x<0\end{aligned}\right.</script><p>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/Qm21bR.png" alt="Qm21bR.png"></p><h1 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h1><p>PReLU(Parametric Rectified Linear Unit) 参数化修正线性单元。其参数随着网络训练而改变。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmRH0A.png" alt="QmRH0A.png"></p><p>参数更新：</p><p><img src="https://s2.ax1x.com/2019/12/02/QmROtP.png" alt="QmROtP.png"></p><h1 id="RReLU"><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h1><p>Random Leaky ReLU，其参数是随机生成的在[0, 1)之间的值。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmWjER.png" alt="QmWjER.png"></p><h1 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h1><p>指数线性单元。右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。ELU的输出均值接近于零，所以收敛速度更快。α为常数。</p><p><a href="https://imgse.com/i/Qmf1bj" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qmf1bj.md.png" alt="Qmf1bj.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sigmoid&quot;&gt;&lt;a href=&quot;#Sigmoid&quot; class=&quot;headerlink&quot; title=&quot;Sigmoid&quot;&gt;&lt;/a&gt;Sigmoid&lt;/h1&gt;&lt;p&gt;Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。&lt;/p&gt;
&lt;p&gt;函数
      
    
    </summary>
    
      <category term="神经网络" scheme="http://a-kali.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="激活函数" scheme="http://a-kali.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv1 - YOLOv3</title>
    <link href="http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/"/>
    <id>http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/</id>
    <published>2019-11-27T09:00:31.000Z</published>
    <updated>2020-01-06T10:57:54.994Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>众所周知，在深度学习目标检测领域有着两个流派，分别是<strong>基于候选区域的R-CNN流派</strong>和<strong>直接回归输出边框的YOLO流派</strong>。R-CNN系列的准确率较高，但即便发展到Faster R-CNN，运算速度也才只有7fps。为了使检测工作更接近实时，作者提出了YOLO结构。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCYCPs.png" alt="QCYCPs.png"></p><h2 id="YOLO-的实现"><a href="#YOLO-的实现" class="headerlink" title="YOLO 的实现"></a>YOLO 的实现</h2><p>（第一次看可能有点复杂，建议拿笔出来边梳理边看）</p><p>YOLO 将输入图像划分为 <strong>S × S 个网格</strong>。如果一个物体的中心点在这个网格中，则该网格负责检测这个物体。每个网格预测 <strong>B 个边框(bounding box)</strong>及其<strong>置信度(confidence)</strong>。其中置信度为<strong>该网格包含目标物体的概率</strong>乘以预测边界框与真实边界框(ground truth)的<strong>交并比(IOU)</strong>，即：</p><script type="math/tex; mode=display">Confidence=Pr(Object)×IOU^{truth}_{pred}</script><p>也就是说，<strong>当置信度为0时，边框内不含有任何目标物，除此之外置信度都等于交并比</strong>。该置信度只是个预测值，受真实的置信度监督。这点可以从后面的损失函数看出来。</p><p>于是我们得知，每个边框由5个预测值组成，分别为$x,y,w,h,confidence$。</p><p>同时每个网格预测一组 <strong>C 个类别的概率</strong> $Pr(Class_i|Object)$，即输出一组长度为 C 的概率向量。这个概率表示网格含有物体的情况下，各个类别属于该网格的概率。</p><p>经过上述步骤，最终在神经网络末端输出一个$S×S×(B×5+C)$的张量。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCIFr6.png" alt="QCIFr6.png"></p><p>测试时，将置信度和每一类概率相乘</p><script type="math/tex; mode=display">Score=Pr(Class_i|Object)×Pr(Object)×IOU^{truth}_{pred}=Pr(Class_i)×IOU^{truth}_{pred}</script><p>得到的Score表示<strong>每一类在每个边框中的置信度(class-specific confidence for each box)</strong>。通过设置阈值筛选出得分高的box，再以的分最高的box为基准进行NMS选出最优结果。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>如图，在 PASCAL VOC 数据集中，图像输入为 448×448，取 S=7，B=2，一共有20 个类别（C=20），则输出就是 7x7x30 的一个 tensor。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCHg76.png" alt="QCHg76.png"></p><p>可以看出这是一个<strong>彻头彻尾的端到端网络</strong>。看到这里可能会有点震惊，上面讲了那么多复杂的设定到头来居然只是个这么朴素的端到端网络？事实上，上面那么多设定大多都是来源于其巧妙设计的损失函数。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://s2.ax1x.com/2019/11/28/QCq2WD.png" alt="QCq2WD.png"></p><p>由于坐标、长宽、置信度的重要性不同，作者给予了他们不同的损失函数和权重。</p><ul><li>重视坐标预测，给这些损失前面赋予更大的权重，取 5。</li><li>对没有 object 的 box 的 confidence loss，赋予较小的损失权重，取 0.5。</li><li>有 object 的 box 的 confidence loss 和类别的 loss 的损失权重取 1。</li><li>对不同大小的边框预测中，相比于大边框，小边框预测偏一点造成的影响更大。而均方误差中对同样的偏移 loss 是一样。为了缓和这个问题，作者用了一个比较取巧的办法，就是将 box 的 width 和 height 取平方根代替原本的 height 和 width。</li></ul><h2 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a>YOLO 的缺点</h2><ul><li>YOLO对比较密集的、小型的物体（如鸟群）检测效果不佳。因为会有两个同类物体出现在同一个网格中的情况。</li><li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱。</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li><li>召回率远低于RCNN系列。</li></ul><h1 id="YOLOv2-amp-YOLO9000"><a href="#YOLOv2-amp-YOLO9000" class="headerlink" title="YOLOv2 &amp; YOLO9000"></a>YOLOv2 &amp; YOLO9000</h1><p>论文地址：<a href="http://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>YOLOv2：在多方面进行改进，在mAP上超过了使用resnet作为backbone的Faster R-CNN 和 SSD，而且速度更快。</li><li>YOLO9000：使用大量分类数据集和检测数据集进行联合训练，能够对9000+类别进行检测。</li></ul><h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><p>YOLOv2 和 YOLOv1 对比：</p><p><img src="https://s2.ax1x.com/2019/11/29/QA2ryq.png" alt="QA2ryq.png"></p><ul><li><strong>增加了 batch norm</strong>。</li><li><strong>使用高分辨率微调模型</strong>：YOLOv1 采用224×224大小的图片进行预训练，但训练检测模型时使用的是448×448，这一变动对模型性能会产生一定影响。而YOLOv2在常规预训练和进行正式训练之间使用了448×448的分类图像样本进行了微调，缓解了分辨率突然切换造成的影响。</li><li><strong>采用了 Anchor Boxes</strong>：借鉴Faster R-CNN的做法使用了锚框，大幅提高了召回率但mAP轻微下降。</li><li>将图片输入尺寸改为416×416，grid改为13×13，<strong>使grid长宽为奇数</strong>，这样能更有效地预测图片中央的目标物（根据经验，目标物在图片中央的可能性较大）。</li><li><strong>使用 k-means 聚类算法来选择锚框</strong>：手工选择的锚框可能对性能产生影响性能。作者使用k-means对训练集目标框进行聚类，以IOU为距离计算指标，即 $d = 1 - IOU$。在对性能和准确率进行衡量之后，选择了 $k = 5$，得出聚类结果的5个聚类中心作为锚框的最终选择（只取锚框的大小和形状，不取锚框的位置）。</li><li><strong>Direct location prediction</strong>：在RPN中的锚框非常不稳定，其公式如下：</li></ul><script type="math/tex; mode=display">x=(t_x*w_a)−x_a\\y=(t_y*h_a)−y_a</script><p>$t_x$和$t_y$为预测值，当$t_x=1$时，预测框相比于原本的锚框将右移一整个锚框的宽度！YOLOv2对这种方法进行了改进：</p><script type="math/tex; mode=display">b_x=σ(t_x)+c_x\\b_y=σ(t_y)+c_y\\b_w=p_we^{t_w}\\b_h=p_he^{t_h}\\Pr(object)*IOU(b,object)=σ(t_o)</script><p>$t_x,t_y,t_w,t_h,t_o$为预测值，被Sigmoid函数限制在(0,1)之间。之后再通过下图的一些运算得到最终box。在<strong>限制了预测值大小</strong>的情况下，模型参数会更容易学习。</p><p><img src="https://i.loli.net/2019/12/03/Pb7ZzNEjQhDtS4c.png" alt="RH.png"></p><ul><li><strong>Fine-Grained Features</strong>：为了能在小型目标上获得更好的效果，作者把浅层高分辨率的特征图叠加到深层低分辨率的图上了，但没详细说明，估计跟U-Net差不多吧。</li><li><strong>多尺度训练</strong>：因为YOLOv2是全卷积，所以能用任意大小的图像作为输入。作者使用了{320,352，…，608}大小的图像进行训练以提高模型的泛化性能。</li></ul><h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><p>大部分网络使用VGG16作为backbone，但是VGG有点臃肿。作者自定义了<strong>Darknet-19</strong>作为网络的backbone。</p><p><img src="https://i.loli.net/2019/12/03/MAWNrVztIbf3CjJ.png" alt="BPV.png"></p><h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><p>这部分讲了下YOLO9000和WordTree，但没有看懂而且好像不是很重要的亚子，跳了。</p><h1 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h1><p>待更新</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/woduoxiangfeiya/article/details/80866155" target="_blank" rel="noopener">YOLOv1论文翻译</a></p><p>[2]<a href="https://blog.csdn.net/guleileo/article/details/80581858" target="_blank" rel="noopener">从YOLOv1到YOLOv3，目标检测的进化之路</a></p><p>[3]<a href="https://www.jianshu.com/p/517a1b344a88" target="_blank" rel="noopener">YOLOv2 / YOLO9000 深入理解</a></p><p>[4]<a href="https://www.jianshu.com/p/b02f64e0d44b" target="_blank" rel="noopener">Yolo系列其二：Yolo_v2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;YOLOv1&quot;&gt;&lt;a href=&quot;#YOLOv1&quot; class=&quot;headerlink&quot; title=&quot;YOLOv1&quot;&gt;&lt;/a&gt;YOLOv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot; target
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>深度学习自动驾驶概述</title>
    <link href="http://a-kali.github.io/2019/11/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2019/11/05/深度学习自动驾驶概述/</id>
    <published>2019-11-05T09:45:08.000Z</published>
    <updated>2019-11-05T09:46:18.720Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目标：</strong>使用端到端的深度学习方法，根据车载摄像头的画面来判断如何<strong>打方向盘和踩油门</strong>。</p><p><img src="https://s2.ax1x.com/2019/11/05/MprAb9.png" alt="MprAb9.png"></p><p>参考论文：End to End Learning for Self-Driving Cars</p><p><strong>收集数据：</strong></p><p><img src="https://s2.ax1x.com/2019/11/05/MpBqT1.png" alt="MpBqT1.png"></p><p>汽车人为行驶时，其左中右三个摄像头、方向盘转向、油门、转向灯等数据都会通过其 CAN bus 传入处理器。而如今的汽车中基本都带有上述传感器帮忙训练神经网络；当汽车自动驾驶时，汽车根据中间摄像头传入的数据来操控方向盘等设备。</p><p><img src="https://s2.ax1x.com/2019/11/05/Mpye1K.png" alt="Mpye1K.png"></p><p><strong>自动驾驶模拟器：</strong></p><p><a href="https://imgchr.com/i/Mp6LR0" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/05/Mp6LR0.md.png" alt="Mp6LR0.md.png"></a></p><p>看起来很好玩的样子，有空看看源码了解下神经网络输出如何操控这些游戏。</p><p><strong>图像处理：</strong></p><ul><li>亮度调整（适应白天、晚上、阴天、晴天等情景）</li><li>归一化</li><li>图像切割（去除地平线以上和车头部分的无关紧要的数据）</li><li>水平翻转（左转右转）</li><li>数据平衡（欠采样、过采样、给样本少的数据更大权重、合成新数据）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;目标：&lt;/strong&gt;使用端到端的深度学习方法，根据车载摄像头的画面来判断如何&lt;strong&gt;打方向盘和踩油门&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/11/05/MprAb9.png&quot; al
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="概述" scheme="http://a-kali.github.io/tags/%E6%A6%82%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle days China, Oct 2019</title>
    <link href="http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/"/>
    <id>http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/</id>
    <published>2019-10-30T12:17:30.000Z</published>
    <updated>2019-11-09T15:52:47.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Winning-competitions-with-deep-learning-skills-—-SeuTao"><a href="#Winning-competitions-with-deep-learning-skills-—-SeuTao" class="headerlink" title="Winning competitions with deep learning skills — SeuTao"></a>Winning competitions with deep learning skills — SeuTao</h1><p><img src="https://s2.ax1x.com/2019/11/09/Mnu7rQ.png" alt="Mnu7rQ.png"></p><h2 id="Prepare-for-a-DL-competition"><a href="#Prepare-for-a-DL-competition" class="headerlink" title="Prepare for a DL competition"></a>Prepare for a DL competition</h2><ul><li>GPUs 是基础&amp;必要条件，但不是获得金牌的决定性条件。有着9块金牌的涛神在2019年也才只有2块1080ti而已。</li><li>多读 paper 是获得 idea 的关键，在很多 paper 中能找到相似问题的解决方案。</li><li>多读别人的代码。</li></ul><h2 id="Five-steps-to-Win-a-DL-competition"><a href="#Five-steps-to-Win-a-DL-competition" class="headerlink" title="Five steps to Win a DL competition"></a>Five steps to Win a DL competition</h2><ul><li>Understand the data</li><li>Build a strong baseline</li><li>Find the tricks</li><li>Ensemble</li><li>Pseudo-labels</li></ul><h3 id="Build-a-strong-baseline"><a href="#Build-a-strong-baseline" class="headerlink" title="Build a strong baseline"></a>Build a strong baseline</h3><ul><li>据涛神的看法，建立一个 <strong>strong baseline</strong> 是整个比赛中最重要的一环。一个高质量的 baseline 可以直接让你拿到<strong>银牌</strong>甚至 top15。可以建立一个高质量的 pipeline 并重复利用。</li><li>不要使用花里胡哨的神经网络架构和损失函数。这里大概可以理解为，baseline应使用简单轻量的神经网络，便于快速训练、调参、尝试 tricks。</li><li><strong>优化器</strong>：动量梯度下降或者 lr(3e-4) Adam优化器。优化器的改变对网络性能提升不大。</li><li><strong>学习率</strong>：可以尝试 warm up 和 余弦退火/cyclic lr</li><li>找到对数据合适的<strong>数据增强</strong>。</li><li>可靠的<strong>本地验证</strong>。在kaggle上提交验证相对麻烦而且有次数限制，而有一个可靠的本地验证就能快速地尝试验证各种 tricks。</li><li><strong>BatchNorm</strong>问题，基线很难高分的一个原因，涉及到神经网络细节。这里没看懂先挂张图：<img src="https://s2.ax1x.com/2019/11/09/Mn3W5j.png" alt="Mn3W5j.png"></li></ul><h3 id="Find-the-tricks"><a href="#Find-the-tricks" class="headerlink" title="Find the tricks"></a>Find the tricks</h3><ul><li>任务型 trick：图片分类trick、目标检测trick等。这些trick需要大量相关论文的积累。</li><li>数据型 trick：这需要你对数据敏锐的分析。数据相关的trick往往是制胜的关键。</li></ul><h3 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h3><p>融合技巧很重要，比如stacking、blending等</p><p><img src="https://s2.ax1x.com/2019/11/09/Mn82y6.png" alt="Mn82y6.png"></p><h3 id="Pseudo-labels"><a href="#Pseudo-labels" class="headerlink" title="Pseudo labels"></a>Pseudo labels</h3><ul><li>易于使用而且几乎在所有的深度学习竞赛中都奏效。</li><li>可以通过测试集或者外部数据来生成伪标签。</li><li>在比赛的最后stage使用——Overfit the LB then create pseudo labels（这个有点难理解）</li><li>注意不要 overfit 伪标签</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li>实验效率很重要，总结每一次实验经验，不管是成功还是失败。</li><li>在 kernel only 这种限制测试时间的比赛上，可以使用模型蒸馏、加速。</li><li>找到任务实质相关的论文。</li><li>熟读计算机视觉各个分支的论文，很可能会在之前读过的相关的论文上找到thick。</li></ul><h1 id="Tricks-of-image-classification-—-Jun-Lan"><a href="#Tricks-of-image-classification-—-Jun-Lan" class="headerlink" title="Tricks of image classification — Jun Lan"></a>Tricks of image classification — Jun Lan</h1><ul><li><p>图像分类大致可以分为两种：多类别分类（一个样本属于一个类别） vs 多标签分类（一个样本属于多个类别）</p></li><li><p>找到之前相似的比赛，观察高分solution</p></li><li><p>将数据增强后的图片可视化查看效果，根据任务选择增强方法</p></li><li><p>医疗影像预训练数据：MedicalNet。目前没开源2d数据</p></li><li><p>cycle learning rate：减少调参，更快收敛</p></li><li><p>多类别：交叉熵损失；多标签：二值交叉熵损失</p></li><li><p>mixup：一种数据增强的方法。将两张图片及其标签按一定比例进行融合</p></li><li><p>apex：基于pytorch的低精度运算（32位或16位）。减少显存占用，增加训练速度。pure float可能会导致精度损失和溢出。解决方案：混合精度训练。（设成O1就行了）</p></li><li><p>梯度累加（batch accumulation）：增大batch的方法，（多累积几步再更新梯度？）</p></li><li><p>伪标签：数据少或有大量额外数据且没有标签的情况下</p><ol><li>训练集训练模型</li><li>测试数据</li><li>将置信度较高的数据放入训练集（0.95、0，98）</li><li>再训练</li></ol></li><li><p>数据蒸馏（knowledge distillation）：使用小模型（student）来获取大模型（teacher）中的核心知识</p><ol><li>将数据集分为k折</li><li>k折交叉验证训练teacher model</li><li>预测out-of-fold的标签</li><li>在out-of-fold训练student model</li></ol></li></ul><hr><h1 id="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"><a href="#半年5战5金：Kaggle史上最快GrandMaster是如何炼成的" class="headerlink" title="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"></a>半年5战5金：Kaggle史上最快GrandMaster是如何炼成的</h1><p>下面内容跟 kaggle days 没什么关系，是一些很有用的 tricks。整理自网络，有删改，原文地址：<a href="https://zhuanlan.zhihu.com/p/89476481" target="_blank" rel="noopener">Kaggle你问我答【1】——SeuTao</a></p><p>这是 Kaggle 你问我答 (AMA) 的第一期活动，本期请到的嘉宾是 SueTao，他研究生毕业于东南大学，目前是腾讯的一名算法工程师。SueTao 擅长计算机视觉（Computer Vision），半年 5 战 5 金，也许是史上最快的 GrandMaster。截至目前共斩获 9 金 3 银，kaggle 最高排名全球第 10。</p><p>以下是本期活动的问答集锦：</p><p><strong>Q1：如何搭建kaggle data pipeline?</strong></p><p>A1：我目前的比赛还是集中在cv，也做过语音，还有前段时候的PMP，都是DL相关的竞赛。 数据的pipeline其实是可以积累并且优化的。我觉得可以参考一些前人的代码，尤其是蛙神的code。 可以在蛙神的code基础上，慢慢优化跟积累出自己的数据pipeline。 DL数据pipeline中还有个很重要的部分就是数据增强，这块针对不同比赛可能有不同的做法。</p><p><strong>Q2：自己曾经努力拿过银牌，但是觉得金牌好难，特别是solo的情况，请问金牌和银牌的差距在哪里，如何突破？</strong></p><p>A2：我还是从我参与比较多的cv竞赛角度出发哈。首先，如果你是cv新人，在kaggle竞赛上觉得拿金牌很困难，其实是很正常的。目前cv赛基本被cv高手霸榜了。 如果你是已经比较熟悉cv各个方向的模型，那你可能需要一个竞赛好手来给你带路。毕竟竞赛还是有很多套路的。 如果是新人，我的建议是坚持，通过几个cv竞赛来积累对这个方向的认识。了解不同模型不同任务。 我觉得可以参考padue，大家如果看他竞赛的成绩的话，开始他也只是银牌水平，但是从前段时间的protein开始，他现在在cv赛的水平基本就是solo gold了。 deep learning实践的积累还是很重要，一口吃不成胖子。</p><p><strong>Q4：新出的3d object比赛是不是一种趋势，请问涛神对computer vision的发展有什么观察和展望？</strong></p><p>A4：cv的话3d绝对是一个趋势，包括学术界和工业界； sensor的成本越来越低，性能也越来越好；就人脸识别来说，用3d来说安全性和可靠性就更高了。 其实我目前也算是退坑computer vision了，也谈不上对cv有深入的认识。大家从kaggle上cv赛的数量上可以发现，cv对企业的价值还是非常高的。前景是非常好，例如工业检测之类的。</p><p><strong>Q5：怎么判断该改进网络结构还是调学习率？</strong></p><p>A5：学习率和学习策略可能是搭建baseline里面最重要的部分。这块需要在比赛的前期优化到最好，建议使用简单的网络作为baseline，然后仔细优化学习策略。没有提升空间之后再考虑别的方向的优化。</p><p><strong>Q6：是否应该从分类错误的sample中提取灵感继续改进？如果是该怎么做？</strong></p><p>A6：cv最好的一点是可以看图，非常直观。举个例子：比如之前的鲸鱼竞赛，baseline模型的bad case大多是一些姿态较大，分辨率较差的图像。那么我们就可以考虑增加对应的数据增强。效果也很显著。 再举个反面例子：刚刚结束的nips的cellsignal竞赛，是细胞的荧光成像。整个比赛我完全没有看bad case。 因为没有domain知识，图像非自然，很难观察。 但是也不妨碍比赛能拿名次，只看log来调参。</p><p><strong>Q7：请评价cv 各项任务中 state of the art 模型的实用性，有何推荐？</strong></p><p>A7：“试过才有发言权”，这是我做kaggle之后的一个经验。没做kaggle之前，我工作集中在轻量级的模型，对于sota的大模型几乎没有尝试。所以我在竞赛中会尽量去尝试各种sota，最终会有很多有意思的结论。 会发现kaiming的resnet为什么强，unet为什么就是好用。 有些很fancy的模型真的只是过拟合特定的数据集。 我也没有尝试过所有的sota，但是我觉得paper里的内容看看就好，去伪存真，实践出真知。</p><p><strong>Q8：作为一个新人从头开始拿到金牌的最佳策略？比如选择比赛的类型？</strong></p><p>A8：哈哈 因为我cv一把梭，只能给到cv的经验。如果新人想拿金牌的话，最好就是找一个蛙神all in的比赛，step by step follow蛙神！只要比所有人都肝，有足够计算资源，对齐discussion report出来的模型精度，solo gold就有希望！ 其实我第一个比赛TGS就是这么做的。</p><p><strong>Q9：在kaggle学到的东西是否有应用到别的地方？能否举例说明？</strong></p><p>A9：非常多。举个例子：模型集成（ensemble）。可能有些人说模型集成在实际工作中用不了；工作中的场景有效率的要求；在计算资源受限的情况下，3个小模型集成的效果可能远好于1个大模型的效果。 我之前的参与的人脸项目，其实就用了这样的策略，很好用。但是如何去集成，怎么增大模型间diversity，这些技巧大家可以从kaggle上学习。</p><p><strong>Q10：回头看自己的经历，对刚入坑的新人，有什么想提醒的经验和教训？</strong></p><p>A10：教训到没有，做比赛一年感触还是蛮多的，投入越多收获越大吧。希望大家坚持。 真的只有投入去做了，才会有收获。</p><p><strong>Q11：CV比赛假如遇到瓶颈会往哪些方向尝试？</strong></p><p>A11：数据层面绝对是提分收益最大的方向；还是要多看数据，多分析bad case；不看数据就调网络结构是不可取的。 数据层面有些线索之后，可以指导你对模型结构本身做一些改进。另外最重要的：多看paper，paper是idea的来源。</p><p><strong>Q12：一般会用哪种方式平时积累知识？</strong></p><p>A12：过去很长一段时间内，我积累的方式还是来自比赛 通过一个比赛，我可以验证很多paper的方法，实践在工作中无法使用的模型；帮助我深入理解一些数据上和模型上的问题 感觉从我个人而言，比赛和工作相辅相成，给我工作提供了非常好的积累和储备。</p><p><strong>Q13：想知道打比赛的节奏是什么， 比如比赛结束前一个月， 一周， 几天主要干什么？</strong></p><p>A13：基本上最后一周前，最终方案就要定了。考虑最终的集成。</p><p><strong>Q14：有复现比赛top solution的习惯吗？ 有的话是一种怎样的方式呢？</strong></p><p>A14：会看，但是很少会跑。因为一直忙着做新的比赛。其实应该仔细去研究下的。</p><p><strong>Q15：分类比赛中的最后的sub的阈值应该根据什么来选取呢，有什么选取技巧呢？</strong></p><p>A15：我只能说可靠的local validation是最重要的，所有涉及模型选择，调参；其实都需要一个依据，local validation就是这个依据。这样问题就变成如何建立可靠的local validation了。</p><p><strong>Q16：分类比赛中最后的两个sub一般会怎么样选择呢，不同的方案的模型，还是其他？</strong></p><p>A16：这个问题比较好。前期几个比赛的sub一般都是我选的，有幸抽中过金牌。我个人的建议是，差异一定要大，一个激进一个保守。 就dl比赛来说，集成最稳的是weight ave，简单有效，一般来说我会选一个这个； 然后一些存在过拟合风险的方法，但是lb和cv都很可观的方案，我也会选择一个。</p><p><strong>Q17：请问经常看到各位大佬同时参加好几个比赛，还能拿到很好的名次，这是怎么做到的？</strong></p><p>A17：其实kaggle上的top CVer都会有自己积累下来的pipeline。竞赛任务无非是这几种，迅速搭建一个可靠的baseline，对top选手很容易； 看似在做多个竞赛，可能跑的是一套代码。真的要最终比赛冲刺了，会有针对性地去理解数据和优化。</p><p><strong>Q18：图像比赛有什么通用的技巧吗？厉害的选手一次提交就可以进到绿圈，细节处理上有什么独到之处？</strong></p><p>A18：DL调参的细节太多了，需要很长时间的积累。同样的数据+网络，不同人的训练结果可能相差巨大。这是top CVer的核心竞争力 通用技巧的话，paper上带着“bag of tricks”的都需要仔细阅读 bag of tricks for image classification， bag of tricks for object detection。</p><p><strong>Q19：想问下之前说没法做bad case的时候通过log调参是怎么调的， 另外一般bad case怎么样比较好的分析？</strong></p><p>A19：其实很简单: bias-variance trade off，只看log的话，拿捏好这个。 比如nips cellsignal比赛，baseline效果是，training拟合的非常好，test却非常差。其实是一种train test consistency。从1）数据层面；2）网络层面，去分析可能的情况。1）数据层面:数据分布的问题，2）网络层面：batchnorm。针对性地去做实验，确定问题所在，继续观察bias-variance，要得出可靠结论，再进行下一步。</p><p><strong>Q20：我这边自己写了个基于 pytorch 的轮子, 每次基本上能跟上 public kernel 的步伐, 但是就是很难超越. 我估计是训练资源和调参问题. 那么: 调参大部分用已经训练好的模型来调, 还是每次改变参数都重新训练个几天, 哪种方法对 top CVer 比较实际?</strong></p><p>A20：建议解决计算资源问题，保证快速学习，训练资源很重要，其实最优的实验周期我个人感觉在半天。 半天能出一个实验结果最好，中间可以干别的。 结果出得太快也不好，要及时总结和记录实验。</p><p><strong>Q21：之前看到有新闻说模型会用贴纸识别面包机，用肤色识别罪犯的这种过拟合的情况，还有aptos存在模型通过图片尺寸leak发现lable，有没有什么好办法避免这种情况？</strong></p><p>A21：我感觉过拟合问题其实比大家想象的更严重，之前做活体检测基本就是这么个情况，难以范化。 目前的DL还比较‘蠢’，要说办法的话，加数据算不算？</p><p><strong>Q22：问一个技术性问题，碰到一些受阈值影响的metrics时，训练的时候取最好的模型应该依据val-metrics还是val-loss呢？valid的时候如果遍历阈值，可能会极大的影响效率。不同模型/不同epoch，用不同阈值取得的metrics比较，会不会‘不公平’？</strong></p><p>A22：其实我也没有很好的答案。是我的话，最优的val-metrics和val-loss模型我都会存。其实最担心的是优化的loss和metrics不一致。</p><p><strong>Q23：还想问下对warmRestart这类的循环式的scheduler有什么看法？和传统的ReduceLROnPlateau相比有什么优劣？</strong></p><p>A23：最近发现这个真的很好用。如果用step LR的话，很可能下降的位置就不够好。循环的学习策略，我的感受是既不会有太多过拟合，也不需要很仔细调参，基本会有个不错的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Winning-competitions-with-deep-learning-skills-—-SeuTao&quot;&gt;&lt;a href=&quot;#Winning-competitions-with-deep-learning-skills-—-SeuTao&quot; class=&quot;h
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="kaggle days" scheme="http://a-kali.github.io/tags/kaggle-days/"/>
    
      <category term="kaggle" scheme="http://a-kali.github.io/tags/kaggle/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
      <category term="比赛技巧" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E6%8A%80%E5%B7%A7/"/>
    
      <category term="优化器" scheme="http://a-kali.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
      <category term="学习率" scheme="http://a-kali.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
      <category term="数据蒸馏" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E8%92%B8%E9%A6%8F/"/>
    
      <category term="伪标签" scheme="http://a-kali.github.io/tags/%E4%BC%AA%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>深度学习语义分割初期（FCN、UNet、SegNet）</title>
    <link href="http://a-kali.github.io/2019/10/26/FCN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://a-kali.github.io/2019/10/26/FCN论文解读/</id>
    <published>2019-10-26T02:06:38.000Z</published>
    <updated>2019-12-17T09:02:42.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p>论文地址：<a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>原始的 CNN 在图像的分类和定位任务中都获得了不错的成绩，但在分割任务中表现不佳。本文提出了一种<strong>全卷积网络(Fully Convolution Network, FCN)</strong>，通过进行像素级的预测(pixelwise prediction)来实现<strong>语义分割(semantic segmentaion)</strong>。</p><p><a href="https://imgchr.com/i/MUdneI" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/15/MUdneI.md.png" alt="MUdneI.md.png"></a></p><p>实现全卷积网络主要基于三种技术：</p><ul><li>全卷积化（Fully Convolutional）</li><li>反卷积（Deconvolution）</li><li>跃层结构（Skip Layer）</li></ul><h2 id="全卷积化"><a href="#全卷积化" class="headerlink" title="全卷积化"></a>全卷积化</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUB4hV.png" alt="MUB4hV.png"></p><p>简单来说就是把传统CNN最后的全连接层换成了卷积层。全卷积在多篇目标检测的论文中都有提到，其能提取出样本的特征图，样本目标区域对应特征图的感兴趣区域所在位置（如上图中的猫对应heatmap中的彩色像素）。</p><h2 id="上采样（Upsampling）"><a href="#上采样（Upsampling）" class="headerlink" title="上采样（Upsampling）"></a>上采样（Upsampling）</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUBJ6e.png" alt="MUBJ6e.png"></p><p>图像(图a)在经过卷积、池化等一系列处理后，得到的特征图(图b)分辨率远小于原图像。这样一来特征图中的像素无法与原图中一一对应，无法对每个像素进行预测。于是需要对特征图进行<strong>上采样</strong>以提高特征图的分辨率。文中对比了三种上采样的方法，最终选择了<strong>反卷积</strong>。</p><h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>反卷积是文章作者最终采用的方法，下面是两种反卷积的示例，图解起来十分直观：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBwkt.gif" alt="MUBwkt.gif"></p><p><img src="https://s2.ax1x.com/2019/11/15/MUBBff.gif" alt="MUBBff.gif"></p><p>下面是另一种解释，这样一看好像确实是把卷积的操作反过来了：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUB20s.png" alt="MUB20s.png"></p><h2 id="跃层结构-Skip-Layer"><a href="#跃层结构-Skip-Layer" class="headerlink" title="跃层结构(Skip Layer)"></a>跃层结构(Skip Layer)</h2><p>FCN 通过卷积和反卷积我们基本能定位到目标区域，但是，我们会发现模型前期是通过卷积、池化、非线性激活函数等作用输出了特征权重图像，我们经过反卷积等操作输出的图像实际是很粗糙的，毕竟丢了很多细节。因此我们需要找到一种方式填补丢失的细节数据，所以就有了<strong>跃层结构</strong>。</p><p>跃层结构将浅层的位置信息和深层的语义信息结合起来，得到更佳鲁棒的结果，其过程如图：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBz9K.png" alt="MUBz9K.png"></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUDAAI.png" alt="MUDAAI.png"></p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>训练过程分为四个阶段，也体现了作者的设计思路，值得研究。</p><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdWsPg.png" alt="MdWsPg.png"></p><p>使用数据集对模型的分类backbone进行预训练，使卷积层获得提取相应特征的能力。最后两层红色的是全连接层。</p><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdW5IU.png" alt="MdW5IU.png"></p><p> <strong>从特征小图（16×16×4096）预测分割小图（16×16×21），之后直接升采样为大图（300×300×21）。</strong>这里输出通道数为21的原因是：采用的PASCAL数据集中有20类，算上背景类一共21类。每个通道预测一类的像素。反卷积（橙色）的步长为32，故该网络被称为<strong>FCN-32s</strong>。</p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdosiV.png" alt="MdosiV.png"></p><p>这个阶段上采样分为两次完成（橙色×2）。 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）通过跃层结构融合进来，提升精确性。 第二次反卷积步长为16，这个网络称为<strong>FCN-16s</strong>。 </p><h3 id="第四阶段"><a href="#第四阶段" class="headerlink" title="第四阶段"></a>第四阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdTPSS.png" alt="MdTPSS.png"></p><p>这个阶段和第三阶段差不多，相较多了一次上采样。这大概是最终得出的FCN模型，因为同样的原因被称为<strong>FCN-8s</strong>。</p><p>比较这几个阶段的输出可以看出，跃层结构利用浅层信息辅助逐步升采样，有更精细的结果。 </p><p><img src="https://s2.ax1x.com/2019/11/15/MdTHkq.png" alt="MdTHkq.png"></p><h2 id="FCN-的缺点"><a href="#FCN-的缺点" class="headerlink" title="FCN 的缺点"></a>FCN 的缺点</h2><ol><li>分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节。</li><li>因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系。</li></ol><h1 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h1><p>论文地址：<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p><p><strong>U-Net</strong>是医学图像领域十分常用的一种分割网络，因为跟FCN十分相似，就放这里顺便讲了。</p><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/18/MyR5wj.png" alt="MyR5wj.png"></p><p>由于整个结构图呈”U”字型，故名”U-Net”。在知道FCN的原理后，从图中可以很明显地看出U-Net的结构和FCN没太大区别。其主要区别于以下几点：</p><ul><li><p>由于Unet的主要目标数据集为医学影像（最开始是细胞图像），只需要对每个像素点进行二值分割（有病/没病），故输出的特征图只有2个channel。(output segmentation: 388×388×2)</p></li><li><p>在上采样部分依然有大量的特征通道，使得网络可以将环境信息向更高的分辨率层传播。下采样和上采样部分几乎是对称的。</p></li><li><p>输入图像尺寸(572×572)和输出图像尺寸(388×388)不一样。这点似乎是为了配合一种名为<strong>overlap-tile</strong>的方法。如下图，使用左图蓝色区域预测右图黄色区域，滑动蓝色区域重复此操作直到预测完整张图片（这种细胞图尺寸通常都很大）。最终会导致最边上的蓝色区域没法预测，对于这部分使用<strong>镜像法(mirroring)</strong>外推。</p><p>注：关于这部分我也不太确定，想要了解详细原理可以去官网看原版的实现代码。</p><p><img src="https://s2.ax1x.com/2019/11/18/MyqBmq.png" alt="MyqBmq.png"></p></li><li><p>浅层特征和深层特征合并时，Unet使用的是拼接方法（图中白色模块，估计是为了保留更多的channel），而FCN使用的是求和。</p></li><li><p>用少量图像训练便能取得不错的效果，这点对医学领域图像数据集较少的特性十分友好。</p></li></ul><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p><p>说实话这篇文章没啥意思，就概括地科普一下吧。</p><p>首先架构还是和FCN一样，没啥变化，但文中将网络前面提取特征的部分称为<strong>编码器(Encoder)</strong>，后面上采样的部分称为<strong>解码器(Decoder)</strong>。这组词被沿用至今，可能就是在这里提出来的。</p><p><img src="https://i.loli.net/2019/12/04/OTcs6yDtWQJRoup.png" alt="U61.png"></p><p>然后整篇文章的亮点在于：解码器通过使用从相应的编码器接受的<strong>max-pooling索引</strong>来进行非线性上采样。这种方法<strong>减少了所需要训练的参数量，并且改善了边界划分效果</strong>。</p><p><img src="https://i.loli.net/2019/12/04/mPTJNcjrvlVC6qM.png" alt="E5QZ.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/fate_fjh/article/details/52882134" target="_blank" rel="noopener">卷积神经网络CNN（1）——图像卷积与反卷积（后卷积，转置卷积）</a></p><p>[2]<a href="https://blog.csdn.net/qq_31347869/article/details/89429211" target="_blank" rel="noopener">【论文笔记】FCN</a></p><p>[3]<a href="http://www.sohu.com/a/270896638_633698" target="_blank" rel="noopener">10分钟看懂全卷积神经网络（ FCN ）：语义分割深度模型先驱 </a></p><p>[4]<a href="https://blog.csdn.net/qq_36269513/article/details/80420363" target="_blank" rel="noopener">FCN的学习及理解（Fully Convolutional Networks for Semantic Segmentation）</a></p><p>[5]<a href="https://blog.csdn.net/qq_37274615/article/details/73251503" target="_blank" rel="noopener">FCN的理解</a></p><p>[6]<a href="https://blog.csdn.net/justpsss/article/details/77170004" target="_blank" rel="noopener">FCN和U-Net</a></p><p>[7]<a href="https://blog.csdn.net/natsuka/article/details/78565229" target="_blank" rel="noopener">U-net翻译</a></p><p>[8]<a href="https://blog.csdn.net/mieleizhi0522/article/details/82025509" target="_blank" rel="noopener">U-net论文解析</a></p><p>[9]<a href="http://tech.ifeng.com/c/7kx5uizAx5u" target="_blank" rel="noopener">一文带你读懂 SegNet（语义分割）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FCN&quot;&gt;&lt;a href=&quot;#FCN&quot; class=&quot;headerlink&quot; title=&quot;FCN&quot;&gt;&lt;/a&gt;FCN&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot; target=&quot;_blank&quot; rel
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="FCN" scheme="http://a-kali.github.io/tags/FCN/"/>
    
      <category term="CNN" scheme="http://a-kali.github.io/tags/CNN/"/>
    
      <category term="U-Net" scheme="http://a-kali.github.io/tags/U-Net/"/>
    
      <category term="反卷积" scheme="http://a-kali.github.io/tags/%E5%8F%8D%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
</feed>
