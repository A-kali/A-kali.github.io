<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2021-01-18T01:41:23.718Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>EfficientNet简述</title>
    <link href="http://a-kali.github.io/2021/01/18/EfficientNet%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2021/01/18/EfficientNet简述/</id>
    <published>2021-01-18T01:01:09.000Z</published>
    <updated>2021-01-18T01:41:23.718Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p><p>EfficientNet作者系统地研究了模型缩放，并发现对网络<strong>深度、宽度和分辨率</strong>进行平衡可以带来更好的性能，而前人的文章多是放大其中的一个以达到更高的准确率。EfficientNet仅使用了很小的参数量就超越了当时的SOTA模型。</p><p><img src="https://i.loli.net/2021/01/18/zeOAPEBmIoHFCyi.png" alt="image.png"></p><p>作者发现只对模型的深度、宽度和分辨率其中一个维度进行扩张能得到性能提升，但是会有较大的局限性。作者认为<strong>各个维度之间的扩张不应该是相互独立的</strong>，比如说，对于更大分辨率的图像，应该使用更深、更宽的网络，这就意味着需要平衡各个扩张维度，而不是在单一维度张扩张。</p><p>对此作者提出了<strong>复合缩放法(compound scaling method)</strong>，方程式如下，其中约束(s.t.)限制了模型的复杂度。在约束式中宽度和分辨率都有一个平方项，这是因为如果增加宽度或分辨率两倍，其计算量是增加四倍，但是增加深度两倍，其计算量只会增加两倍。</p><p><img src="https://i.loli.net/2021/01/18/WZAmwraFBPnHfiq.png" alt="image.png"></p><p>求解方式：</p><ol><li>固定公式中的φ=1，然后通过网格搜索（grid search）得出最优的α、β、γ，得出最基本的模型EfficientNet-B0.</li><li>固定α、β、γ的值，使用不同的φ，得到EfficientNet-B1, …, EfficientNet-B7</li></ol><p>φ的大小对应着消耗资源的大小，相当于：</p><ol><li>当φ=1时，得出了一个最小的最优基础模型；</li><li>增大φ时，相当于对基模型三个维度同时扩展，模型变大，性能也会提升，资源消耗也变大。</li></ol><p>值得一提的是EfficientNet中使用了<strong>移动翻转瓶颈卷积(mobile inverted bottleneck convolution，MBConv)模块</strong>，该模块引入了深度分离卷积和SENet的思想。</p><p>参考文献：</p><p>[1]<a href="https://zhuanlan.zhihu.com/p/96773680" target="_blank" rel="noopener">令人拍案叫绝的EfficientNet和EfficientDet</a><br>[2]<a href="https://www.jianshu.com/p/2ac06d97a830" target="_blank" rel="noopener">速度与精度的结合 - EfficientNet 详解</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/111115509" target="_blank" rel="noopener">EfficientNet-B0解读</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/258386372" target="_blank" rel="noopener">EfficentNet详解之MBConvBlock</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EfficientNet: Rethinking Model Scaling for Convolutional N
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
  </entry>
  
  <entry>
    <title>RANSAC算法概述</title>
    <link href="http://a-kali.github.io/2020/12/30/RANSAC%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/30/RANSAC算法概述/</id>
    <published>2020-12-30T09:14:17.000Z</published>
    <updated>2020-12-30T09:15:36.758Z</updated>
    
    <content type="html"><![CDATA[<p><strong>RANSAC（Random Sample Consensus，随机取样一致）</strong>是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。</p><p>一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图）。最小二乘法会使用包括异常样本的所有的数据样本进行计算，得到拟合参数，但其会受到异常点的影响。而RANSAC就是为了解决这个问题。</p><p><img src="https://i.loli.net/2020/12/29/r9S51AEdvij6N3O.png" alt="image.png"></p><p>RANSAC算法通过反复选择数据中的以组随机子集进行验证，概述如下：</p><ol><li>随机选择一组点作为<strong>局内点</strong>，用这部分局内点拟合一个模型；</li><li>将所有满足该模型的点加入一个新的局内点集合；</li><li>使用新的局内点集合去拟合一个新的模型，并测试模型的准确度（即满足模型的点                                                                                          占所有点的比例）；</li><li>重复步骤2、3，最终得到一个准确度达到预期的模型。</li></ol><p>RANSAC常用于删除特征匹配过程中的异常点。</p><p><img src="https://i.loli.net/2020/12/29/eVio9WpMPXntUIA.png" alt="image.png"></p><p>参考博客：<a href="https://blog.csdn.net/robinhjwy/article/details/79174914" target="_blank" rel="noopener">RANSAC算法理解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;RANSAC（Random Sample Consensus，随机取样一致）&lt;/strong&gt;是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。&lt;/p&gt;
&lt;p&gt;一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图
      
    
    </summary>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="RANSAC" scheme="http://a-kali.github.io/tags/RANSAC/"/>
    
  </entry>
  
  <entry>
    <title>SIFT特征提取和描述</title>
    <link href="http://a-kali.github.io/2020/12/28/SIFT%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%8F%8F%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/28/SIFT特征提取和描述/</id>
    <published>2020-12-28T10:26:10.000Z</published>
    <updated>2020-12-30T09:12:19.343Z</updated>
    
    <content type="html"><![CDATA[<p>SIFT，即<strong>尺度不变特征变换（Scale-invariant feature transform，SIFT）</strong>，是用于图像处理领域对局部特征的一种描述方法。主要原理是<strong>通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点的值生成一个用于稳定描述该特征点的向量</strong>。该算法常用于特征匹配领域。</p><p>SIFT算法具有如下一些特点：</p><ol><li>SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</li><li>区分性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；</li><li>多量性，即使少数的几个物体也可以产生大量的SIFT特征向量；</li><li>高速性，经优化的SIFT匹配算法甚至可以达到实时的要求；</li><li>可扩展性，可以很方便的与其他形式的特征向量进行联合。</li></ol><h1 id="一、图像尺度空间"><a href="#一、图像尺度空间" class="headerlink" title="一、图像尺度空间"></a>一、图像尺度空间</h1><p>在一定范围内，无论物体是大还是小，人眼都能分辨出来；但计算机对不同尺度下的物体分辨能力却很低。所以要让机器能够对物体在不同尺度下有一个统一的认知，就需要考虑图像在不同尺度下都存在的特点。</p><p>不同尺度的图像可以通过<strong>高斯模糊</strong>（或称为<a href="https://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/">高斯滤波</a>）来模拟：<br>$$<br>L(x, y, \sigma)=G(x, y, \sigma)*I(x, y)<br>$$<br>其中I为图像，G为高斯函数：<br>$$<br>G(x, y, \sigma)=\frac{1}{2\pi\sigma^2}e^{\frac{x^2+y^2}{2\sigma^2}}<br>$$<br>其中σ用于控制滤波器权重，σ越大，权重数值分布越均匀，即周边值更大、中心值更小，得到的图像越模糊。</p><p><img src="https://i.loli.net/2020/12/27/LU4sBYMQXWIb8Fx.png" alt="image.png"></p><p>通过这个方法可以得到不同尺度空间的图像，图像越模糊，就相当于得到了更远距离/更小尺度空间的图像。</p><h1 id="二、高斯差分金字塔"><a href="#二、高斯差分金字塔" class="headerlink" title="二、高斯差分金字塔"></a>二、高斯差分金字塔</h1><p>对于一张图像进行n-1次上/下采样操作，得到的n个不同分辨率的结果可以组合成一个<strong>图像金字塔</strong>。对金字塔的每一层进行m次不同程度的高斯模糊，最终得到n×m张图像。</p><p><img src="https://i.loli.net/2020/12/27/HerEtTBQoWiIXD2.png" alt="image.png"></p><p>之后对金字塔每一层得到的模糊结果分别求差，得到n×(m-1)个二维数组，得到的这个结果就是<strong>高斯差分金字塔（Difference of Gaussian, DOG）</strong>。DOG中较大的值表示该点对于不同的模糊程度变化更大，该点更有可能是特征的关键点/边缘。找出<strong>局部极值点</strong>需要对每个点与其周围的26个点（同平面8个，上下各9个）点进行比较。</p><p><img src="https://i.loli.net/2020/12/28/9HfCAV4gOTuz57l.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/28/QX2VGIU7OmFjH98.png" alt="image.png"></p><p>DOG用数学公式表示如下：<br>$$<br>D(x,y,\sigma)=L(x, y, k\sigma)-L(x, y, \sigma)<br>$$</p><h1 id="三、特征描述"><a href="#三、特征描述" class="headerlink" title="三、特征描述"></a>三、特征描述</h1><h2 id="1-特征点的方向"><a href="#1-特征点的方向" class="headerlink" title="1    特征点的方向"></a>1    特征点的方向</h2><p>每个像素点L(x, y)的梯度的模m(x, y)以及方向θ(x, y)计算如下：<br>$$<br>m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L{(x,y-1)}]^2}\<br>\theta(x,y)=arctan \frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}<br>$$<br>每个像素点可以得到<strong>位置(x, y)、尺度m和方向θ</strong>三个信息。对于每个特征点，统计其邻域所有像素点在8个方向上的尺度和，尺度和最大的方向视为该特征点的<strong>主方向</strong>，占比超过主方向80%的视为<strong>辅方向</strong>。具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，于是一个特征点就产生了多个坐标、尺度相同，但方向不同的特征点。</p><p><img src="https://i.loli.net/2020/12/28/qjAZ2HrOoeJ8law.png" alt="image.png"></p><h2 id="2-生成特征描述"><a href="#2-生成特征描述" class="headerlink" title="2    生成特征描述"></a>2    生成特征描述</h2><p>本部分将使用邻域像素的方向和尺度为该特征点生成一个唯一的指纹，称为<strong>描述符</strong>。首先在关键点周围采用16×16的邻域，将该16×16区域进一步划分为4×4子块。由于子块中的每一个像素都具有8个方向中的一个，并且具有尺度。于是对于每一个子块都能用一个长度为8的向量来表示该子块所有像素在8个方向上的尺度和。</p><p>最终对于每一个特征点，我们得到了一个总长度为4×4×8=128的特征描述符。</p><p><img src="https://i.loli.net/2020/12/28/26wiWJ8cZzmL9aV.png" alt="image.png"></p><p>计算两个特征点描述符之间的欧氏距离即可进行匹配。原算法中使用的是kd树进行搜索匹配，这里不作详细描述。</p><p><img src="https://i.loli.net/2020/12/28/rKe1gRlLtJNPi4k.png" alt="image.png"></p><h1 id="四、OpenCV中的SIFT算法"><a href="#四、OpenCV中的SIFT算法" class="headerlink" title="四、OpenCV中的SIFT算法"></a>四、OpenCV中的SIFT算法</h1><p>由于算法版权问题，SIFT算法只能在OpenCV3.4及以下版本才能使用。</p><p>特征描述：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sift = cv2.xfeatures2d.SIFT_create()  <span class="comment"># 创建一个SIFT对象</span></span><br><span class="line">kp, des = sift.detectAndCompute(img_gray, <span class="literal">None</span>) <span class="comment"># 返回关键点对象和以及每个关键点的特征向量</span></span><br><span class="line">show_kp_img = cv2.drawKeypoints(img_gray, kp, img)  <span class="comment"># 在图像中标出关键点</span></span><br></pre></td></tr></table></figure><p>特征匹配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一对一匹配</span></span><br><span class="line">bf = cv2.BFMatcher(crossCheck=<span class="literal">True</span>)  </span><br><span class="line">matchs = bf.match(des1, des2)</span><br><span class="line">matches.sort(key=<span class="keyword">lambda</span> x: x.distance)</span><br><span class="line"><span class="comment"># 可视化匹配结果</span></span><br><span class="line">matched_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:<span class="number">10</span>], <span class="literal">None</span>, flags=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SIFT，即&lt;strong&gt;尺度不变特征变换（Scale-invariant feature transform，SIFT）&lt;/strong&gt;，是用于图像处理领域对局部特征的一种描述方法。主要原理是&lt;strong&gt;通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>OpenCV与角点检测</title>
    <link href="http://a-kali.github.io/2020/12/28/OpenCV%E4%B8%8E%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/28/OpenCV与角点检测/</id>
    <published>2020-12-28T10:24:08.000Z</published>
    <updated>2020-12-28T10:25:13.604Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Harris角点检测原理"><a href="#Harris角点检测原理" class="headerlink" title="Harris角点检测原理"></a>Harris角点检测原理</h1><p>Harris角点检测的基本原理：类似于边缘检测，只不过边缘检测只判断单个方向的梯度，而角点检测判断多个方向的梯度/相似性。</p><p>对于一张灰度图，计算在点(x, y)处平移(Δx, Δy)后的自相似性c：<br>$$<br>c(x, y;Δx, Δy)=\sum_{(u, v)\in W(x,y)}w(u,v)(P(u,v)-P(u+Δx, v+Δy))^2<br>$$<br>W(x, y)表示以点(x, y)为中心的窗口；而w(x, y)是每个像素的权重，既可以是常数，也可以是高斯加权函数；P(u, v)表示坐标为(u, v)的像素值。上述公式表示计算窗口滑动前后，窗口中的每个像素及其对应像素的差值加权总和。</p><p>对平移后的像素点进行泰勒一阶展开，可得：<br>$$<br>P(u+Δx, v+Δy)\approx P(u,v)+P_x(u,v)Δx+P_y(u,v)Δy<br>$$<br>于是相似性c简化后如下，此时可以看出这是一个椭圆函数：<br>$$<br>c(x, y;Δx, Δy)\approx\ AΔx^2+2CΔxΔy+BΔy^2<br>$$<br>其中<br>$$<br>A=I_x(x,y)^2\<br>B=I_y(x,y)^2\<br>C=I_x(x,y)I_y(x,y)<br>$$<br>经过对角化消除C后得：<br>$$<br>c(x, y;Δx, Δy)\approx\ \lambda_1Δx^2+\lambda_2Δy^2<br>$$<br>可以看出λ越大，表示平移前后的滑窗灰度值差别越大。λ1大表示在横坐标上相差较大，λ2大表示在纵坐标上相差较大。当λ1和λ2的值都很大且值相近时，判断(x, y)为物体角点。</p><p><img src="https://i.loli.net/2020/12/24/ZGvlFgDmr7i1yOT.png" alt="image.png"></p><p>于是可以通过计算<strong>角点响应值</strong>R来判断该点是否为角点。<br>$$<br>R=\lambda_1\lambda_2-k (\lambda_1+\lambda_2)^2<br>$$<br>其中k为系数，通常取值0.04 ~ 0.06。R&gt;&gt;0表示该点为角点，R≈0表示该点为平坦区，R&lt;&lt;0表示该点为边界。</p><h1 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.cornerHarris(img, blockSize, ksize, k)</span><br></pre></td></tr></table></figure><ul><li>img：输入图像；</li><li>blockSize：滑动窗口的大小；</li><li>ksize：Sobel求导中使用的矩阵大小；</li><li>k：计算角点响应值的系数。</li></ul><p>检测结果：</p><p><img src="https://i.loli.net/2020/12/24/8YvEOeaNGot7qSz.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Harris角点检测原理&quot;&gt;&lt;a href=&quot;#Harris角点检测原理&quot; class=&quot;headerlink&quot; title=&quot;Harris角点检测原理&quot;&gt;&lt;/a&gt;Harris角点检测原理&lt;/h1&gt;&lt;p&gt;Harris角点检测的基本原理：类似于边缘检测，只不过边缘检
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="角点检测" scheme="http://a-kali.github.io/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>图像直方图与均衡化</title>
    <link href="http://a-kali.github.io/2020/12/28/%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%B8%8E%E5%9D%87%E8%A1%A1%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/12/28/图像直方图与均衡化/</id>
    <published>2020-12-28T10:21:39.000Z</published>
    <updated>2020-12-28T10:22:58.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a>图像直方图</h1><p><strong>图像直方图</strong>即对图像中每个像素值出现次数的统计直方图。</p><p><img src="https://i.loli.net/2020/12/21/xt3kON27wSyeZ85.png" alt="image.png"></p><p>OpenCV：<code>cv2.calcHist(images, channels, mask, histSize, ranges)</code></p><ul><li>images：输入图像；</li><li>channels：输入一个包含[0, 1, 2]的列表选择通道，全选表示BGR，[0]表示灰度图；</li><li>mask：与images对齐的掩码，用于标记需要统计的像素，全选则输入None；</li><li>histSize：直方图横坐标的个数；</li><li>ranges：像素值范围，默认为[0, 256]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'cat.jpg'</span>)</span><br><span class="line">color = (<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate(color):</span><br><span class="line">    hist = cv2.calcHist([img], [i], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    plt.plot(hist, color=col)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/21/c4CFThHt6OUIVZr.png" alt="image.png"></p><h1 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h1><p><strong>直方图均衡化</strong>是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。其基本思想是对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减。从而达到清晰图像的目的。</p><p><img src="https://i.loli.net/2020/12/21/DfW9auiAH875oBJ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/21/KpfIYFsvorSGyRT.png" alt="image.png"></p><p>步骤：</p><ol><li>确定图像的灰度级（通常情况下，如果我们的图像是彩色。需要将其转换为灰度图像，其灰度级一般是0-255）；</li><li>统计每一个灰度在原始图像上的像素所占总体的比例，即每个灰度的概率；</li><li>计算累加概率（将低灰度级的概率累加到高灰度级的概率上）；</li><li>根据公式求映射结果：</li></ol><p>$$<br>SS(i)=int((max(pix)-min(pix))*S(i)+0.5)<br>$$</p><p><img src="https://i.loli.net/2020/12/21/k1VlTa9vjCKhszf.png" alt="image.png"></p><p>OpenCV代码：<code>cv2.equalizeHist(img)</code></p><p>虽然直方图均衡化后能让图像整体更加清晰，但有时候会使得原本突出的局部特征变得模糊（如图中的人脸）。</p><p><img src="https://i.loli.net/2020/12/21/Bep6arx71ok8FLb.png" alt="image.png"></p><p>可以使用<strong>自适应直方图均衡化</strong>解决这个问题。将图像划分为多个小区域（在opencv中默认为8×8），对每一个划分子域分别进行直方图均衡化。最后使用双线性插值对每一个子域的边界进行拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a CLAHE object (Arguments are optional).</span></span><br><span class="line">clahe = cv2.createCLAHE(clipLimit=<span class="number">2.0</span>, tileGridSize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">img = clahe.apply(img)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像直方图&quot;&gt;&lt;a href=&quot;#图像直方图&quot; class=&quot;headerlink&quot; title=&quot;图像直方图&quot;&gt;&lt;/a&gt;图像直方图&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;图像直方图&lt;/strong&gt;即对图像中每个像素值出现次数的统计直方图。&lt;/p&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="直方图" scheme="http://a-kali.github.io/tags/%E7%9B%B4%E6%96%B9%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（二）：骨干网络的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（二）：骨干网络的改进/</id>
    <published>2020-12-21T00:17:53.000Z</published>
    <updated>2020-12-21T00:21:32.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CSPDarknet53"><a href="#CSPDarknet53" class="headerlink" title="CSPDarknet53"></a>CSPDarknet53</h1><p>在Darknet53的基础上参考CSPNet加入了CSP(Cross Stage Paritial)结构。整个backbone划分为5个模块，每个模块下采样2倍。</p><p>如下图所示，CSP结构仅仅是将特征的一部分直接concat到block的末尾，这个操作使得该block中的每个dense layer的梯度不再直接参与更浅层的梯度计算，而是新计算出一个值（于是可以在反向传播到浅层时，清除深层的dense layer梯度信息），大量减少了内存的消耗和计算瓶颈。</p><p><img src="https://i.loli.net/2020/12/21/hrMEojey9fZ13LR.png" alt="image-20201219060045088.png"></p><h1 id="Mish激活函数"><a href="#Mish激活函数" class="headerlink" title="Mish激活函数"></a>Mish激活函数</h1><p>$$<br>Mish = x*tanh(ln(1+e^x))<br>$$</p><p><img src="https://i.loli.net/2020/12/19/7AFq31YoTClmSDt.png" alt="image.png"></p><p>可以看出Mish比ReLU更加平滑，在每个点上都是可微的，优化效果更好；同时允许了较小的负梯度流入，更好地保证信息的流动。在YOLOv4中仅backbone使用了Mish，其它地方都用的Leaky ReLU。</p><h1 id="Dropblock"><a href="#Dropblock" class="headerlink" title="Dropblock"></a>Dropblock</h1><p>Dropblock与Dropout类似。Dropout会随机丢弃网络中的一些信息，但卷积层对随机丢弃的信息并不敏感，即使随机丢弃一些像素，卷积也能从相邻的像素学到相同的信息。</p><p>于是Dropblock对整个局部区域进行丢弃，相当于对特征图进行CutOut操作。</p><p><img src="https://i.loli.net/2020/12/19/PiCgEXtkaUsK9cl.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CSPDarknet53&quot;&gt;&lt;a href=&quot;#CSPDarknet53&quot; class=&quot;headerlink&quot; title=&quot;CSPDarknet53&quot;&gt;&lt;/a&gt;CSPDarknet53&lt;/h1&gt;&lt;p&gt;在Darknet53的基础上参考CSPNet加入了CSP(C
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（一）：输入端的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%BE%93%E5%85%A5%E7%AB%AF%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（一）：输入端的改进/</id>
    <published>2020-12-21T00:12:33.000Z</published>
    <updated>2020-12-21T00:15:33.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mosaic"><a href="#Mosaic" class="headerlink" title="Mosaic"></a>Mosaic</h1><p>YOLOv4中提出的<strong>Mosaic数据增强</strong>是参考2019年底提出的<strong>CutMix数据增强</strong>的方式，但CutMix只使用了两张图片进行拼接，而Mosaic则采用了4张图片，<strong>随机缩放</strong>、<strong>随机裁剪</strong>、<strong>随机排布</strong>的方式进行拼接。一张图相当于4张图，可以减少训练所需的batch size。</p><p><img src="https://i.loli.net/2020/08/17/XseLfUlb6SiREVW.png" alt="image.png"></p><h1 id="CmBN"><a href="#CmBN" class="headerlink" title="CmBN"></a>CmBN</h1><p>在梯度累积时，模型不同batch的参数梯度会累积在一起一次性反向传播更新参数，但BN的统计量却是在每个batch迭代更新的，这使得反向传播时会产生偏差。<strong>CmBN(Cross mini-Batch Normalization)</strong>则是在梯度累积的同时保证BN的统计量只累积更新，在反向传播的同时更新统计量。</p><p><img src="https://i.loli.net/2020/12/17/5rEOw9ouQyHj3n8.png" alt="image.png"></p><h1 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h1><p><strong>SAT(Self Adversarial Training)</strong>即自对抗训练，文中没有详细说明，可以参考论文Adversarial Examples for Semantic Segmentation and Object Detection。</p><p>第一阶段，将权重设置为固定，图片设置为可导，对目标检测正确的区域采用反向label（即错误的label），计算Loss并进行反向传播，对图片产生扰动，叠加到图片上实现攻击。此时图片肉眼看上去变化不大，但对于神经网络来说，图片已经更接近于反向label。在第二阶段就是正常训练，把被攻击了的图片输入网络进行训练。作者将其归纳为一种数据增强操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mosaic&quot;&gt;&lt;a href=&quot;#Mosaic&quot; class=&quot;headerlink&quot; title=&quot;Mosaic&quot;&gt;&lt;/a&gt;Mosaic&lt;/h1&gt;&lt;p&gt;YOLOv4中提出的&lt;strong&gt;Mosaic数据增强&lt;/strong&gt;是参考2019年底提出的&lt;stro
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】MaxPooling和AveragePooling</title>
    <link href="http://a-kali.github.io/2020/12/21/MaxPooling%E5%92%8CAveragePooling/"/>
    <id>http://a-kali.github.io/2020/12/21/MaxPooling和AveragePooling/</id>
    <published>2020-12-21T00:10:42.000Z</published>
    <updated>2020-12-30T09:32:08.922Z</updated>
    
    <content type="html"><![CDATA[<p>在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了<strong>辨识度更高的特征</strong>，并且提供了<strong>非线性</strong>。</p><p>而Average Pooling通常用来做特征对齐，在<strong>减小特征图尺寸</strong>的同时，保证特征的<strong>完整性</strong>。比如说DenseNet的模块之间大多采用Average Pooling进行连接。或者是用于网络的最后一层，<strong>代替平铺操作</strong>，将三维特征图转化为一维向量。</p><p><strong>Q：MaxPooling的反向传播是如何进行的？</strong></p><p>梯度正常传播给最大值，并将其余非最大值的损失/梯度置零。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了&lt;strong&gt;辨识度更高的特征&lt;/strong&gt;，并且提供了&lt;strong&gt;非线性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而Average Pooling通常用来做特征对齐，在
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="池化" scheme="http://a-kali.github.io/tags/%E6%B1%A0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Dropout</title>
    <link href="http://a-kali.github.io/2020/12/13/Dropout/"/>
    <id>http://a-kali.github.io/2020/12/13/Dropout/</id>
    <published>2020-12-13T00:13:47.000Z</published>
    <updated>2020-12-13T00:15:00.739Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout有什么作用？</strong></p><p>解决过拟合的问题。</p><p><strong>Q：Dropout如何实现？</strong></p><p>使该层每个神经元的激活层都有一定概率输出0。</p><p><strong>Q：Dropout 反向传播的处理</strong></p><p>反向传播时忽略被Dropout的神经元。</p><p><strong>Q：Dropout是失活神经元还是失活连接？</strong></p><p>失活神经元并清除其周围连接。</p><p><strong>Q：Dropout为什么能防止过拟合？</strong></p><ol><li>使模型泛化能力更强，不依赖于某些局部的特征；</li><li>Dropout可以看作多个共享部分参数的模型的集成（其实这条跟上面那条是共通的）；</li></ol><p><strong>Q：Dropout在训练和测试时有何不同？</strong></p><p>Dropout在测试时不会失活神经元。并且在训练时根据失活率p对神经元权重进行缩放，即除以1-p；或者在测试时乘以p。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout有什么作用？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决过拟合的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：Dropout如何实现？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使该层每个神经元的激活层都有一定概率输出0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：D
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="Dropout" scheme="http://a-kali.github.io/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】FLOPs与参数量的计算</title>
    <link href="http://a-kali.github.io/2020/12/13/FLOPs%E4%B8%8E%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97/"/>
    <id>http://a-kali.github.io/2020/12/13/FLOPs与参数量的计算/</id>
    <published>2020-12-12T20:36:07.000Z</published>
    <updated>2020-12-21T00:28:56.497Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h1><p><strong>FLOPs(floating point operations)</strong>，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。注意不要和FLOPS(floating point operations per second)搞混。</p><p>拿卷积操作举例。这里需要注意，卷积的计算量如果从output map size考虑的话，是可以与input map size无关的。因为卷积的每一次矩阵相乘都一定输出output map的一个像素点，但如果从input map下手的话，还需要考虑stride和padding。</p><p>所以<strong>卷积层</strong>的FLOPs计算公式为：<br>$$<br>FLOPs=(2×C_i×K^2)×(H×W×C_o)<br>$$<br>其中Ci和Co分别为输入输出的通道数，K为卷积核大小，H和W分别为output map的高和宽。</p><p>这个公式前半部分的运算就是每计算一个输出元素所需要的计算量，就是output map的元素数量。之所以要×2是因为一次卷积运算需要进行乘法和加法两种操作（相乘后求和），比如说两个3×3大小的矩阵相乘求和得到一个元素值，需要进行3×3次乘法操作和3×3-1次加法操作，再加上神经网络中的bias，运算量一共为2×3×3。</p><p>同理可得<strong>全连接层</strong>的FLOPs计算公式为：<br>$$<br>FLOPs=2×I×O<br>$$</p><h1 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h1><p>卷积参数量的计算比较简单，这里带过一下：<br>$$<br>params=(K^2×C_i+1)×C_o<br>$$</p><p>实际工程上要计算这两个值的话的话，感觉github上有挺多包的……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FLOPs&quot;&gt;&lt;a href=&quot;#FLOPs&quot; class=&quot;headerlink&quot; title=&quot;FLOPs&quot;&gt;&lt;/a&gt;FLOPs&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;FLOPs(floating point operations)&lt;/strong&gt;，意指浮点运算数，
      
    
    </summary>
    
    
      <category term="FLOPs" scheme="http://a-kali.github.io/tags/FLOPs/"/>
    
      <category term="参数量" scheme="http://a-kali.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV中的平滑、腐蚀膨胀和边缘检测</title>
    <link href="http://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/13/OpenCV中的平滑、腐蚀膨胀和边缘检测/</id>
    <published>2020-12-12T20:32:16.000Z</published>
    <updated>2020-12-30T09:13:33.947Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像平滑与滤波"><a href="#图像平滑与滤波" class="headerlink" title="图像平滑与滤波"></a>图像平滑与滤波</h1><p>图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，这些图像滤波都是一些简单的卷积操作。</p><ul><li>均值滤波：即在一个卷积核的感受野内的平均值作为该像素点的值，opencv代码为<code>cv2.blur(img, (3, 3))</code>。</li><li>高斯滤波：高斯滤波会根据距离取周围的像素点加权和，越近的像素权重越大。opencv代码为<code>cv2.GaussianBlur</code>。由于高斯滤波对中心点权重最高，使得其对椒盐类的噪声处理效果较差。</li><li>中值滤波：即在一个卷积核的感受野内的中值作为该像素点的值，opencv代码为<code>cv2.midianBlur(img, (3, 3))</code>。中值滤波对中心点的取值较为绝对化，故对椒盐类的噪声处理能力较强。</li></ul><p>下面三张图分别为原图、高斯滤波结果、中值滤波结果。</p><p><img src="https://i.loli.net/2020/12/05/tHxuK4iXhCP85cz.png" alt="image.png"></p><h1 id="图像的形态学操作"><a href="#图像的形态学操作" class="headerlink" title="图像的形态学操作"></a>图像的形态学操作</h1><h2 id="1-腐蚀与膨胀"><a href="#1-腐蚀与膨胀" class="headerlink" title="1    腐蚀与膨胀"></a>1    腐蚀与膨胀</h2><p>进行膨胀(dilation)操作时，使用卷积核遍历图像，使用内核覆盖区域的最大相素值代替锚点位置的相素。这一最大化操作将会导致图像中的亮区扩大，因此名为膨胀 。腐蚀(erosion)则与膨胀相反。在OpenCV中代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line">cv2.erode(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 腐蚀</span></span><br><span class="line">cv2.dilate(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 膨胀</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/05/XHDSuCR7yxOYIin.png" alt="image.png"></p><h2 id="2-开运算与闭运算"><a href="#2-开运算与闭运算" class="headerlink" title="2    开运算与闭运算"></a>2    开运算与闭运算</h2><p>开运算指的是先腐蚀后膨胀的操作，闭运算则是先膨胀后腐蚀的操作。通常用于消除噪点、沟壑，平滑物体轮廓。在OpenCV中代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # 开运算</span><br><span class="line">cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)  # 闭运算</span><br></pre></td></tr></table></figure><h2 id="3-梯度计算"><a href="#3-梯度计算" class="headerlink" title="3    梯度计算"></a>3    梯度计算</h2><p>使用膨胀的结果减去腐蚀的结果可以计算得到图像的梯度，但OpenCV中提供了更便捷的操作：<code>cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</code>。</p><h1 id="Sobel算子"><a href="#Sobel算子" class="headerlink" title="Sobel算子"></a>Sobel算子</h1><p>Sobel算子是计算机视觉领域的一种重要处理方法。主要用于获得数字图像的<strong>一阶梯度</strong>，常用于<strong>边缘检测</strong>（注意与轮廓检测相区分）。Sobel算子是把图像中每个像素的上下左右四领域的<strong>灰度值加权差</strong>，在边缘处达到极值从而检测边缘。</p><p>该算子包含两组3x3的矩阵，分别为横向及纵向，将之与图像作平面<strong>卷积</strong>，即可分别得出横向及纵向的差值。如果以A代表原始图像，Gx及Gy分别代表经横向及纵向边缘检测的图像，其公式如下：</p><p><img src="https://i.loli.net/2020/12/04/rIq3OVLCfDnWvpu.png" alt="image.png"></p><p>在OpenCV中，可通过<code>cv2.Sobel(src, ddepth, dx, dy, ksize)</code>调用Sobel算子。具体操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'pic.png'</span>, cv2.IMREAD_GRAYSCALE)  <span class="comment"># 读取灰度图</span></span><br><span class="line">sobelx = cv2.Sobel(img, cv2.CV_64F, <span class="number">1</span>, <span class="number">0</span>, ksize=<span class="number">3</span>)  <span class="comment"># x方向的梯度</span></span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)  <span class="comment"># 取绝对值将复数转为正数</span></span><br><span class="line">sobely = cv2.Sobel(img, cv2.CV_64F, <span class="number">0</span>, <span class="number">1</span>, ksize=<span class="number">3</span>)  <span class="comment"># y方向的梯度</span></span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x和y方向分别求到的梯度加权求和</span></span><br><span class="line">sobelxy = cv2.addWeighted(sobelx, <span class="number">0.5</span>, sobely, <span class="number">0.5</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>求得的结果如下所示，其中左边是原图，右边是边缘检测结果：</p><p><img src="https://i.loli.net/2020/12/04/CgwknPWiDm12b7x.png" alt="image.png"></p><p>因为直接求两个方向的梯度效果较差，所以此处是对两个方向分别求梯度再加权平均，而不是直接求两个方向的梯度。</p><h1 id="Canny边缘检测"><a href="#Canny边缘检测" class="headerlink" title="Canny边缘检测"></a>Canny边缘检测</h1><p>Canny边缘检测算法是对图像进行的一系列用于边缘检测的操作步骤：</p><ol><li>使用高斯滤波器对图像进行平滑处理，滤除噪声；</li><li>计算图像中每个像素点的梯度强度和方向；</li><li>使用非极大值抑制消除边缘检测带来的杂散效应；</li><li>应用双阈值检测找出潜在的边缘，抑制孤立弱边缘。</li></ol><p><img src="https://i.loli.net/2020/12/05/f3BTRk6cFPYgqHG.png" alt="image.png"></p><p>OpenCV：<code>cv2.Canny(img, min_threshold, max_threshold)</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像平滑与滤波&quot;&gt;&lt;a href=&quot;#图像平滑与滤波&quot; class=&quot;headerlink&quot; title=&quot;图像平滑与滤波&quot;&gt;&lt;/a&gt;图像平滑与滤波&lt;/h1&gt;&lt;p&gt;图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="边缘检测" scheme="http://a-kali.github.io/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】手撕卷积</title>
    <link href="http://a-kali.github.io/2020/12/03/%E6%89%8B%E6%92%95%E5%8D%B7%E7%A7%AF/"/>
    <id>http://a-kali.github.io/2020/12/03/手撕卷积/</id>
    <published>2020-12-03T09:18:27.000Z</published>
    <updated>2020-12-03T09:19:44.350Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(z, K, b, padding=<span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多通道卷积前向过程</span></span><br><span class="line"><span class="string">    :param z: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数</span></span><br><span class="line"><span class="string">    :param K: 卷积核,形状(C,D,k1,k2), C为输入通道数，D为输出通道数</span></span><br><span class="line"><span class="string">    :param b: 偏置,形状(D,)</span></span><br><span class="line"><span class="string">    :param padding: padding</span></span><br><span class="line"><span class="string">    :param strides: 步长</span></span><br><span class="line"><span class="string">    :return: 卷积结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    padding_z = np.lib.pad(z, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (padding[<span class="number">0</span>], padding[<span class="number">0</span>]), (padding[<span class="number">1</span>], padding[<span class="number">1</span>])), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)  <span class="comment"># 在H和W维度上进行padding</span></span><br><span class="line">    N, _, height, width = padding_z.shape</span><br><span class="line">    C, D, k1, k2 = K.shape</span><br><span class="line">    conv_z = np.zeros((N, D, <span class="number">1</span> + (height - k1) // strides[<span class="number">0</span>], <span class="number">1</span> + (width - k2) // strides[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> np.arange(N):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> np.arange(D):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> np.arange(height - k1 + <span class="number">1</span>)[::strides[<span class="number">0</span>]]:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> np.arange(width - k2 + <span class="number">1</span>)[::strides[<span class="number">1</span>]]:</span><br><span class="line">                    conv_z[n, d, h // strides[<span class="number">0</span>], w // strides[<span class="number">1</span>]] = np.sum(padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]</span><br><span class="line">    <span class="keyword">return</span> conv_z</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="卷积" scheme="http://a-kali.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>CenterLoss简述</title>
    <link href="http://a-kali.github.io/2020/12/03/CenterLoss%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/03/CenterLoss简述/</id>
    <published>2020-12-03T09:18:01.000Z</published>
    <updated>2020-12-12T20:43:25.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h1><p>对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图为神经网络在MNIST数据集上进行手写体数字分类任务得到的特征分布，在类与类之间能看到很明显的界限，能够便于softmax对特征进行分类。但是仍然可以看出部分不同类的样本距离很近，而同类的样本分布距离较远，这有可能对最终的分类结果有影响。</p><p><img src="https://i.loli.net/2020/12/02/4M5jQxUcpW3H6G1.png" alt="image.png"></p><p>而<strong>Center Loss</strong>的提出就是为了<strong>缩小类内(intra-class)距离</strong>。其公式表示如下：<br>$$<br>L_c=\frac{1}{2}\sum^m_{i=1}‖x_i-c_{yi}‖^2_2<br>$$<br>其中$c_{yi}$表示第y个类别的特征中心，$x_i$表示全连接层之前的特征，$m$表示mini-batch的大小。该损失函数试图使每个样本的特征向其类别中心靠近。</p><p>至于类中心怎么更新迭代，这里就不写了（公式太复杂了yingyingying）。反正最终得到的特征分布如下：</p><p><img src="https://i.loli.net/2020/12/02/pdn8rZzMEN7OaoA.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Center-Loss&quot;&gt;&lt;a href=&quot;#Center-Loss&quot; class=&quot;headerlink&quot; title=&quot;Center Loss&quot;&gt;&lt;/a&gt;Center Loss&lt;/h1&gt;&lt;p&gt;对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图
      
    
    </summary>
    
    
      <category term="损失函数" scheme="http://a-kali.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>CycleGAN简述</title>
    <link href="http://a-kali.github.io/2020/11/19/CycleGAN%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/11/19/CycleGAN简述/</id>
    <published>2020-11-19T04:18:04.000Z</published>
    <updated>2020-11-19T04:19:12.186Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks)</p><p>PyTorch代码：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p><p>CycleGAN（或许）是首个将GAN用于风格迁移的网络。</p><p>CycleGAN的作用本质是Image2Image的风格迁移，并不是传统意义上的生成。</p><p><img src="https://i.loli.net/2020/11/16/x2lpJWfVCsqhyja.png" alt="image.png"></p><p>相比于其它Image2Image的网络，CycleGAN并不需要成对的样本数据用来训练，而仅仅需要两组无关的图片，其中目标样本通常为一组风格相似的图片集。</p><p><img src="https://i.loli.net/2020/11/16/Af6MzV79s8NCoTE.png" alt="image.png"></p><p>为了避免网络将X中所有样本映射到Y中的某一个样本，CycleGAN同时包含Y→X的映射，要求网络能够对自己生成的图片进行还原。这使得F(X)必须包含X的内容信息，因此更容易学习Y整体的风格信息（大概）。</p><p><img src="https://i.loli.net/2020/11/16/ivueyVzcxMwRrAk.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="GAN" scheme="http://a-kali.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】如何进行多标签分类</title>
    <link href="http://a-kali.github.io/2020/11/12/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/"/>
    <id>http://a-kali.github.io/2020/11/12/如何进行多标签分类/</id>
    <published>2020-11-11T16:37:46.000Z</published>
    <updated>2020-11-11T16:38:38.377Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：多标签分类怎么解决？</strong></p><p>解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。</p><p>第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这应该是最简单常见的方法，但该方法忽略了标签与标签之间的相关性。</p><p>第二种是使用CNN+RNN+Embedding的策略（参考文献：<a href="https://arxiv.org/abs/1604.04573" target="_blank" rel="noopener">CNN-RNN: A Unified Framework for Multi-label Image Classification</a>）。该方法考虑到了各个标签之间的关联性，将每个标签映射到高维空间上。每次运行预测一个标签，并根据该标签预测下一个标签。理论上该方法的准确率会比较高，但时间效率很低。</p><p><img src="https://i.loli.net/2020/11/11/S5p13cqzibZQfK2.png" alt="image.png"></p><p><a href="https://www.kaggle.com/c/imet-2019-fgvc6" target="_blank" rel="noopener">Kaggle: iMet Collection 2019 - FGVC6</a> 大都会文物分类竞赛就是多标签分类任务，几乎所有参赛者都使用第一种方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：多标签分类怎么解决？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。&lt;/p&gt;
&lt;p&gt;第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>DB：基于可微二值化的实时场景文本检测</title>
    <link href="http://a-kali.github.io/2020/11/12/DB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8F%AF%E5%BE%AE%E4%BA%8C%E5%80%BC%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/11/12/DB：基于可微二值化的实时场景文本检测/</id>
    <published>2020-11-11T16:36:11.000Z</published>
    <updated>2021-01-15T03:15:22.431Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1911.08947" target="_blank" rel="noopener">Real-time Scene Text Detection with Differentiable Binarization</a></p><p>Github：<a href="https://github.com/MhLiao/DB（代码结构很复杂）" target="_blank" rel="noopener">https://github.com/MhLiao/DB（代码结构很复杂）</a></p><p><strong>Structure</strong></p><p>DB(Differentiable Binarization)是一个轻量级的、基于分割的场景文本检测(Scene Text Detection, STD)模型。该模型的原理简洁易懂，在本文中就简单介绍一下。</p><p><img src="https://i.loli.net/2020/11/04/MF73kDbCcueraNL.png" alt="image.png"></p><p>上图展示了DB的模型结构。前半部分是常见的32倍下采样+8倍上采样+特征融合，以及参考FPN采用了后四层反卷积的输出concat到一起进行预测。图中的pred是3×3卷积，输出两张map分别用来表示概率和阈值。使用阈值map对概率map进行二值化后，就将输出结果分为了前景（文本域）和背景。</p><p>这个自适应阈值能把分布比较密集的文本域给隔开，避免混淆成一个文本域。</p><p>值得一提的是阈值map只在训练的时候使用，在测试时仅使用固定阈值。估计在测试时使用DB并不能得到比较有效的提升，出于运算量的考虑决定去掉这部分。</p><p><strong>Loss</strong></p><p>损失函数分为三部分：概率图损失，阈值损失，二值图损失。其中概率图和二值图都使用交叉熵损失函数，而阈值损失使用的是L1损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1911.08947&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-time Scene Text Detection with Differentiable Binariz
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本检查" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%9F%A5/"/>
    
      <category term="DB" scheme="http://a-kali.github.io/tags/DB/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Batch Normalization</title>
    <link href="http://a-kali.github.io/2020/11/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/11/12/神经网络中的归一化/</id>
    <published>2020-11-11T16:32:30.000Z</published>
    <updated>2020-12-12T23:52:51.697Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：BN解决了什么问题？</strong></p><p>解决两个问题：</p><ol><li>Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。</li><li>梯度消失：由于之前Sigmoid一类的激活函数的存在，数据在网络中传播时整体分布逐渐往非线性函数的取值区间的上下限两端靠近，导致反向传播时低层神经网络的梯度消失，神经网络收敛变慢。</li></ol><p><strong>Q：BN的运作方式</strong></p><p>通过一定的规范化手段，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布。让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。同时固定该层的输入分布，使后一层的神经元不用反复重新适应分布的变化。</p><p>但经过这一步后大部分值落入激活函数的线性区内，使得激活函数失去了其本身的非线性意义，网络表达能力下降。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了缩放平移操作(y=scale*x+shift)。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</p><p><strong>Q：手撕BN</strong></p><p>BatchNorm2D（常用于卷积神经网络）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm2D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channel, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = np.zeros(channel) <span class="comment"># 用于测试时</span></span><br><span class="line">        self.running_var = np.ones(channel)   <span class="comment"># 同上</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps                        <span class="comment"># 接近于0的数，用于避免分母为0</span></span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># input.shape: (B, C, H, W)</span></span><br><span class="line">        len_ch = input.size(<span class="number">1</span>)</span><br><span class="line">        output = np.zeros(input.size())</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len_ch):</span><br><span class="line">            in_ch = input[:, i, :, :]</span><br><span class="line">            total_elem = in_ch.numel()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">                mean = in_ch.sum() / total_elem</span><br><span class="line">                var = ((in_ch - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">                out_ch = (in_ch - mean) / (var + self.eps) ** <span class="number">0.5</span>  <span class="comment"># 归一化</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新参数</span></span><br><span class="line">                var_unbiased = ((in_ch - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">                self.running_mean[i] = self.running_mean[i] * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">                self.running_var[i] = self.running_var[i] * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_ch = (in_ch - self.running_mean[i]) / (self.running_var[i] + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">            out_ch = self.gamma * out_ch + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">            output[:, i, :, :] = out_ch</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>BatchNorm1D大概也能根据以上代码进行修改（我瞎写的，仅供参考）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm1D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = <span class="number">0</span></span><br><span class="line">        self.running_var = <span class="number">1</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps           </span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line"></span><br><span class="line">        total_elem = input.numel()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">            mean = input.sum() / total_elem</span><br><span class="line">            var = ((input - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">            output = (input - mean) / (var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            var_unbiased = ((input - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">            self.running_mean = self.running_mean * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">            self.running_var = self.running_var * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = (input - self.running_mean) / (self.running_var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">        output = self.gamma * output + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><strong>Q：BN能防止过拟合吗？为什么？</strong></p><p>BN能一定程度上缓解过拟合。BN使得模型在训练时的输出不仅仅根据当前的输入样本信息，还包含了同一batch其他样本的信息。训练时，同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的。相当于在神经网络中进行了数据增强。</p><p><strong>Q：BN 有哪些参数？</strong></p><p>可训练的参数有缩放因子和平移因子，统计参数有均值和方差，超参数有动量，2D的超参数还包含通道数。</p><p><strong>Q：BN 的反向传播推导</strong></p><ul><li><input disabled type="checkbox"> TODO</li></ul><p><strong>Q：BN 在训练和测试的区别？</strong></p><p>训练时使用的是当前batch的样本统计量进行归一化，测试时使用的是在训练过程中更新迭代计算得到的均值和方差进行归一化。</p><p><strong>Q：BN通常放在什么位置？</strong></p><p>BN通常放在激活函数前。因为BN的作用本来就是为了调整上一层的输出分布，让激活层更好地使用这些输出值。</p><p><strong>Q：BN可以防止过拟合吗？</strong></p><p>BN可以一定程度上缓解过拟合。在样本shuffle训练的情况下，某个样本在不同epoch遇到的同一个batch的其他样本都是不一样的，于是会产生不同的均值和标准差，相当于在模型内部做了数据增强。</p><p><strong>Q：BN和Dropout同时用会怎样？怎样才能同时使用？</strong></p><p>Dropout在训练（或测试）阶段会根据神经元保留率来对神经元权重进行缩放，这会导致测试时隐藏层输出值的方差跟训练时不同。而BN此时已经根据训练数据统计固定了方差参数，无法适应改变后的方差。多层累积下来产生方差偏移，影响模型效果。所以只有在Dropout在所有BN后面时能同时使用。</p><p><strong>Q：有什么其它的归一化方法？</strong></p><ul><li>IN(Instance Norm)：实例归一化。与BN的区别在于，BN使用整个batch的统计量作为参数进行归一化，而IN仅使用当前样本的统计量。IN常用于风格迁移任务中。</li><li><a href="https://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/#more">AdaIN</a>：自适应的实例归一化。在IN的基础上，将缩放和平移参数分别固定为目标风格图像的标准差和均值。在风格迁移中可以快速适应任意风格。</li><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">LN(Layer Norm)</a>：与BN的区别在于，BN是对于一个batch样本的单个通道进行归一化，LN是对单个样本的所有通道进行归一化。可用于RNN或者小batch。</li><li>GN(Group Norm)：组归一化。和LN类似，比LN多一个超参数G，G表示分组的数量。同样用来解决在小batch时BN效果较差的问题。</li></ul><p><img src="https://i.loli.net/2020/11/11/ZtDTfcHxUvyeioj.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：BN解决了什么问题？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
      <category term="BN" scheme="http://a-kali.github.io/tags/BN/"/>
    
      <category term="Normalization" scheme="http://a-kali.github.io/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】IoU和mIoU</title>
    <link href="http://a-kali.github.io/2020/11/03/%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91IoU%E5%92%8CmIoU/"/>
    <id>http://a-kali.github.io/2020/11/03/【面试题】IoU和mIoU/</id>
    <published>2020-11-03T14:53:10.000Z</published>
    <updated>2020-11-11T16:31:05.748Z</updated>
    
    <content type="html"><![CDATA[<p>害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。</p><p>虽然老夫从来没做过基于检测框的目标检测项目。</p><p><strong>Q1：啥是IoU？如何计算IoU？</strong></p><p>IoU就是交并比嘛，两个框相交的面积除以合并的面积。</p><p>定义bbox1, bbox2为两个长度为 4 的数组，用于表示两个检测框左上和右下坐标点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BoxIoU</span><span class="params">(bbox1, bbox2)</span>:</span></span><br><span class="line">x11, y11, x12, y12 = bbox1</span><br><span class="line">    x21, y21, x22, y22 = bbox2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相交面积</span></span><br><span class="line">    iw = max(min(x12, x22) - max(x11, x21), <span class="number">0</span>)</span><br><span class="line">    iy = max(min(y12, y22) - max(y11, y21), <span class="number">0</span>)</span><br><span class="line">    inter = iw * iy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算合并面积</span></span><br><span class="line">area1 = (x12 - x11) * (y12 - y11)</span><br><span class="line">    area2 = (x22 - x21) * (y22 - y21)</span><br><span class="line">uni = area1 + area2 - inter</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = inter / uni</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>其实以上应该都只是基于常规检测框的IoU计算，如果是非矩形检测框或者分割任务中的IoU则要另当别论。</p><p>对于分割任务，定义mask1, mask2为两个相同大小的二维二值numpy数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SegIoU</span><span class="params">(mask1, mask2)</span>:</span></span><br><span class="line"><span class="keyword">return</span> (mask1 &amp; mask2).sum() / (mask1 | mask2).sum()</span><br></pre></td></tr></table></figure><p><strong>Q2：啥是mIoU？如何计算mIoU？</strong></p><p>mIoU即均交并比。对于每个类计算一遍IoU后取平均就行了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。&lt;/p&gt;
&lt;p&gt;虽然老夫从来没做过基于检测框的目标检测项目。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q1：啥是IoU？如何计算IoU？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;IoU就是交并比嘛，两个框相交的面积除以合并的面积。
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="IoU" scheme="http://a-kali.github.io/tags/IoU/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】SRN：使用语义推理网络进行场景文本识别</title>
    <link href="http://a-kali.github.io/2020/10/28/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91SRN%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AF%AD%E4%B9%89%E6%8E%A8%E7%90%86%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/10/28/【论文翻译】SRN：使用语义推理网络进行场景文本识别/</id>
    <published>2020-10-28T13:27:04.000Z</published>
    <updated>2020-10-28T13:28:07.565Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2003.12294" target="_blank" rel="noopener">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></p><p>Github：<a href="https://github.com/chenjun2hao/SRN.pytorch" target="_blank" rel="noopener">https://github.com/chenjun2hao/SRN.pytorch</a> （非官方）</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>场景文本图像包含两个层次的内容：视觉纹理和语义信息</strong>。近年来，虽然已有的场景文本识别方法取得了很大的进展，但<strong>挖掘语义信息来辅助文本识别</strong>的研究却很少，只有类似RNN的结构被用来对语义信息进行隐式建模。然而，基于RNN的译码方法存在着时效性强、语义上下文单向串行传输等缺点，极大地限制了语义信息的利用和计算效率。为了缓解这些限制，我们提出了一种全新的端到端场景文本识别框架——语义推理网络(semantic reasoning network, SRN)，其中引入了一个全局语义推理模块(GSRM)，通过多路并行传输捕获全局语义。我们在常规文本、不规则文本和非拉丁文本等7个公共基准上都验证了该方法的有效性和鲁棒性。此外，相对于基于RNN的方法，SRN的速度有明显的优势，在实际应用中具有一定的价值。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>文本数据通常具有十分丰富的语义信息，这些信息已经被应用在许多计算机视觉的应用中，如自动驾驶、旅游翻译、产品检索等。场景文本识别是场景文本阅读系统的关键步骤。虽然Seq2Seq识别在过去的几十年里取得了一些显著的突破，但在现实场景中进行文本识别仍然是一个巨大的挑战，这归咎于场景文本在颜色、字体、空间布局甚至不可控的背景上都有很大的变化。</p><p>最近的大部分研究都试图从提取视觉特征的角度来提高场景文本识别的性能，如升级backbone，增加校正模块，改进注意机制等。然而对于人来说，场景文本的识别不仅依赖于视觉感知信息，还受到高层次文本语义语境理解的影响。如下图所示的一些例子，在只考虑视觉特征的情况下，很难区分这些图像中的每个字符，尤其是用红色虚线框突出的字符。相反，如果考虑到语义信息，人类很可能会根据单词的全部内容推断出正确的结果。</p><p><img src="https://i.loli.net/2020/10/26/rkAqgxsctiaTP92.png" alt="image.png"></p><p>但主流的文本识别方法对于语义信息通常采用单向串行传输的方式(RNN)，如下图(a)。这种方式有几个明显的缺点：第一，它的每个time step只能感知非常有限的语义语境；其次，当一个time step出现错误解码时，会对后面的time step产生错误累积；同时，序列模型难以并行计算，耗时且低效。</p><p><img src="https://i.loli.net/2020/10/26/RAJg5ZyfB3QKGbz.png" alt="image.png"></p><p>在本文中，我们引入了一种名为全局语义推理模块(GSRM)的子网络结构来解决这些问题。GSRM使用一种全新的多路并行传输方式将全局语义内容联系在一起。如图(b)所示，多路并行传输可以同时感知一个单词或文本行中所有字符的语义信息。单个字符的语义内容错误，对其他步骤的负面影响十分有限。</p><p>在此基础上，我们提出了一种基于语义推理网络的场景文本识别框架，该框架不仅集成了全局语义推理模块(GSRM)，还集成了并行视觉注意模块(PVAM)和视觉语义融合解码器(VSFD)。PVAM的目的是在并行注意机制中提取每个time step的视觉特征，VSFD则用于融合视觉信息和语义信息。</p><p>这篇论文的贡献主要包含三部分：首先，我们提出了一个<strong>全局语义推理模块(GSRM)</strong>来处理全局语义信息。该方法比单向串行语义传输方法具有更好的鲁棒性和有效性。其次，提出了一种新的场景文本识别框架——<strong>语义推理网络(SRN)</strong>，该框架有效地结合了视觉信息和语义信息。第三，SRN是可以端到端训练的，并在<strong>一些baseline中表现SOTA，其中包括常规文本、不规则文本和非拉丁长文本</strong>。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>不会吧不会吧，不会真有人看Related Work吧</p><h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3    Approach"></a>3    Approach</h1><p>SRN是一个端到端的可训练框架，它由四部分组成：backbone、并行视觉注意模块(PVAM)、全局语义推理模块(GSRM)、以及视觉语义融合解码器(VSFD)。对于一个给定的图像输入，首先使用backbone提取二维特征，然后使用PVAM生成$N$个对齐的一维特征$G$，其中每个特征对应于文本中的一个字符，并包含相对应的视觉信息。然后将这$N$个一维特征$G$输入到GSRM中以获取语义信息$S$。将对齐后的视觉特征$G$和语义信息$S$融合在一起，对$N$个字符进行预测。对于小于$N$的文本字符串使用’EOS’填充。SRN的详细结构如下图。</p><p><img src="https://i.loli.net/2020/10/26/73LvbolV6IqyQDH.png" alt="image.png"></p><h2 id="3-1-Backbone-Network"><a href="#3-1-Backbone-Network" class="headerlink" title="3.1    Backbone Network"></a>3.1    Backbone Network</h2><p>在backbone中，我们使用FPN来融合ResNet50的stage-3, stage-4和stage-5层的特征。因此ResNet50+FPN最终输出特征图尺寸为输入图像的$1/8$，通道数为$512$。其灵感来源于non-local mechanisms，我们还采用transformer多头注意网络和一个前馈模块捕获全局空间依赖性。将二维特征图输入到两个堆叠transformer单元中，其中多头注意力的头数为$8$，前馈输出维度为$512$。至此，我们提取出了一个（大概是）$H×W×512$的特征。</p><p><strong>总结：ResNet50 + FPN + 8 head transformer</strong></p><h2 id="3-2-Parallel-Visual-Attention-Module"><a href="#3-2-Parallel-Visual-Attention-Module" class="headerlink" title="3.2    Parallel Visual Attention Module"></a>3.2    Parallel Visual Attention Module</h2><p>注意机制在序列识别中得到了广泛的应用。它可以看作是一种特征对齐的形式，将输入中的相关信息与相应的输出进行对齐。我们使用注意机制生成$N$个特征，每个特征对应文本中的一个字符。现有的注意力方法由于存在一些时间依赖项而导致效率低下。本文提出了一种新的注意方法——<strong>平行视觉注意(PVA)</strong>，通过突破这些障碍来提高效率。</p><p>一般来说，注意机制可以描述如下：给定一个key-value集合和一个query, 计算query与所有keys的相似性。然后使values根据相似性来进行融合。在我们的研究中，key-value集合是输入的二维特征，现有的方法使用隐藏层$H_{t-1}$作为query生成第$t$个特征。为了使计算并行，我们使用读取序号作为query，而不是依赖于时间的$H_{t-1}$。文本中的第一个字符的读取序号为0，第二个字符的读取序号顺序为1，以此类推。我们的PVA可以总结为：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             e_{t,ij}=W^T_e tanh(W_of_o(O_t)+W_vv_{ij})\<br>             \alpha_{t,ij}=\frac{exp(e_{t,ij})}{\sum_{∀i,j}exp(e_{t,ij})}\<br>             \end{array}<br>\right.<br>$$<br>其中$W$均为可训练的权值。$O_t$是每个字符的读取顺序，$f_o$是embedding函数。</p><p>基于PVA的想法，我们设计了并行视觉注意模块(PVAM)用于对齐每个视觉特征和time step。对齐第$t$个time step和视觉特征的过程描述如下：<br>$$<br>g_t=\sum_{∀i,j}\alpha_{t,ij}v_{ij}<br>$$<br>由于<strong>这个计算方法具有时间无关性，PVAM可以在所有time step上并行执行对齐操作</strong>。</p><p>如图所示，所得到的注意图能够正确地注意对应字符的视觉区域，验证了PVAM的有效性。</p><p><img src="https://i.loli.net/2020/10/26/6gR1KYUsrDlXI92.png" alt="image.png"></p><h2 id="3-3-Global-Semantic-Reasoning-Module"><a href="#3-3-Global-Semantic-Reasoning-Module" class="headerlink" title="3.3    Global Semantic Reasoning Module"></a>3.3    Global Semantic Reasoning Module</h2><p>在本节中，我们提出了遵循多路并行传输思想的全局语义推理模块(GSRM)，以克服单向语义上下文传递的缺点。首先我们回顾一下典型的类RNN结构的Bahdanau注意机制中需要最大化的概率公式。可以表示为：<br>$$<br>p(y_1y_2…y_N)=\prod_{t=1}^Np(y_t|e_{t-1},H_{t-1},g_t)<br>$$<br>其中$e_t$为第$t$个label $y_t$的词嵌入。在每个time step，类RNN的方法会参考先前的labels或者预测结果。由于$e_{t-1}$、$H_{t-1}$等信息只能在time step中获取，使得这类方法的只能以序列的方式进行，限制了语义推理的能力，导致推理效率较低。</p><p>为了克服上述问题，我们使用一个时间无关的近似嵌入$e’$来代替真正的嵌入$e$。这种改进可以带来几个好处。1)首先，可以将上式中最后一步的$H_{t-1}$隐藏状态值去除，从而将串行推理过程升级为高效并行推理过程，因为所有的时间依赖项都已经被消除。2)第二，包括前后所有字符在内的全局语义信息都能用来推导当前时刻的语义状态。因此，我们将概率表达式改进如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|f_r(e_1…e_{t-1}e_{t+1}…e_N),g_t)\<br>\approx \prod^N_{t=1}p(y_t|f_r(e’<em>1…e’</em>{t-1}e’<em>{t+1}…e’_N),g_t)<br>$$<br>其中$f_r$表示用于建立全局语义和当前语义信息的函数。如果我们使$s_t=f_r(e_1…e</em>{t-1}e_{t+1}…e_N)$，$s_t$表示第$t$个语义信息的特征，上式可以简化如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|s_t, g_t)<br>$$<br>于此我们提出了GSRM。该结构分为两个关键部分：视觉语义嵌入模块(Visual-to-semantic embedding block)和语义推理模块(semantic reasoning block)。</p><p><img src="https://i.loli.net/2020/10/27/SyL1OrqVf4NGWbh.png" alt="image.png"></p><p><strong>视觉语义嵌入模块</strong>用于生成$e’$，其输入特征已经经过PVAM对每个字符进行对齐。该视觉特征首先输入到一个全连接层和softmax层，并受到交叉熵损失监督。然后使用argmax选出可能性最大的字符进行embedding，得到$e’_t$。</p><p><strong>语义推理模块</strong>用于全局语义推理，相当于上上条公式里的$f_r$。多个transformer单元使模型能够高效地感知全局上下文信息，词语的语义可以通过多个transformer单元隐式建模。最后通过该模块输出每一步的语义特征，受交叉熵损失监督。</p><p>通过交叉熵损失，从语义信息的角度对客观概率进行优化，也有助于减少收敛时间。值得注意的是，在GSRM中，全局语义是并行推理的，这使得SRN比传统的基于注意力的方法运行得更快，特别是在长文本的情况下。</p><h2 id="3-4-Visual-Semantic-Fusion-Decoder"><a href="#3-4-Visual-Semantic-Fusion-Decoder" class="headerlink" title="3.4. Visual-Semantic Fusion Decoder"></a>3.4. Visual-Semantic Fusion Decoder</h2><p>在场景文本识别的同时考虑视觉对齐特征和语义信息是非常重要的。然而视觉和语义属于不同的领域，在不同的情况下，它们在最终序列识别中的权重应该是不同的。受门控单元的启发，我们引入了一些可训练的权重来平衡VSFD中不同领域的特征贡献。其操作方式如下：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             z_t=\sigma (W_z\cdot[g_t,s_t])\<br>            f_t=z_t<em>g_t+(1-z_t)</em>s_t\<br>             \end{array}<br>\right.<br>$$<br>其中$W_z$为可训练的权值，$f_t$为第$t$次融合特征向量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2003.12294&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Towards Accurate Scene Text Recognition with Semantic Reas
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>AdaIN：基于适应性实例归一化的实时任意风格迁移</title>
    <link href="http://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    <id>http://a-kali.github.io/2020/10/12/AdaIN：基于适应性实例归一化的实时任意风格迁移/</id>
    <published>2020-10-12T15:20:11.000Z</published>
    <updated>2020-10-12T15:21:14.171Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1703.06868.pdf" target="_blank" rel="noopener">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></p><p>Github：<a href="https://github.com/xunhuang1995/AdaIN-style" target="_blank" rel="noopener">https://github.com/xunhuang1995/AdaIN-style</a></p><p>PyTorch版代码：<a href="https://github.com/naoto0804/pytorch-AdaIN" target="_blank" rel="noopener">https://github.com/naoto0804/pytorch-AdaIN</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Gatys等人最近引入了一种神经算法，可以将一幅内容(Content)图像以另一幅图像的风格(Style)呈现，实现所谓的<strong>风格迁移(Style Transfer)</strong>。然而，他们的框架需要经历一个缓慢的迭代优化过程，这限制了其实际应用。有人提出用前向神经网络进行快速逼近，以加快神经风格迁移。不幸的是，速度的提高是有代价的：网络通常与一组固定的风格绑定在一起，不能适应任意的新的风格。在本文中，我们提出了一个简单而有效的方法，首次实现了实时任意风格的传输。该方法的核心是一种新的<strong>自适应实例归一化(adaptive instance normalization, AdalN)</strong>层，它将内容特征的均值和方差与风格特征的均值和方差对齐。我们的方法达到了与现有方法相比最快的速度，并且不受预定义样式集的限制。此外，用户可以灵活调控通过这个方法训练出来的模型，如内容样式权衡、样式插值、颜色和空间控制，所有这些都仅使用一个前向神经网络。</p><p><img src="https://i.loli.net/2020/10/09/aEOytThG9LQ5q1b.png" alt="image.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p><a href="http://pdfs.semanticscholar.org/3eeb/f249182614838294f7658da7e8d20c0a1917.pdf" target="_blank" rel="noopener">Gatys等人的开创性工作</a>表明，深度神经网络(DNNs)不仅编码图像的内容，而且编码图像的风格信息。此外，图像风格和内容在某种程度上是可分离的：可以在保留内容的同时改变图像的风格。他们的风格迁移方法足够灵活，可以组合任意图像的内容和样式。但是该方法的优化过程十分缓慢。</p><p>在加速神经迁移方向已经有了许多的研究。部分研究试图训练前向神经网络，通过单次前向传递来执行风格化操作。但这些方法限制了每个网络只能训练单一的风格。最近有一些研究解决了这个问题，但它们要么仍然局限于有限的风格集合，要么比单一风格的传输方法慢得多。</p><p>在这篇论文中中，我们首次提出了能够解决速度与灵活性矛盾的风格迁移算法。<strong>我们的方法可以实时对任意新的风格进行迁移，将基于优化框架的灵活性和前向方法的速度结合在一起</strong>。我们的方法灵感来源于<strong><a href="https://arxiv.org/abs/1701.02096v1" target="_blank" rel="noopener">实例归一化(instance normalization, IN)</a></strong>层，其在前向风格迁移中非常有效。IN通过对特征统计(feature statistics)进行归一化来进行风格归一化，有些研究表明特征统计携带了图像的风格信息。根据这个解释，我们引入了一个简单的IN扩展，即自适应实例归一化(AdaIN)。对于一组内容和风格，AdaIN只需调整内容输入的平均值和方差，就能匹配风格输入的平均值和方差。通过实验，我们发现AdaIN通过传递特征统计量，有效地将前者的内容与后者的风格结合起来。然后，通过将AdaIN输出反向返回到图像空间，解码网络学会生成最终的风格化图像。在实现了高速风格迁移的同时，我们的方法提供了大量的用户控件，不需要对训练过程进行任何修改。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p><strong>Style transfer.</strong> 风格迁移问题起源于非真实感渲染，与纹理合成和转换密切相关。早期的一些方法包括线性滤波器响应的直方图匹配和非参数采样。这些方法通常依赖于低级的统计信息，并且常常无法捕获语义结构。Gatys等通过匹配DNN卷积层的特征统计量，首次展示了令人印象深刻的风格迁移结果。最近，许多研究对风格迁移的算法进行了一些改进。Li和Wand在深度特征空间中引入了一种基于马尔可夫随机场的框架来加强局部模式。Gatys等人提出了控制色彩保存、空间位置的方法。以及风格迁移的规模。Ruder等人通过添加时序约束提高了视频风格迁移的质量。</p><p>Gatys等人的框架基于一个缓慢的优化过程，迭代更新图像，以最小化由内容损失和样式损失。即使是现代GPU也需要几分钟的时间，而移动应用程序中的设备处理速度更慢，难以实现。一个常见的解决方法是用训练最小化相同目标的前馈神经网络来代替优化过程。这些前馈风格的传输方法比基于优化的替代方法大约快三个数量级，为实时应用打开了大门。Wang等人的使用多分辨率架构增强了前馈式传输的粒度。Ulyanov等人提出了提高生成样本质量和多样性的方法。然而，上述前馈方法的局限性在于，每个网络都被绑在一个固定的样式上。为了解决这个问题，Dumoulin等人引入了一个能够编码32种样式及其插值的单一网络。与我们的工作同时，Li等人提出了一种前馈架构，可以合成多达300种纹理和转移16种风格。但是，上述两种方法不能适应训练中没有观察到的任意风格。</p><p>最近，Chen和Schmidt引入了一种前馈方法，借助风格交换层可以传输任意的风格。对于给定内容和风格图像的特征激活，风格交换层将以patch-by-patch的方式将内容特征替换为最匹配的风格特征。然而，他们的风格交换层创造了一个新的计算瓶颈：超过95%的计算花费在512 x 512输入图像的样式交换上。我们的方法允许任意的风格的同时，比他们的方法快1-2个数量级。</p><p>风格迁移的另一个核心问题是使用哪种风格损失函数。Gatys等人的原始框架通过匹配Gram矩阵捕获的特征激活之间的二阶统计量来匹配风格。后来也有研究提出了其它的损失函数，如MRF损失，adversarial 损失，直方图损失，CORAL损失，MMD损失，以及信道平均和方差之间的距离。注意，以上所有的损失函数都是为了匹配风格图像和合成图像之间的一些特征统计。</p><p><strong>Deep generative image modeling.</strong> 有几个图像生成框架可供选择，包括变分自动编码器、自回归模型和生成对抗网络(GANs)。值得注意的是，GANs已经取得了最令人印象深刻的视觉质量。GAN框架的各种改进已经被提出，比如条件生成、多级处理以及更好的训练目标。GANs也被应用于风格迁移和跨域图像生成。</p><h1 id="3-Background"><a href="#3-Background" class="headerlink" title="3    Background"></a>3    Background</h1><h2 id="3-1-Batch-Normalization"><a href="#3-1-Batch-Normalization" class="headerlink" title="3.1    Batch Normalization"></a>3.1    Batch Normalization</h2><p>loffe和Szegedy的开创性地引入了批归一化(BN)层，通过对特征统计进行归一化，显著地简化了前馈网络的训练。BN层的设计初衷是为了加速识别网络的训练，但后来被发现在生成图像模型中也是有效的。对于给定批处理输入x，BN对每个特征通道的均值和标准差进行归一化：<br>$$<br>BN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>其中$\gamma$和$\beta$从数据中习得，$\mu(x)$和$\sigma$是batch的均值和标准差，由每个特征通道的batch size和空间维度独立计算而得：<br>$$<br>\mu_c(x)=\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_c(x))^2+\epsilon}<br>$$<br>BN在训练时使用mini-batch统计，在推理时使用常规的统计代替，造成了训练和推理的差异。为了解决这个问题，最近提出了批重正化，在训练期间逐步使用常规的统计数据。Li等人发现BN的另一个有趣应用：BN可以通过重新计算目标域的常规统计数据来减轻域偏移。</p><h2 id="3-2-Instance-Normalization"><a href="#3-2-Instance-Normalization" class="headerlink" title="3.2    Instance Normalization"></a>3.2    Instance Normalization</h2><p>在原始的前馈风格化方法中，风格迁移网络在每个卷积层之后包含一个BN层。令人惊讶的是，Ulyanov等发现，只需将BN层替换为IN层，就可以得到显著的改善：<br>$$<br>IN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>和BN不同的是，IN的的$\mu(x)$和$\sigma(x)$是对每个通道和每个样本独立计算的：<br>$$<br>\mu_{nc}(x)=\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_{nc}(x))^2+\epsilon}<br>$$<br>另一个区别是，在测试时应用的层不变，而BN层通常使用常规统计代替mini-batch统计。</p><h2 id="3-3-Conditional-Instance-Normalization"><a href="#3-3-Conditional-Instance-Normalization" class="headerlink" title="3.3    Conditional Instance Normalization"></a>3.3    Conditional Instance Normalization</h2><p>Dumoulin等人没有学习单一的仿射参数集$\gamma$和$\beta$，而是提出了条件实例归一化(CIN)层，该层对每种不同的风格$s$学习不同的参数集$\gamma^s$和$\beta^s$：<br>$$<br>CIN(x;s)=\gamma^s \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta^s<br>$$<br>在训练过程中，从一组固定的风格集合$s\in  {1,2,…,S}$(实验中$S = 32$)中随机选取一幅风格图像及其索引$s$。然后将内容图像输入到一个网络中，在CIN层使用对应的$\gamma^s$和$\beta^s$。令人惊讶的是，<strong>有着相同卷积参数、不同仿射参数的多个网络，可以生成完全不同风格的图像。</strong></p><p>与没有归一化层的网络相比，有CIN层的网络需要增加$2FS$的附加参数，其中$F$为网络中feature map的数量。由于附加参数的数量与样式的数量成线性关系，因此很难用这个方法生成非常多的风格。而且，每添加一种新的风格，都需要重新训练一次网络。</p><h1 id="4-Interpreting-Instance-Normalization"><a href="#4-Interpreting-Instance-Normalization" class="headerlink" title="4     Interpreting Instance Normalization"></a>4     Interpreting Instance Normalization</h1><p>尽管IN取得了巨大的成功，但它们对样式转换起作用的原因仍然是难以捉摸的。Ulyanov等人把IN的成功归于其内容图像的不变性。然而，IN发生在特征空间中，因此它应该比在像素空间中进行简单的对比归一化具有更深远的影响。也许更令人惊讶的是IN中的仿射参数可以完全改变输出图像的风格。</p><p>众所周知，DNN的卷积特征统计可以捕捉到图像的风格。Gatys等人使用二阶统计量作为其优化目标，而Li等人最近表明，包括channel-wise的均值和方差在内的其它统计量对风格迁移也是有效的。因此<strong>我们认为IN通过归一化特征统计（即均值和方差），在某种程度上执行了“风格归一化(style normalization)”。于是我们认为网络的特征统计也可以控制生成图像的风格。</strong></p><p>我们分别运行带有IN和BN层的网络来执行单一风格的转换。正如预期的那样，使用IN的模型比BN模型收敛得更快（如下图）。为了检验Ulyanov的解释，我们通过对亮度通道进行直方图均衡化，将所有训练图像归一化到相同对比度。如图(b)所示，IN仍然有效，说明Ulyanov的解释不完全。为了验证我们的假设，我们使用预训练的风格转移网络将所有的训练图像归一化为相同的风格(不同于目标风格)。从图(c)可以看出，在对图像进行了风格归一化后，IN带来的改进就小得多了。另外，使用风格归一化图像训练BN的模型和使用原始图像训练IN的模型收敛速度一样快，表明IN确实执行了一种风格归一化。</p><p><img src="https://i.loli.net/2020/09/25/BJ5SiRZen3bx2Uh.png" alt="image.png"></p><p>由于BN是在一个batch的样本上进行特征统计，可以直观地理解为将一个batch的样本围绕着单一风格进行归一化。然而每一个的样本都有不同的风格，很难将一个batch中所有样本转化成同一个风格。虽然卷积层可能会学会弥补样本之间风格的差异，但也为训练增加了难度。另一方面，IN可以将每个样本的风格归一化为目标风格，网络的其他部分可以在舍弃原有信息风格的同时专注于内容处理，提高了训练速度。CIN成功的原因也很明确：不同的仿射参数可以将特征统计值归一化到不同的值，从而将输出的图像归一化到不同的风格。</p><h1 id="5-Adaptive-Instance-Normalization"><a href="#5-Adaptive-Instance-Normalization" class="headerlink" title="5    Adaptive Instance Normalization"></a>5    Adaptive Instance Normalization</h1><p>如果将输入归一化为由仿射参数指定的单一风格，是否有可能通过自适应仿射变换使其适应任意给定的风格？我们对IN进行了一个简单的扩展。我们称之为自适应实例归一化(AdaIN)。AdaIN接收一个内容输入x和一个样式输入y，并简单地将x的通道平均值和方差与y的平均值和方差匹配。<strong>与BN、IN和CIN不同，AdaIN没有可以学习的仿射参数。相反，它自适应地从风格输入中计算仿射参数</strong>：<br>$$<br>AdaIN(x,y)=\sigma (y)\begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\mu (y)<br>$$<br>相比于IN，我们仅仅是将两个仿射参数替换成了$\sigma (y)$和$\mu (y)$，这两个统计值的依然是在空间位置上进行计算。</p><p>假设存在一个检测特定风格纹路的特征通道。具有这种纹路的风格图像将在该层产生较高的平均激活值。AdaIN产生的输出在保持内容图像的空间结构的同时，对该特征具有同样高的平均激活度。纹路特征可以通过前馈解码器转换到到图像空间。该特征通道的方差可以将更细微的风格信息传递到AdaIN输出和最终输出的图像中。</p><p>简而言之，AdaIN通过迁移特征统计量，即通道方向上的均值和方差，在特征空间中进行风格迁移。</p><h1 id="6-Experimental-Setup"><a href="#6-Experimental-Setup" class="headerlink" title="6    Experimental Setup"></a>6    Experimental Setup</h1><p>如下是我们基于AdaIN的风格迁移网络的概览图：</p><p><img src="https://i.loli.net/2020/09/29/j2ZQMDuv8HtWqhk.png" alt="image.png"></p><h2 id="6-1-Architecture"><a href="#6-1-Architecture" class="headerlink" title="6.1    Architecture"></a>6.1    Architecture</h2><p>我们的风格迁移网络$T$以一个内容图像$c$和一个任意风格的图像$s$作为输入，并合成一个输出图像，该图像重新组合前者的内容和后者的样式。我们采用一种简单的encoder-decoder架构，其中encoder $f$ 固定在预训练VGG-19的前几层（直到relu4_1）。在特征空间中对内容和风格图像进行编码后，我们将这两种特征图输入AdalN层，使内容特征图的均值和方差与风格特征图的均值和方差对齐，生成目标特征图$t$：<br>$$<br>t=AdaIN(f(c),f(s))<br>$$<br>训练一个随机初始化的decoder $g$将$t$映射回图像空间，生成风格化图像$T (c, s)$：<br>$$<br>T(c,s)=g(t)<br>$$<br>decoder大部分是encoder的镜像，所有池化层替换为最近的上采样，以减少棋盘效应。我们在$f$和$g$中使用反射填充（(reflflection padding)来避免边界失真。另一个问题是decoder应该使用IN、BN还是不使用标准化层。如第4节所述，IN将每个样本归为单个样式，而BN将一批样本归一化，以单个样式为中心。当我们希望decoder生成风格迥异的图像时，两者都是不可取的。因此，我们在decoder中不使用归一化层。</p><h2 id="6-2-Training"><a href="#6-2-Training" class="headerlink" title="6.2    Training"></a>6.2    Training</h2><ul><li>Dataset<ul><li>Content: MS-COCO</li><li>Style: WikiArt</li></ul></li><li>Sample size: 80000</li><li>Optimizer: adam</li><li>Batch size: 8 content-style image pairs</li><li>Resize: 512, RandomCrop: 256×256</li><li>Model: VGG-19</li><li>Loss: $L=L_c+\lambda L_s$</li></ul><p>损失函数为内容损失和风格损失的加权和。内容损失是目标特征与输出图像特征之间的欧氏距离。我们使用AdaIN输出$t$作为内容目标，而不是内容图像：<br>$$<br>L_c=|f(g(t))-t|<em>2<br>$$<br>由于AdaIN层只迁移了风格特征的平均值和标准差，所以我们的风格损失只与这些统计数据匹配。虽然我们发现常用的Gram矩阵损失可以产生类似的结果，但我们还是使用IN统计，因为它在概念上更清晰。<br>$$<br>L_s=\sum^L</em>{i=1}|\mu (\phi_i(g(t)))-\mu(\phi_i(s))|<em>2+\sum^L</em>{i=1}|\sigma (\phi_i(g(t)))-\sigma(\phi_i(s))|_2<br>$$<br>其中$\phi$表示VGG-19中用于计算风格损失的层。在我们的实验中，我们在relu1_1, relu2_1, relu3_1,  relu4_1中使用了相等的权重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1703.06868.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Arbitrary Style Transfer in Real-time with Adaptive In
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="风格迁移" scheme="http://a-kali.github.io/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
</feed>
