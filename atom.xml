<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-03-03T09:22:57.794Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文笔记：使用RNN进行情感识别</title>
    <link href="http://a-kali.github.io/2020/03/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/03/03/论文笔记：使用RNN进行情感识别/</id>
    <published>2020-03-03T08:31:11.000Z</published>
    <updated>2020-03-03T09:22:57.794Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1701.08071" target="_blank" rel="noopener">Emotion Recognition From Speech With Recurrent Neural Networks</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在<strong>自动语音识别(Automated Speech Recognition, ASR)</strong>任务中，语音被模型转化成文字。但是在人们的对话过程中除了文本以外还有其它重要的信息，比如<strong>语调、情感、响度</strong>。这些信息在语音理解中亦扮演者十分重要的角色。本文将围绕其中“情感”部分进行概述，即<strong>语音情感识别(Speech Emotion Recognition, SER)</strong>。</p><p>在语音情感识别方面存在一些难点：</p><ol><li><strong>情感是主观的</strong>，不同人对于同一段语音，理解出的情感可能不同。</li><li><strong>同一段语音可能包含多种情感</strong>。（可以通过<a href="https://zhuanlan.zhihu.com/p/86745594" target="_blank" rel="noopener">CTC损失函数</a>解决）</li><li><strong>数据来源</strong>：从电影中截取的语音可能和现实中存在偏差。通常会找专业演员来演绎各种情感来制造数据。</li></ol><h1 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2    Related works"></a>2    Related works</h1><p>大部分文献将语音情感识别视为一个分类问题，对每一个utterance分配一个label。utterance即为一小段语音，是语音的最小单元。</p><p>在深度学习之前，大多研究提取底层的手工特征，用传统分类器进行分类，比如HMM（隐马尔可夫模型）或GMM（高斯混合模型）。</p><p>深度学习出现后，有人把utterance分帧计算低层特征，用三层全连接层，对输出概率聚合成utterance水平的特征（用简单的统计量，比如最大值，最小值，平均值等），最后用ELM（Extreme Learning Machine）分类。</p><p>后面出现了纯深度学习和端到端的架构模型。有人使用Attention CNN，有人用DBN，还有人用迁移学习把语音识别的任务（数据集）迁移到语音情感识别中。</p><h1 id="3-Data-and-preprocessing"><a href="#3-Data-and-preprocessing" class="headerlink" title="3    Data and preprocessing"></a>3    Data and preprocessing</h1><p>IEMOCAP（Interactive Emotional Dyadic Motion Capture）被选作数据集，因为它有详尽的获取方法，免费的学术许可，较长的语音时长和良好的标注。</p><p>大约包括12个小时，含有视频，音频和人脸关键点的数据。由南加利福尼亚大学戏剧系的10位专业演员表演所得。评估者对每个utterance给出评价（10个情感选项），当一半以上的评估者对某个utterance的评估一致时，该utterance才分配到评估的感情。本文中选取其中4种情感用于分析（生气，兴奋，中立和伤心），只有这些样本才被考虑到本文工作中，下图是标签分布。</p><p><img src="https://img2018.cnblogs.com/blog/1160281/201811/1160281-20181113191458804-356625669.png" alt="img"></p><p>原始信号的采样率是16kHz，直接使用计算量很大，需要尽量保持信息的同时减小计算量。本文对utterance进行分帧，帧长为200ms，帧移为100ms，在帧上计算声学特征（用了哪些声学特征见下文介绍），然后把这些特征合在一起作为utterance的特征输入到模型。关于帧长的选取，论文从30ms到200ms都做过实验发现效果差别不大，而较长的帧可以导致比较少的帧，能减小计算量，所以使用了200ms。</p><p>对于语音信号的特征主要有三种，一是声学特征，也就是声波的一些属性；二是音律特征，指的是停用词，韵律（押韵，平仄），响度，这个特征依赖于说话人，所以没有用这类特征；三是语义学特征，就是语音对应的文字内容的信息。</p><p>本文只使用了声学特征，使用的是python库PyAudioAnalysis的API提供的34个特征，主要包括3个时域特征（过零率，能量，能量熵），5个谱特征，13个MFCC特征，13个音阶特征。也就是一帧的声音用34维的向量来表示。</p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4    Approach"></a>4    Approach</h1><p>因为一个utterance只对应一个标签，但是有很多帧，有些帧是不包含情感的，所以输入序列和输出序列难以一一对应，为了应对这个问题，可以使用CTC（Connectionist Temporal Classification）的方法。</p><p>CTC模型中的LSTM的输入时间步和输出时间步T为78，因为每个语音样本划分成了78帧。情感标签有4个，加上空白符，得到大小为5的字符集合。真实输出只有一个标签，所以在这些长度为78的输出序列中，经过B转换后能得到一个真实情感标签的那些序列才是我们要的序列，用CTC的方法来使得这些序列产生的概率最大。</p><p>注：文本大量参考这篇文章<a href="https://www.cnblogs.com/liaohuiqiang/p/9954088.html" target="_blank" rel="noopener">论文笔记：Emotion Recognition From Speech With Recurrent Neural Networks</a>，以自己的语言整理一遍笔记，旨在巩固记忆加深理解，并非完全原创。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1701.08071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Emotion Recognition From Speech With Recurrent Neural Netw
      
    
    </summary>
    
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="RNN" scheme="http://a-kali.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>DAIC-WOZ抑郁评估数据格式</title>
    <link href="http://a-kali.github.io/2020/02/09/DAIC-WOZ%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/"/>
    <id>http://a-kali.github.io/2020/02/09/DAIC-WOZ抑郁评估数据格式/</id>
    <published>2020-02-09T14:52:27.000Z</published>
    <updated>2020-02-11T13:02:26.459Z</updated>
    
    <content type="html"><![CDATA[<p>DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的数据。该代理能够自动对人们进行访谈，并在语言(verbal)和非语言(nonverbal)指标上识别精神疾病。收集的数据包括音频和视频记录以及大量的的问卷回答；这部分语料库包括一个名为Ellie的动画虚拟面试官主持的Oz访谈，由另一个房间里的真人面试官控制。数据已被转录和注释的各种语言的和非语言的特征。</p><h1 id="Data-description"><a href="#Data-description" class="headerlink" title="Data description"></a>Data description</h1><p>数据包中包含<strong>编号300-492、共189个数据样本</strong>（其中 342,394,398,460 因技术原因被移除）。数据包格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Pack\</span><br><span class="line">300_P</span><br><span class="line">301_P</span><br><span class="line">...</span><br><span class="line">492_P</span><br><span class="line">util</span><br><span class="line">documents</span><br><span class="line">train_split.csv</span><br><span class="line">dev_split.csv</span><br><span class="line">test_split.csv</span><br></pre></td></tr></table></figure><p>部分样本需要提醒：</p><ul><li>373 - 在5:52-7:00有一个中断，助手进入房间解决一个小的技术问题，会议继续进行并结束。</li><li>444 - 在4:46-6:27左右有一个中断，参与者的手机响了，助手进入房间帮助他们关机。</li><li>451,458,480 - 会话在技术上是完整的，但是缺少了Ellie(虚拟人类)部分的记录。参与者的成绩单仍然包括在内，但没有面试官的问题。</li><li>402 - 视频结尾被删减了约2分钟。</li></ul><p><strong>train_split_Depression_AVEC2017.csv</strong>：该文件包含参与者id、PHQ8二进制标签(PHQ8得分&gt;= 10)、PHQ8得分和参与者性别，以及PHQ8问卷的每个问题的唯一回答。详细信息参见documents文件夹下的scherer_etal2015_VowelSpace.pdf。</p><p><strong>dev_split_Depression_AVEC2017.csv</strong>：同上。</p><p><strong>test_split_Depression_AVEC2017.csv</strong>：仅包含id和性别。</p><p>每个样本文件夹下文件组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">XXX_P\ </span><br><span class="line">  XXX_CLNF_features.txt </span><br><span class="line">  XXX_CLNF_features3D.txt </span><br><span class="line">  XXX_CLNF_gaze.txt </span><br><span class="line">  XXX_CLNF_hog.bin </span><br><span class="line">  XXX_CLNF_pose.txt </span><br><span class="line">  XXX_CLNF_AUs.csv   </span><br><span class="line">  XXX_AUDIO.wav </span><br><span class="line">  XXX_COVAREP.csv </span><br><span class="line">  XXX_FORMANT.csv </span><br><span class="line">  XXX_TRANSCRIPT.csv</span><br></pre></td></tr></table></figure><p>util文件夹组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">util\ </span><br><span class="line">  runHOGread_example.m </span><br><span class="line">  Read_HOG_files.m</span><br></pre></td></tr></table></figure><h1 id="File-description-and-feature-documentation"><a href="#File-description-and-feature-documentation" class="headerlink" title="File  description  and  feature  documentation"></a>File  description  and  feature  documentation</h1><p>这部分表述的是每个样本文件夹下各个文件的作用。</p><h2 id="1-CLNF-framework-output"><a href="#1-CLNF-framework-output" class="headerlink" title="1    CLNF framework output"></a>1    CLNF framework output</h2><p>这部分是由CLNF人脸关键点检测算法输出的数据，包括以下文件：</p><ul><li>XXX.CLNF_features.txt<ul><li>包含68个2D人脸关键点；</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success, x0,  x1,…,  x67,  y0,  y1,…, y67。分别表示 帧、时间点、置信度、是否检查成功，各个关键点坐标；</li><li>值之间由逗号分隔，虽然后缀是txt但应该当作csv文件处理。</li></ul></li><li>XXX_CLNF_AUs.csv<ul><li>AU表示Action Unit，是<a href="http://www.360doc.com/content/15/0128/13/10690471_444446832.shtml" target="_blank" rel="noopener">面部表情编码系统</a>(Facial Action Coding System, FACS)的运动单元。每一个AU代表一个表情元素；</li><li>文件格式：frame,  timestamp,  confidence,  success,  AU01_r,  AU02_r, AU04_r,  AU05_r,  AU06_r, AU09_r,  AU10_r,  AU12_r,  AU14_r, AU15_r,  AU17_r,  AU20_r,  AU25_r,  AU26_r,  AU04_c, AU12_c,  AU15_c,  AU23_c,  AU28_c,  AU45_c。其中AUX_r表示该面部包含该AU的概率，而AUX_c则用二值表示是否包含该AU。</li></ul></li><li>XXX.CLNF_features3D.txt<ul><li>包含68个3D人脸关键点；</li><li>格式与2D的类似，只是多了个坐标轴。以摄像机为坐标(0,0,0)，单位为毫米。</li></ul></li><li>XXX.CLNF_gaze.txt<ul><li>文件包含4个视线向量。前两个表示以世界为坐标空间，双眼的视线向量；后两个表示以头为坐标空间，双眼的视线向量。</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success,  x_0,  y_0,  z_0,  x_1,  y_1,  z_1, x_h0,  y_h0,  z_h0,  x_h1,  y_h1,  z_h1</li></ul></li><li>XXX.CLNF_hog.bin<ul><li>Felzenswalb’s  HoG</li></ul></li><li>XXX.CLNF_pose.txt<ul><li>pose文件包含两个坐标，X,Y,Z是位置坐标，Rx,Ry,Rz是头部旋转坐标。位置是以毫米为单位的世界坐标，旋转是以弧度和欧拉角为单位的(为了得到一个合适的旋转矩阵，使用R= Rx * Ry * Rz)。</li><li>文件格式：frame_number,  timestamp(seconds),  confidence,  detection_success,  X,  Y,  Z,  Rx,  Ry,  Rz</li></ul></li></ul><h2 id="2-Audio-file"><a href="#2-Audio-file" class="headerlink" title="2    Audio file"></a>2    Audio file</h2><ul><li>XXX_AUDIO.wav<ul><li>耳机录音频率为16kHz。音频文件中可能包含少量虚拟面试官的信息，在处理时使用记录文件(transcript files)来缓解这个问题。</li></ul></li></ul><h2 id="3-Transcript-file"><a href="#3-Transcript-file" class="headerlink" title="3    Transcript file"></a>3    Transcript file</h2><ul><li>XXX_TRANSCRIPT.csv</li></ul><h2 id="4-Audio-features"><a href="#4-Audio-features" class="headerlink" title="4    Audio features"></a>4    Audio features</h2><ul><li>XXX_COVAREP.csv</li><li>XXX_FORMANT.csv</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的
      
    
    </summary>
    
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>3D点云深度学习综述之 Shape Classification</title>
    <link href="http://a-kali.github.io/2020/02/08/3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E4%B9%8B-Shape-Classification/"/>
    <id>http://a-kali.github.io/2020/02/08/3D点云深度学习综述之-Shape-Classification/</id>
    <published>2020-02-08T15:45:21.000Z</published>
    <updated>2020-02-09T14:57:27.169Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1    摘要"></a>1    摘要</h1><p>3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术，已经成功地应用于解决各种二维视觉问题。然而，由于使用深度神经网络处理点云所面临的独特挑战，对点云的深度学习仍处于起步阶段。最近，点云上的深度学习变得越来越流行，人们提出了许多方法来解决这一领域的不同问题。目前点云分类方法可以主要被分为两类：<strong>基于投影的网络（Projection-based Networks）</strong>和<strong>点基网络（Point-based Networks）</strong>。本文将从这两个方向对用于点云分类的神经网络方法进行了综述。</p><p><img src="https://i.loli.net/2020/02/09/KUAF1fT6tzu3nVl.png" alt="64NDV_C@`6_PE_XEMYT9OL6.png"></p><h1 id="2-基于投影的网络"><a href="#2-基于投影的网络" class="headerlink" title="2    基于投影的网络"></a>2    基于投影的网络</h1><p>基于投影的方法将三维点云投影到不同的表达模式中(例如：多视图、体积表示)，用于特征学习和形状分类。</p><h2 id="2-1-多视图表示"><a href="#2-1-多视图表示" class="headerlink" title="2.1    多视图表示"></a>2.1    多视图表示</h2><p>多视图（Multi-view）方法通常<strong>提取一个3D对象从多个角度投影的特征，之后再通过某种方法将多个特征融合起来</strong>。而如何将视图特征融合起来则是这个方法的关键。MVCNN开创了这种方法的先例，其简单地将多个视图的特征max-pools到一个全局描述符（global descriptor）中。而max-pooling仅仅保留了单一视图的最大值元素，损失了大量信息。MHBN在此之上进行改进，通过双线性池化（bilinear pooling）来得到一个全局的描述符。后续有许多网络进行了进一步的改进，在此不一一赘述。</p><h2 id="2-2-体积表示"><a href="#2-2-体积表示" class="headerlink" title="2.2    体积表示"></a>2.2    体积表示</h2><p>早期的研究者们通常<strong>使用3D卷积网络对基于3D点云的体积表示（Volumetric representation）进行特征提取</strong>。Daniel等人提出了一种体积占用网络VoxNet来实现3D对象识别。Wu等人提出了一种基于卷积深度信念的3D ShapeNets来学习不同3D形状的点分布。在体素网格上，三维形状通常由二进制变量的概率分布来表示。虽然取得了令人鼓舞的性能，但这些方法无法很好地扩展到稠密的三维数据，因为计算和内存占用是随分辨率的立方增长的。为此，引入了层次结构和紧凑的图结构(如八叉树)来降低这些方法的计算和内存开销。Wang等人提出了一种基于八叉树的OctNet来进行三维形状分类。将在最细叶八分区采样的三维模型的平均法向量输入网络，将3D- cnn应用于被三维形状表面占据的八分区。与基于密集输入网格的基线网络相比，OctNet对高分辨率点云的内存和运行时间要求要少得多。leet等人提出了一种称为点网格的混合网络，它集成了点和网格表示来实现高效的点云处理。在每个嵌入式网格单元中采样的点的数量是恒定的，这使得网络可以通过使用3D卷积来提取几何细节。</p><h1 id="3-点基网络"><a href="#3-点基网络" class="headerlink" title="3    点基网络"></a>3    点基网络</h1><p>点基网络使用最原始的点作为网络的输入。由于其能保留更多的原始特征，点基网络成为当前3D点云识别的主流方法。该方法可以被分为<strong>点级多层感知机（Pointwise MLP）、卷积神经网络、图神经网络、基于数据索引网络（Data indexing-based networks）和其它神经网络</strong>。</p><h2 id="3-1-点级多层感知机"><a href="#3-1-点级多层感知机" class="headerlink" title="3.1 点级多层感知机"></a>3.1 点级多层感知机</h2><p>这些方法<strong>使用MLPs对每个点独立建模，然后使用对称函数聚合全局特征</strong>，如图所示。由于对称函数的存在，这类网络可以保证三维点云的无序性。然而，三维点之间的几何关系并没有得到充分的考虑。</p><p><img src="https://i.loli.net/2020/02/09/8O26rgyBF3i5UvJ.png" alt="FK_UYCNROC_~JTZ_`B`C_KU.png"></p><p>这个方向的开山之作是PointNet，其使用max-pooling聚合多个MLP提取的特征。该网络独立地提取每个点的特征，没有考虑到点之间的结构信息。分层网络PointNet++于此被提出用来捕获邻近点之间的几何结构信息。</p><p>由于其简单性和强大的表示能力，许多网络都是基于PointNet开发的。Achlioptas等人提出了一种深度自编码网络学习点云表示。它的编码器遵循PointNet的设计，使用5个一维卷积层、ReLU非线性激活、批处理标准化和最大池化操作独立地学习点特征。在PATs(Point Attention Transformers)中，每个点都由其绝对位置和相对于其相邻点的相对位置来表示,然后利用GSA(Group Shuffle Attention)来捕获点之间的关系，并建立了一个排列不变、可微、可训练的端到端GSS(Gumbel Subset Sampling)层来学习层次特征。Mo-Net的架构类似于PointNet，但它采用有限的一组矩作为其网络的输入。PointWeb也是在PointNet++上构建的，它使用局部邻居的上下文来使用自适应特性调整(Adaptive Feature Adjustment, AFA)改进点特性。Duan等人提出了一种结构关系网络(SRN)，利用MLP来学习不同局部结构之间的结构关系特征。Lin等人通过为PointNet学习的输入空间和函数空间构造一个查找表来加速推理过程。在中等大小的机器上，ModelNet和ShapeNet数据集上的推理时间加快了1.5 ms，是PointNet的32倍。SRINet首先投影一个点云来获得旋转不变量表示，然后利用基于PointNet的backbone来提取全局特征，并使用基于图的融合来提取局部特征。</p><h2 id="3-2-卷积网络"><a href="#3-2-卷积网络" class="headerlink" title="3.2    卷积网络"></a>3.2    卷积网络</h2><p>与二维网格结构(如图像)定义的卷积核相比，三维点云的卷积核由于点云的不规则性而难以设计。根据卷积核的类型，目前的三维卷积网络可以分为<strong>连续卷积网络和离散卷积网络</strong>，如图所示。</p><p><img src="https://i.loli.net/2020/02/09/YNDcs95ty12ig6p.png" alt="_OW`QQ_M_`4TFGIAJRAWGHW.png"></p><h3 id="3-2-1-3D连续卷积网络"><a href="#3-2-1-3D连续卷积网络" class="headerlink" title="3.2.1    3D连续卷积网络"></a>3.2.1    3D连续卷积网络</h3><p><strong>3D连续卷积在连续空间上定义卷积核，其邻近点的权重与中心点的空间分布有关。</strong>3D连续卷积可以被解释为点子集的加权线性组合，通常使用MLP学习每个点的权重。</p><p>例如 RS-CNN 的关键层 RS-Conv 需要点的子集在某种程度上作为其输入，使用MLP学习局部点之间从低级关系（如Euclidear距离和相对位置）到高级关系之间的映射来实现卷积。</p><p>一些方法还使用现有的算法来执行卷积。在PointConv中，卷积被定义为相对于重要抽样的连续三维卷积的蒙特卡罗估计。卷积核由一个加权函数(通过MLP层学习)和一个密度函数(通过核化密度估计和MLP层学习)组成。为了提高记忆和计算效率，将三维卷积进一步简化为矩阵乘法和二维卷积两种运算。在相同的参数设置下，其内存消耗可减少约64倍。类似的网络还有MCCNN、SpiderCNN、PCNN、KPConv。</p><p>而一些方法则被提出用于解决三维卷积网络所面临的旋转等变问题。Esteves等人提出了以多值球函数为输入，学习旋转等变表示的三维球面卷积神经网络(Spherical CNN)。利用球面谐域内的锚点对谱进行参数化，得到局部卷积滤波器。张量场网络(Tensor field networks)被提出用于定义点积运算，它是一个可学习的径向函数和球面调和函数的乘积，这两个函数对于点的三维旋转、平移和排列是局部等价的。SPHNet以PCNN为基础，通过在体积度量函数的卷积过程中加入球谐核实现旋转不变性。</p><h3 id="3-2-2-3D离散卷积网络"><a href="#3-2-2-3D离散卷积网络" class="headerlink" title="3.2.2    3D离散卷积网络"></a>3.2.2    3D离散卷积网络</h3><p><strong>3D离散卷积在规则网格上定义卷积核，其中相邻点的权值与相对于中心点的偏移量相关。</strong></p><p>Hua等人将非均匀三维点云转化为均匀网格，并在每个网格上定义卷积核。与2D卷积(为每个像素分配权重)不同，3D卷积核为落入相同网格的所有点分配相同的权重。对于给定的点，位于同一网格上的所有相邻点的平均特征是从上一层计算出来的。然后对所有网格的平均特征进行加权求和，得到当前层的输出。Lei等人定义了一个球面卷积核，方法是将一个三维球面邻近区域划分为多个容量容器，并将每个容器与一个可学习的加权矩阵相关联。一个点的球面卷积核的输出由相邻点的加权激活值的平均值的非线性激活决定。类似的神经网络还有GeoConv、PointCNN、InterpConv、RIConv、A-CNN、ReLPV、SFCNN等。</p><h2 id="3-3-图神经网络"><a href="#3-3-图神经网络" class="headerlink" title="3.3    图神经网络"></a>3.3    图神经网络</h2><p><strong>基于图的网络将点云中的每个点视为一个图的顶点，并基于每个点的邻居为图生成有向边。然后在空间或光谱域中进行特征学习。</strong>典型的基于图的网络如图所示。</p><p><img src="https://i.loli.net/2020/02/09/JkrwP75SA2TaXCd.png" alt="MLVH~H__VJDM32NKGZ4_7LX.png"></p><h2 id="3-4-基于数据索引的网络"><a href="#3-4-基于数据索引的网络" class="headerlink" title="3.4    基于数据索引的网络"></a>3.4    基于数据索引的网络</h2><p>这类网络是<strong>基于不同的数据索引结构(如八叉树和kd-tree)构建的。在这些方法中，点特征由沿着树从叶节点到根节点的层次学习得到。</strong>其典型代表作有Kd-Net、3DContextNet、SO-Net等。</p><h1 id="4-性能对比"><a href="#4-性能对比" class="headerlink" title="4    性能对比"></a>4    性能对比</h1><p><img src="https://i.loli.net/2020/02/09/lzaiosyh2jkuqr9.png" alt="ST8DMA9EJ2DE__N48_9HN@F.png"></p><p>大概可以看出基于卷积的GeoCNN表现最佳。</p><p>参考论文：Deep Learning for 3D Point Clouds: A Survey</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-摘要&quot;&gt;&lt;a href=&quot;#1-摘要&quot; class=&quot;headerlink&quot; title=&quot;1    摘要&quot;&gt;&lt;/a&gt;1    摘要&lt;/h1&gt;&lt;p&gt;3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="3D点云" scheme="http://a-kali.github.io/tags/3D%E7%82%B9%E4%BA%91/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multi-level Attention network using text, audio and video for Depression Prediction</title>
    <link href="http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/"/>
    <id>http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/</id>
    <published>2020-02-05T02:18:16.000Z</published>
    <updated>2020-04-07T13:06:17.571Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一种常见的心理疾病，严重影响心理和身体健康，甚至可能导致失去生命。由于缺乏诊断抑郁症的有效手段，越来越多的人对通过行为线索来自动诊断以及阶段预测抑郁症感兴趣。摘要提出了一种<strong>基于多级注意的多模态抑郁症预测网络</strong>，该网络融合了音频、视频和文本模式的特征，同时学习模式内和模式间的相关性。多层次的注意力通过选择每个模式中最具影响力的特征来加强整体学习。我们进行了详尽的实验，为音频、视频和文本模式创建不同的回归模型。建立了几种不同构型的融合模型，分析了各特征和模态的影响。就均方根误差而言，我们比当前基准高出17.52%。</p><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2    Related Works"></a>2    Related Works</h1><p>在[28]中，作者对融合技术在抑郁症检测中的应用进行了综述。他们还提出了一种基于计算语言学的多模态融合检测方法。在[25]中，作者基于上下文感知特征生成技术和端到端可训练的深度神经网络对临床访谈语料库数据集的抑郁水平进行了分析。他们进一步在变压器网络中注入基于主题建模的数据增强技术。Zhang等人发布了一个用于人类行为分析的多模态自发情感语料库[44]。面部表情由3D动态测距、高分辨率视频采集和红外成像传感器捕捉。除了面部环境，还会监测血压、呼吸和脉搏率，以判断一个人的情绪状态。利用AVEC挑战中发布的数据，对音频、视频和生理参数进行调查，以观察受试者情绪状态的关键发现。在[30]中，作者融合了音频、视觉和文本线索来获取多媒体内容中的情感。他们利用特征和决策级融合技术来进行有效的决策。在[2]中，作者利用副语言、头部姿势和眼睛凝视进行多模式抑郁检测。在选定特征的统计检验的帮助下，推理机将受试者分为抑郁和健康两类。当结合多种模式时，了解每种模式在任务预测中的贡献是很重要的，注意力网络可以用来研究其相对重要性。在这篇论文中，我们在每个情态中使用注意来理解情态中低层或深层特征的相对重要性。在融合三种模式的同时，我们还使用了注意力层，并学习了注意力的权重，从而找到每种模式的重要性比例。Querishi等人的论文[32]是唯一一篇在使用数据集子集和在一层应用注意力方面最接近我们的论文。通过在多个层次上使用多层注意力，我们能够获得比它们更好的结果，并且由于注意力操作，网络的计算成本更低，从而最小化了框架的测试时间。</p><h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3    Methodology"></a>3    Methodology</h1><h2 id="3-1-Text-Modality"><a href="#3-1-Text-Modality" class="headerlink" title="3.1    Text Modality"></a>3.1    Text Modality</h2><p>由于有几个参与者使用的是英语口语词汇，所以我们对这些词汇进行了修改，用原始的完整词汇替换这些词汇，否则，在训练语言建模或其他预测的神经网络时，这些词汇就会变成不存在于词向量中的词汇。我们使用预训练的通用语句编码器来获得语句嵌入。为了获得常数大小的张量，我们零补齐短句，并有一个常数数量的时间步为400。每个语句嵌入向量的长度为512，使得最后的数组维数为(400,512)。我们使用两层叠加的双向长短时记忆网络结构，以句子嵌入为输入，以PHQ分数为输出，训练语音转录的回归模型。每个BLSTM层有200个隐藏单元，其中第一层的前向层的每个隐藏单元的输出连接到第二层的前向隐藏单元的输入。同样的连接也建立在每一个隐藏单元的后向层，以创建堆叠。这两层BLSTM在每一步的输出为(batchsize,400)，并将其作为输入发送到前馈层进行回归。我们保持前馈层的节点数为(500,100,60,1)，并使用ReLU作为激活函数。</p><h2 id="3-2-Audio-Modality"><a href="#3-2-Audio-Modality" class="headerlink" title="3.2    Audio Modality"></a>3.2    Audio Modality</h2><p>对于音频模态，我们使用不同的音频特征(低级特征及其功能)创建模型。在功能(functional)方面，变分的算术平均值和协效率被应用于底层特征，并作为底层特征之上的知识抽象。语音音色是由诸如梅尔频率倒谱系数(MFCC)这样的LLD特征编码的，研究表明，低阶MFCC在情感预测和副语言语音分析任务中更重要。eGeMAPS特征集包含88个特征，包括GeMAPS特征集及其频谱特征和功能。GeMAPS特性包括频率相关特性(音高、抖动、共振峰)，能量相关功能(微光，响度，谐波噪声比)，光谱参数(α比，哈马伯格比)，谐波以及六个时序特征相关的语音比率。除了上述的这些低电平特征外，音频样本的高维深度表示是通过将音频通过深度频谱和VGG网络来提取的。这一特征在本文的其余部分被称为深度densenet特征。</p><p>对于音频特征，我们的实验只考虑了参与者所说的向量的跨度。每个特征都是挑战数据的一部分，它们有不同的采样率。功能性音频和深度densenet特征采样频率为1Hz，而BoAW采样频率为10Hz，LLD采样频率为100Hz。低级MFCC特征长度为39，低级eGEMAPS长度为23，总时间步长为140500。而功能的长度分别为78和88，时间步长分别为1300和1410。BoAW-MFCC和BoAW-eGEMAPS的特性长度为100，每个特性的时间步长为14050。深度densenet特征长度为1920维，时间步长为1415步。</p><p>在单独的音频通道，我们训练了另一个堆叠的BLSTM网络，它有两层，每层有200个隐藏单元。我们将最后一层的输出作为激活函数传递给多层感知器，每一层(500,100,60,1)个节点在级数上，并以ReLU作为激活函数。</p><h2 id="3-3-Visual-Modality"><a href="#3-3-Visual-Modality" class="headerlink" title="3.3    Visual Modality"></a>3.3    Visual Modality</h2><p>对于挑战数据集中的视频特征，我们对低级特征和LLD的功能进行了实验。由于深度LSTM网络也可以从数据(如功能和更抽象的信息)中学习类似的特征，所以我们选择使用LLD，因为它包含的信息比其平均值和标准差更多。每个用于姿态、注视和面部动作单元(FAU)的LLD特征都以10Hz采样。这些特征的长度分别为6、8、35，都有15000个时间步长。在挑战数据中还提供了BoVW，它的长度为100，有15000个时间步长。我们使用这些特性来训练每个特征的模型，所有特征都使用一个包含200个隐藏单元的BLSTM单层，然后是一个maxpooling和回归器。我们尝试了各种不同的组合，比如所有输出的和、输出的平均值，还使用了maxpooling作为三个备选方案，但是maxpooling效果最好，所以我们在LSTM输出上使用了maxpooling。</p><h2 id="3-4-Fusion-of-Modalities"><a href="#3-4-Fusion-of-Modalities" class="headerlink" title="3.4    Fusion of Modalities"></a>3.4    Fusion of Modalities</h2><p>早期融合(early fusion)需要消耗大量的计算资源，当使用神经网络训练时可能导致过拟合，因此，<strong>后期融合(late fusion)和混合融合(hybrid fusion)</strong>模型变得更加普遍。我们提出了一种<strong>基于多级注意力机制的网络，该网络学习每个特征的重要性，并对它们进行相应的加权，从而实现更好的早期融合和预测</strong>。这样的注意力网络让我们了解到哪种模态的特征对学习更有影响。它还能表示出各个模态对预测结果准确度所做出的贡献比例。</p><p>在融合的过程中，我们在每个通道之间进行了多次实验。首先，我们融合了视频模态的LLD。我们获取注视、姿势和面部动作单元特征，将它们通过包含200个BLSTM神经元的单层网络传递进行前向传播，并对它们施加注意力。注意层的输出通过另一个具有200个单元的BLSTM层。我们对LSTM的输出进行全局最大池化，并通过128个隐藏单元的网络前向传播。</p><p>在第二个模型中，我们使用一个类似的网络将视频LLD与BoVWs相结合，该网络由包含200个隐藏单元的BLSTM层、注意力层和另一个BLSTM组成，然后通过一个前馈层进行回归。</p><p>第三个融合模型将视频模态的注意向量输出和文本模态的输出进行合并(combine)，并在视频回归器之前通过一个堆叠的BLSTM和一个注意力层。</p><p>第四个融合模型同时使用音频和文本模态。我们再次在每个模态中获取注意层的输出，并构建一个混合融合网络，但通过两条路径传递它们。在第一种路径中，每个模态的注意力层输出被拼接起来输入注意力层；另一条路径通过两个注意层的输出通过一个堆叠的BLSTM，该BLSTM有2层，每层有200个单元。在堆叠的BLSTM层上应用一个注意层，该输出被馈送到128个隐藏单元的前馈网络。由于使用了文本特征，因此比单独使用音频特征的模型具有更好的性能。</p><p>我们的第五个融合模型同时使用视频和文本模态，这里我们再次在视频的每个子模态上使用注意力层，然后使用另一个注意网络将它与文本模态合并。令人惊讶的是，这种融合的结果与音频和文本模态融合非常相似，学习曲线也非常相似。</p><p>我们的第六个也是最终融合模型使用了所有的模态。我们使用基于注意力的视觉模态来获得一个包含128个单元的向量，同时使用基于注意力的音频模态来获得一个128单元向量，随后从文本模态中提取信息并从中得到128位(bit)的向量。我们再次在这三种模态上使用另一个注意力层，将它们融合在一起，回归到PHQ8评分。收敛后，视频、音频和文本的注意率分别为[0.21262352, 0.21262285, 0.57475364]。</p><h1 id="4-Results"><a href="#4-Results" class="headerlink" title="4    Results"></a>4    Results</h1><p>本节详细介绍了所有回归模型的结果及其融合研究。由于测试数据的标签在挑战中不可用，所以我们在验证(dev)分区上展示了大部分结果。测试分区上的唯一结果来自基于文本的模型，我们使用该模型提交了一份报告，并从挑战中获得了所有的分数。</p><h2 id="4-1-Results-from-Text-Modality"><a href="#4-1-Results-from-Text-Modality" class="headerlink" title="4.1    Results from Text Modality"></a>4.1    Results from Text Modality</h2><p>在E-DAIC(挑战数据)和DAIC-WoZ数据的测试集上，基于注意力的BLSTM网络在文本转录方面的训练效果优于其他模态。这与临床医生的观察一致，即<strong>语言内容是一个重要的标志，具有明确的特征，可能直接影响患者处于哪个抑郁阶段</strong>。我们在验证集的根均方误差(RMSE)为4.37 。在测试集上，基于文本的模型能够实现平均绝对误差(MAE)为4.02，RMSE为4.73，对应的一致性相关系数(CCC)为0.67，CCC是该比赛中的主要评估系数。该模型的皮尔逊相关系数(PCC)为0.676，决定系数(r2)为0.457，根据对测试集的挑战结果，斯皮尔曼相关系数(SCC)为0.651。总体而言，该网络工作优于现有模型的8.95%。代码在15个epoch时收敛，验证损失为4.37，批处理大小为10，这些参数是根据经验选择的。预测单个文本抄写的平均测试时间为0.09秒。</p><h2 id="4-2-Results-from-Audio-Modality"><a href="#4-2-Results-from-Audio-Modality" class="headerlink" title="4.2    Results from Audio Modality"></a>4.2    Results from Audio Modality</h2><p>与基线模型相比，我们每个单独的网络在RMSE方面都表现出色。在基于音频MFCC特征的模型中，我们的表现比基线高出29.80%，而在eGEMAPS中，我们的表现则高出29.04%。对于BoAW-MFCC，我们的表现比基准高出10.44%，而对于BoAW-eGE，我们的表现比基准高出14.46%。每个单独的音频特性代码运行15个epoch，批处理大小为10。使用Functional MFCC对一个样本的平均时间要求为0.23秒，使用Functional eGEMAPS为0.14秒，使用BoAW-MFCC为0.45秒，使用BoAW-eGE为0.45秒，DS-DNet特征为0.13秒。对于音频模型，我们尝试了一个卷积神经网络架构来融合MFCC、eGEMAPS和DS-DNet特性，但是我们发现Bi-LSTMs的性能略好于卷积网络，其对序列特征有着更好的学习能力。</p><h2 id="4-3-Results-from-Visual-Modality"><a href="#4-3-Results-from-Visual-Modality" class="headerlink" title="4.3    Results from Visual Modality"></a>4.3    Results from Visual Modality</h2><p>视频特征的结果好于基线和技术水平，但仍比文本和语音模式的结果差。在视觉特征中，BoVW的表现最好，超出基线4.8%。与[32]相比，我们使用姿势特征的表现要比他们好9.3%，使用凝视的表现要比他们好6.6%，使用面部动作单元的表现要比他们好8.7%。</p><h2 id="4-4-Results-from-Fused-Modalities"><a href="#4-4-Results-from-Fused-Modalities" class="headerlink" title="4.4    Results from Fused Modalities"></a>4.4    Results from Fused Modalities</h2><p>该模型使用了所有的特征，融合了多层次的注意力，得到了超出baseline 17.52%的结果。与[32]相比较，音频-文本融合网络和视频-文本融合网络的性能分别提高了5.8%和9.19%，而全融合网络的性能略差。这不是结论性的，因为本文使用的数据集略有不同。注意机制自动权衡每种模式中的每个特征，并允许网络关注回归决策中最重要的特征。这样，网络就能了解特征与PHQ-8 scores之间的关系。</p><h1 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5    Discussion"></a>5    Discussion</h1><p>本文提出了一种基于多级注意力的早期融合网络，融合音频、视频和文本模式来预测抑郁症的严重程度。在这项任务中，我们观察到注意力网络给予文本情态的权重最高，给予音频和视频情态的权重几乎相等。给予特定词更高的权重是与临床医生一致的，因为言语内容对诊断抑郁水平至关重要。音频和视频是同样重要的信息来源，对预防严重程度至关重要。我们对视频数据重要性较低的直觉是，我们可以从视频模态(眼球注视、面部动作单元和头部姿势)中使用有限的特征。临床医生在面对面的访谈中可以观察一个人的身体姿势(自我接触颤抖等)或记录电生理信号，从而进行诊断。</p><p>多层次注意力的使用使我们在所有个体和融合模型中获得了比基线和技术水平更好的结果。把注意力放在每个特征和形态上总体上有两方面的优势。首先，这让我们更深入和更好地理解每个特征在抑郁症预测中的重要性。其次，简化了网络的整体计算复杂度，减少了训练和测试时间。实验结果表明，基于多水平注意力的全特征融合模型较基线有17.52%的提高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一
      
    
    </summary>
    
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁检测" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multimodal Fusion with Deep Neural Networks for Audio-Video Emotion Recognition</title>
    <link href="http://a-kali.github.io/2020/02/03/%E3%80%90%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%91Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/"/>
    <id>http://a-kali.github.io/2020/02/03/【论文解读】Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/</id>
    <published>2020-02-03T13:07:57.000Z</published>
    <updated>2020-04-07T13:53:09.586Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：本文提出了一种<strong>新的音频、视频和文本多模态融合用于情绪识别的深度神经网络</strong>。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平上进行早期模式融合的先进系统相比，我们提出的DNN可以实现更高水平的<a href="http://blog.sina.com.cn/s/blog_1859648c00102ylcy.html" target="_blank" rel="noopener"><strong>一致性相关系数(Concordance Correlation Coefficient, CCC)</strong></a>。该DNN在数据集开发分区上分别获得了0.606、0.534和0.170的CCCs，分别为唤起度(arousal)  、效价(valence)和喜欢度(liking)。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>最先进的传感器捕捉音频和视频信号，这为许多创新技术铺平了道路，这些技术可以实现非接触式的监测和诊断。例如，它们可以对个人进行持续的健康监测，这对于治疗和管理范围广泛的慢性疾病、神经障碍和精神健康问题，如糖尿病、高血压、哮喘、自闭症谱系障碍、疲劳、抑郁、药物成瘾等，变得越来越重要。</p><p>人们普遍认为，未来的人类环境(家庭、工作场所、公共交通等)将包含智能传感器阵列，可以支持和预测所需的行动，以一种普遍和不显眼的方式最佳地自我调节心理状态。识别个人情感状态的技术，特别是图像/视频和语音处理技术以及机器学习技术，有望在这一未来愿景中发挥关键作用。</p><p>然而，尽管有精密的传感器和技术，如何在真实场景精准识别仍然是一项挑战。首先，稳定地捕获时空信息，并为种群中的每个表达提取共同的可有效编码的时间特征同时抑制特定主题(类内)的变化是很难的。根据特定的个体行为和捕捉条件，面部表情和言语表达具有显著的时空变化。此外，创建具有代表性的大型音频视频数据集，并根据需要提供可靠的专家注释，以设计识别模型，并准确检测唤醒和效价水平是非常昂贵的。</p><p>在本文中，我们认为动态表情识别技术能够使用多种不同的模式准确地评估对象的时序情绪状态。我们提出了一种深度神经网络(DNN)结构，用于语音、人脸和文本信息的多模态融合，用于音视频情感识别。为了准确识别，本文提出的DNN提供了一个中间层次的融合，其中特征、分类器和融合函数以端到端的方式进行全局优化。</p><p>论文结构如下。第二部分介绍了文献中提出的针对不同模式的情绪识别技术。第三节介绍了多模态融合的DNN体系结构。第四部分给出了实验协议，第五部分给出了在RECOLA数据集上进行的实验，以及针对音频、视频和文本三种模式的实验结果，以及它们的融合。最后，在最后一节中对未来的工作进行了总结和展望。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>在过去的十年中，面部表情识别(ER)一直是人们非常感兴趣的话题。人们提出了许多ER技术，可以从一张静止的面部图像中自动识别出七种常见的情绪类型——喜悦、惊讶、愤怒、恐惧、厌恶、悲伤和中性。这些面部ER的静态技术往往是基于外观或基于几何图形的方法。最近，动态面部表情识别技术已经成为一种提高性能的方法，其中表情类型是通过在一个对象的物理面部表情过程中所捕获的一系列图像或视频帧来估计的。这不仅可以在空间域提取人脸的外观信息，还可以在时间域提取人脸的演化信息。该技术可以是<strong>基于形状、基于外观或基于动作</strong>的方法。</p><p>基于形状的方法，如约束局部模型(constrained local model, CLM)，基于突出锚点标记来表示面部特征形状，这些标记的运动为识别过程提供了判别信息；基于外观的方法，如LBP-TOP从面部图像中提取图像强度或其他纹理特征来表示面部表情；基于运动的自由变形模型等面部表情时空演化方法，需要可靠的人脸定位方法。例如，Guo等人使用atlas构造和稀疏表示从动态表达式中提取时空信息。虽然计算复杂度较高，包括时间信息和空间信息，但与静态图像相比，具有更高的识别精度。</p><p>在过去的几年里，自动检测说话人的情绪状态变得越来越流行。为了提高抑郁检测和情绪检测系统的准确性，人们探索了不同的方法。这两个方向有一些相似之处。从抑郁检测方面，France等人已经表明，格式频率的变化是抑郁和自杀倾向的良好指标。Cummings等人利用能量和光谱特征对抑郁症(抑郁症和非抑郁症)进行了二分类，其准确率约为70%。Moore等人通过统计分析(平均值、中位数和标准偏差)音高、能量或说话速度等韵律特征，获得了75%的准确率。就像几乎所有涉及到机器学习技术的领域一样，神经网络在情绪检测中的应用已经变得非常流行。研究人员通常使用DNNs、长短时记忆神经网络(LSTMs)和卷积神经网络(CNNs)。</p><p>包括以往AVEC比赛结果在内的多项情绪或减压检测研究的证据表明，通过整合来自多个不同信息源的证据(主要在特征、得分或决策层面)，可以提高识别系统的准确性和可靠性。因此，人们最近对通过多模态融合来检测情绪状态，特别是语音和面部模式的检测产生了一些兴趣。Kachele等人提出了一种分级多分类器框架，该框架通过引入不同程度的确定性来自适应地组合输入模式。声音韵律和面部动作单元也被用来检测抑郁。Menget等人提出了一种利用运动历史直方图动态特征从音频和视频中识别抑郁症的多层系统。Nasir等人提出了一种多模态特征，在多分辨率建模和融合框架中捕捉抑郁行为线索。特别地，他们利用Teager基于能量和i-vector的特征，连同音素率和持续时间来预测音频，并在视频中使用多项式参数化的时间变化和从面部地标获得的区域特征。最后，Williamson等人提出了一种很有前途的系统，该系统利用语音、韵律和面部动作单元特征的互补多模态信息来预测抑郁症的严重程度，尽管每种模态的贡献并没有被讨论。</p><p>尽管有精密的传感器和强大的技术，但在现实世界场景中开发精确的情绪识别模型仍面临一些挑战。在设计过程中，代表性数据的数量是有限的。假设识别模型是使用从特定条件下提取的有限数量的标记参考样本设计的。虽然许多音视频信号可以被捕获来设计识别模型，但它们需要昂贵的专家注释来创建大规模的数据集来检测唤醒和价态水平。在操作过程中，在操作域(即、受试者的办公室、家庭等)，在各种情况下。根据特定的捕捉条件和个体行为，面部和语音表情会随着时间发生显著的动态变化。因此，识别模型并不能代表操作域内模式的类内变化。实际上，任何分布上的变化(无论是域移位还是概念漂移)都会降低系统性能。识别模型需要对特定人员、传感器和计算设备以及操作环境进行校准和调整。</p><p>众所周知，随着时间的推移，结合来自不同模式的时空信息可以提高鲁棒性和识别精度。模式也可以根据上下文或语义信息动态组合，例如记录环境中的噪声。例如，可以根据捕获条件在不同的层(分辨率)组合深度学习架构的输出响应。</p><p>本文主要研究利用深度学习体系结构来生成精确的混合情感识别系统。例如，Kim等人提出了一种层次化的3级CNN结构来组合多模态源。DNNs被认为是学习具有特定目标的转换序列，以获得将在一个系统中组合的特征。由于特征级和分数级融合不一定能获得较高的精度，因此提出了一种混合方法，通过学习特征和分类器来优化多模态融合。</p><h1 id="3-Multimodal-Fusion-with-Deep-NNs"><a href="#3-Multimodal-Fusion-with-Deep-NNs" class="headerlink" title="3    Multimodal Fusion with Deep NNs"></a>3    Multimodal Fusion with Deep NNs</h1><p>本文提出了一种高效的的DNN结构，该结构可以从多个信息源中学习受试者行为与情绪状态之间的映射。对于给定的时序面部和语音模态（包括文本信息）的AVEC SEWA 数据库，本系统旨在从中学习特征表示、分类和融合以对影本进行准确地预测。</p><p>该方法的主要内容在于联合学习每一个判别表示，以及它们的分类和融合函数。每个特征子集首先由一个或多个隐藏层独立处理。网络的这一部分学习给定模态的最佳特征表示。然后，每个块的最后隐藏层相互连接到一个或多个完全连接的层，这些层将进行特征的分类和融合。从全局的角度来看，这个网络应该学习如何转换输入特征，从而进行分类和融合，并产生一个全局决策。训练一个混合分类器来组合这些特征可以提高识别系统的整体精度。该架构使用了三种不同的信息来源——音频（语音）、视频（脸部）和文本。有关特性集的详细信息，请参阅[20]。</p><p>1)音频特征：声学特征由23个声学LLD组成，如能量、频谱和倒频谱、音高、音质和微韵律特征，每10ms的短帧提取一次。使用一个包含1000个音频单词的编码书，在6秒的时间段里计算分段级声学特征，并创建音频单词的直方图。因此，得到的特征向量有1000维。</p><p>2)视频特征：每帧（帧步20ms）提取视频特征，包括三种特征类型：按度数归一化的人脸方向；10个视点的像素坐标；49个面部标志的像素坐标。每个带有独立码本和直方图的BoVW被创建为三个面部特征类型，每个码本的大小为1000，产生一个3000维的节段级特征向量。</p><p>3)文本特征：文本特征是基于语音转录的词袋特征表示。该词典包含521个单词，其中只考虑了一元语法（unigram）。在一段6秒的时间内创建的直方图。总的来说，文本包(BoTW)包含521个特征。</p><h2 id="3-1-DNN-Architecture"><a href="#3-1-DNN-Architecture" class="headerlink" title="3.1    DNN Architecture"></a>3.1    DNN Architecture</h2><p>所提出的多模态融合的DNN体系结构如图所示。DNN对每个模态使用一对全连接层分别处理音频、视频和文本，以生成和探索同一类型特征之间的相关性。然后，第二阶段(b)合并这些独立层的输出。该层在一个块中接收前一步的输出，并提供一个与模式的本质相关联的全连接层(c)。DNN输出由单个线性神经元(d)产生，作为整个网络的回归值。最后，利用标度模(e)来缩小预测量与标签量之间的差距。不同的标度函数被认为是最好的结果-小数标度，最小最大归一化和标准差标度。最终的预测(f)通过线性函数(前一层的加权和)产生。</p><p><img src="https://i.loli.net/2020/02/04/AMvSXGaWkoDQr1d.png" alt="E@CC6K8WWH9YLDBW7UAALCT.png"></p><h1 id="4-Experimental-Methodology"><a href="#4-Experimental-Methodology" class="headerlink" title="4    Experimental Methodology"></a>4    Experimental Methodology</h1><p>RECOLA数据集包括一个训练集和一个开发集(development set)。在本研究的实验中，开发集被分为两个子集。第一个包含从开发集的14份数据中随机选择的5个数据。剩下的数据作为测试集来评估方法的性能。</p><p>在SVR实验中，原始提案被用于建立单峰和早期融合系统。后融合方法使用第一个开发子集来优化每个单模系统。使用第二个开发子集对融合函数进行了优化。</p><p>在DNN的情况下，更小的开发集被用来确定每个模式的最佳层数和神经元数，以及融合层的神经元数。第二个开发子集用于测试模型。注意，每个情感维度都有自己的体系结构，如表所示。</p><p>本次挑战采用的评价指标为CCC，其定义为:</p><p><img src="https://i.loli.net/2020/02/04/VKebGpFvu2qTNLd.png" alt="_1VUA87A@T_2AO9_Y9_4_OI.png"></p><h2 id="4-1-Preprocessing"><a href="#4-1-Preprocessing" class="headerlink" title="4.1    Preprocessing"></a>4.1    Preprocessing</h2><p>当应用延迟补偿函数时，CCC评分的结果有显著的提高。图2显示了CCC随延迟的变化。</p><p><img src="https://i.loli.net/2020/02/04/p9Gat8BS4V3ud1s.png" alt="Y__AZX_RMMEY_8~AES`42H3.png"></p><h1 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5    Conclusion and Future Work"></a>5    Conclusion and Future Work</h1><p>本文提出了一种新的DNN结构来预测情绪状态。它融合了三种不同的模态：音频信号、面部特征和音频信号的对话文本。每个模态首先由两个全连接层独立编码，然后合并成一个单一的表示，然后用于估计主体的情绪状态。该网络以端到端方式训练，提供比其他提出的架构更高的CCC。通过对输入特征进行适当的归一化，并对回归的输出进行时间平滑处理，可以期望得到进一步的改进。对于未来的工作，我们计划扩展我们的工作，包括一个基于递归神经网络的最后阶段，它可以学习情绪的时间模式，从而提高整个系统的准确性。此外，我们还将评估从明确训练的卷积神经网络中提取的视觉特征的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要：本文提出了一种&lt;strong&gt;新的音频、视频和文本多模态融合用于情绪识别的深度神经网络&lt;/strong&gt;。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="多模态" scheme="http://a-kali.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（14）—— 如何选刊</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8814%EF%BC%89%E2%80%94%E2%80%94-%E5%A6%82%E4%BD%95%E9%80%89%E5%88%8A/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（14）——-如何选刊/</id>
    <published>2020-02-03T05:10:55.000Z</published>
    <updated>2020-02-03T05:49:36.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何确定你的目标期刊？"><a href="#如何确定你的目标期刊？" class="headerlink" title="如何确定你的目标期刊？"></a>如何确定你的目标期刊？</h1><ul><li>期刊的类型、领域范围</li><li>期刊的声望和影响力（影响因子、引用率）</li><li>审稿和发表程序</li></ul><h2 id="1-期刊的目标和范围"><a href="#1-期刊的目标和范围" class="headerlink" title="1    期刊的目标和范围"></a>1    期刊的目标和范围</h2><p>回答三个问题：</p><ol><li>研究领域是否相关？</li><li>你的论文类型是否在该期刊发表过？</li><li>期刊的读者群体是什么？（专业/综合、亚洲/欧洲 等）</li></ol><h2 id="2-期刊的声望和影响力"><a href="#2-期刊的声望和影响力" class="headerlink" title="2    期刊的声望和影响力"></a>2    期刊的声望和影响力</h2><p>四个问题：</p><ol><li>同行评价/阅读人数如何？</li><li>期刊的影响因子是否满足你的要求？</li><li>期刊收录的数据库有哪些？</li><li>期刊的出版形式？（在线/印刷）</li><li>期刊的委员会成员、赞助者是否知名？</li></ol><h2 id="3-审稿和发表程序"><a href="#3-审稿和发表程序" class="headerlink" title="3    审稿和发表程序"></a>3    审稿和发表程序</h2><ol><li>期刊的出版频率（月刊/半月刊/季刊）</li><li>发表周期（一审/二审/发表）</li><li>接收后见刊时间（即发表到刊物上）</li><li>发表费用（版面费、彩图费是否合理？能否报销？）</li><li>是否可以公开获取（开放权限），多久能公开获取（数据库收录）</li></ol><h1 id="查找合适的投稿期刊"><a href="#查找合适的投稿期刊" class="headerlink" title="查找合适的投稿期刊"></a>查找合适的投稿期刊</h1><ol><li><strong>从每年的期刊影响因子中查找</strong>：每年的6月份左右Thomson都会发布上一年所有SCI期刊的影响因子，通过这些可以了解本专业领域的顶级期刊，选择合适自己的期刊。</li><li><strong>从参考文献中获得合适的期刊</strong>：统计在撰写论文过程中阅读的大量文献来自于哪些期刊，再根据自己论文的质量选出合适的期刊。</li><li><strong>询问同行或者导师</strong>。</li><li><strong>向期刊编辑或者主编咨询</strong>。 </li><li><strong>借助选刊工具</strong>：<ol><li>Edanz Journal Selector</li><li>Elsevier Journal Finder</li><li>Journal Article Name Estimator</li><li>Springer Journal Selector</li><li>MedSci期刊选择智能支持系统</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;如何确定你的目标期刊？&quot;&gt;&lt;a href=&quot;#如何确定你的目标期刊？&quot; class=&quot;headerlink&quot; title=&quot;如何确定你的目标期刊？&quot;&gt;&lt;/a&gt;如何确定你的目标期刊？&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;期刊的类型、领域范围&lt;/li&gt;
&lt;li&gt;期刊的声望和影响
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（13）—— 写作语句链接</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8813%EF%BC%89%E2%80%94%E2%80%94-%E5%86%99%E4%BD%9C%E8%AF%AD%E5%8F%A5%E9%93%BE%E6%8E%A5/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（13）——-写作语句链接/</id>
    <published>2020-02-03T03:59:33.000Z</published>
    <updated>2020-02-03T04:49:42.992Z</updated>
    
    <content type="html"><![CDATA[<p>在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括<strong>连接词和副词</strong>。</p><p>根据语句的前后关系，连接词可以分为7类：</p><ul><li><p><strong>因果关系</strong>；常用连接词：therefore, consequently, thus, hence, as a result, indeed等，用来对前句的内容进行总结。</p></li><li><p><strong>并列关系</strong>；常用连接词：also, likewise, besides in addition, moreover, furthermore等，用来连接多个同等事物或特征。</p></li><li><p><strong>相反关系</strong>；常用连接词：in contrast, but, however, yet, on the other hand, surprisingly, nevertheless, instead of等，用来描述某些设想与文献描述不符的现象或者结果。</p></li><li><p><strong>相似关系</strong>（类似于并列关系）；常用连接词：similarly, likewise，用于表达两个功能或者结果具有相似性。</p></li><li><p><strong>举例关系</strong>；常用连接词：for example, for instance, specifically, such as including等。</p></li><li><p><strong>时间关系</strong>；如：later等。</p></li><li><p><strong>顺序关系</strong>（类似于时间关系）；常用连接词：then, next, finally, first, second等。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括&lt;strong&gt;连接词和副词&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据语句的前后关系，连接词可以分为7类
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（11）—— 实在憋不出来咋办</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8811%EF%BC%89%E2%80%94%E2%80%94-%E5%AE%9E%E5%9C%A8%E6%86%8B%E4%B8%8D%E5%87%BA%E6%9D%A5%E5%92%8B%E5%8A%9E/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（11）——-实在憋不出来咋办/</id>
    <published>2020-02-03T03:19:11.000Z</published>
    <updated>2020-02-03T03:56:54.403Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><p>本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。</p><p><strong>去哪里“抄”？</strong></p><ul><li>Google 图书、图书馆（书籍）</li><li>维基百科（定义、性质）</li><li>论文文献（Introduction、Methods、Discussion、Figure legend）：抄语法、词组，修改主语等。</li></ul><p><strong>查重</strong>：论文写完后，一定要去做查重。如果被审稿人查出重复率较高会退回重改，影响发表时间。查重服务可以来自于万方等数据库，或者淘宝、闲鱼商家提供的服务。</p><p><strong>语法、词汇差错</strong>：</p><ul><li>易改软件</li><li>英语润色服务</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;p&gt;本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;去哪里“抄”？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google 图书、图书馆（书籍）&lt;/li&gt;
&lt;li&gt;维基百科（定义、性质
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（3）—— Figure Legend</title>
    <link href="http://a-kali.github.io/2020/02/02/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94-Figure-Legend/"/>
    <id>http://a-kali.github.io/2020/02/02/SCI写作入门（3）——-Figure-Legend/</id>
    <published>2020-02-02T13:59:24.000Z</published>
    <updated>2020-02-02T14:07:04.688Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><h1 id="1-图片说明"><a href="#1-图片说明" class="headerlink" title="1    图片说明"></a>1    图片说明</h1><h1 id="2-表格注释"><a href="#2-表格注释" class="headerlink" title="2    表格注释"></a>2    表格注释</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;h1 id=&quot;1-图片说明&quot;&gt;&lt;a href=&quot;#1-图片说明&quot; class=&quot;headerlink&quot; title=&quot;1    图片说明&quot;&gt;&lt;/a&gt;1    图片说明&lt;/h1&gt;&lt;h1 id=&quot;2-表格注释&quot;&gt;&lt;a href=&quot;#2-表格注释&quot;
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>3D点云概念及性质</title>
    <link href="http://a-kali.github.io/2020/02/02/3D%E7%82%B9%E4%BA%91%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <id>http://a-kali.github.io/2020/02/02/3D点云概念及处理方法/</id>
    <published>2020-02-02T06:03:11.000Z</published>
    <updated>2020-02-03T03:31:51.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p><strong>点云与三维图像的关系：</strong>三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以实现天然的物体-背景解耦。点云数据是最为常见也是最基础的三维模型。点云模型往往由测量直接得到，每个点对应一个测量点。</p><p><strong>点云的概念</strong>：点云是在同一空间参考系下表达目标空间分布和目标表面特性的海量点集合，在获取物体表面每个采样点的空间坐标后，得到的是点的集合，称之为“点云”（Point Cloud）。</p><p><strong>点云的内容：</strong>根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。<br>根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。<br>结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）。</p><p><strong>点云存储格式：*</strong>.pts; *.asc ; *.dat; .stl ; .imw；.xyz；.las。LAS格式文件已成为LiDAR数据的工业标准格式，LAS文件按每条扫描线排列方式存放数据,包括激光点的三维坐标、多次回波信息、强度信息、扫描角度、分类信息、飞行航带信息、飞行姿态信息、项目信息、GPS信息、数据点颜色信息等。</p><h1 id="点云性质"><a href="#点云性质" class="headerlink" title="点云性质"></a>点云性质</h1><p>点云数据是在欧式空间下的点的一个子集，它具有以下三个特征：</p><ul><li>无序。点云数据是一个集合，对数据的顺序是不敏感的。这就意味这处理点云数据的模型需要对数据的不同排列保持不变性。目前文献中使用的方法包括将无序的数据重排序、用数据的所有排列进行数据增强然后使用RNN模型、用对称函数来保证排列不变性。由于第三种方式的简洁性且容易在模型中实现，论文作者选择使用第三种方式，既使用maxpooling这个对称函数来提取点云数据的特征。</li><li>点与点之间的空间关系。一个物体通常由特定空间内的一定数量的点云构成，也就是说这些点云之间存在空间关系。为了能有效利用这种空间关系，论文作者提出了将局部特征和全局特征进行串联的方式来聚合信息。</li><li>不变性。点云数据所代表的目标对某些空间转换应该具有不变性，如旋转和平移。论文作者提出了在进行特征提取之前，先对点云数据进行对齐的方式来保证不变性。对齐操作是通过训练一个小型的网络来得到转换矩阵，并将之和输入点云数据相乘来实现。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>[1]<a href="https://blog.csdn.net/hongju_tang/article/details/85008888" target="_blank" rel="noopener">点云概念与点云处理</a></li><li>[2]<a href="https://www.jiqizhixin.com/articles/2019-05-10-13" target="_blank" rel="noopener">PointNet系列论文解读</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;点云与三维图像的关系：&lt;/strong&gt;三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以
      
    
    </summary>
    
    
      <category term="3D" scheme="http://a-kali.github.io/tags/3D/"/>
    
      <category term="点云" scheme="http://a-kali.github.io/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（2）—— Figure and table</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94-Figure-and-table/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（2）——-Figure-and-table/</id>
    <published>2020-02-01T12:13:32.000Z</published>
    <updated>2020-02-02T13:58:41.738Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><h1 id="文字、图片和表格的合理使用"><a href="#文字、图片和表格的合理使用" class="headerlink" title="文字、图片和表格的合理使用"></a>文字、图片和表格的合理使用</h1><p>图表可以用来表示</p><ol><li>结果：得出的数据、统计图、实验对比等</li><li>方法：流程图、示意图</li></ol><p>图表的分类：</p><ul><li>投稿时：一般要求，在尺寸、分辨率、色彩等方面有一定要求，能让审稿人看清就行；</li><li>发表时：对图片质量要求高，包括图片文字、数字、字号大小、线条粗细等都有具体的要求</li></ul><p>文字表述：</p><ol><li>描述定量化的结果</li><li>用来描述定性数据之间的关系（e.g. 同比上升/下降）</li></ol><p>图片表述：</p><ol><li>用来展示数据组之间的趋势</li><li>描述指标随时间改变的情况</li><li>定量化因素之间的复杂关系</li><li>实验方法介绍（流程图）</li><li>复杂概念介绍</li><li>可视化数据结果（e.g. 细胞图片）</li></ol><p>表格表述：</p><ol><li>展示大量数值型数据</li><li>不同项目之间的细节比较</li><li>复杂定性结果之间比较</li></ol><h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><h2 id="常见图片类型"><a href="#常见图片类型" class="headerlink" title="常见图片类型"></a>常见图片类型</h2><p>照片、线型图、柱状图、流程图、示意图等</p><h2 id="图片制作过程注意事项"><a href="#图片制作过程注意事项" class="headerlink" title="图片制作过程注意事项"></a>图片制作过程注意事项</h2><ol><li>图标：一个图片对应一个图标。比如说图标a只能对应一个大图中的一个小图。</li><li>标注：箭头、星号；</li><li>缩写：图片标识的时候可以使用缩写，但一定要定义；</li><li>颜色：根据杂志要求选择色彩或灰度模式。</li></ol><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><h2 id="表格的要素"><a href="#表格的要素" class="headerlink" title="表格的要素"></a>表格的要素</h2><ol><li>数值；</li><li>列标题和列；</li><li>行标题和行；</li><li>注释和说明。</li></ol><p><img src="https://i.loli.net/2020/02/02/TiNsO81RmtFpVDH.png" alt="_SZ59_91_D_N84__N@2J4F5.png"></p><h2 id="论文表格的一般要求"><a href="#论文表格的一般要求" class="headerlink" title="论文表格的一般要求"></a>论文表格的一般要求</h2><ul><li>单位：一般情况下，单位标识在行标题的后面；</li><li>对比：通常使用横向比较而不是纵向比较；</li><li>顺序：时间顺序、大小顺序、字母顺序；</li><li>空白数据：可以使用”-“表示，或者相应缩写（e.g. ND, Not Done)。</li></ul><h2 id="表格中的线条"><a href="#表格中的线条" class="headerlink" title="表格中的线条"></a>表格中的线条</h2><ol><li>在<strong>列标题上下</strong>需要一条线条；</li><li><strong>最后一行数据下面</strong>需要一条线条；</li><li>列的<strong>副标题下面</strong>需要线条；</li><li>除了以上三种情况以外，其它的都不能使用线条！</li></ol><p><img src="https://i.loli.net/2020/02/01/YlAVc1ryZEhmwz9.png" alt="90U_Y9_G7_QE_SF_U~_413D.png"></p><h2 id="表格中的注释"><a href="#表格中的注释" class="headerlink" title="表格中的注释"></a>表格中的注释</h2><p>表格中出现多个注释的时候，可以使用*、**、#等</p><p><img src="https://i.loli.net/2020/02/02/xOKMlTQfFCY6oHS.png" alt="4~@9__~N~6Z9_PKD21@YF0F.png"></p><h2 id="表格中的数值"><a href="#表格中的数值" class="headerlink" title="表格中的数值"></a>表格中的数值</h2><ul><li>如果有百分比数值最好加上百分比数值；</li><li>如果样本数很多，最好在列标题中标注样本总数；</li><li>如果某一列中所有数据相同，需要在注释中说明指出；</li><li>对齐方式：小数点对齐、左圆括号对齐；</li><li>多次检查。表格中数据多，容易出错。</li></ul><p><img src="https://i.loli.net/2020/02/02/YjQlhKbtek8OpDs.png" alt="RZRPX_R_BVE_`FJM_NLK~R4.png"></p><h2 id="表格中的单位"><a href="#表格中的单位" class="headerlink" title="表格中的单位"></a>表格中的单位</h2><p>一般情况下，单位标识在行标题的后面：</p><p><img src="https://i.loli.net/2020/02/02/a1qHDs5I7zSQMGw.png" alt="_D_~_2_`KGKYO__GVEY__VE.png"></p><p>当个别数据单位不同时，可以备注出来：</p><p><img src="https://i.loli.net/2020/02/02/aeMoRFKJZNuTjPl.png" alt="HNSU37T_G23B_OU8F9TUEGJ.png"></p><p>未完待续~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;h1 id=&quot;文字、图片和表格的合理使用&quot;&gt;&lt;a href=&quot;#文字、图片和表格的合理使用&quot; class=&quot;headerlink&quot; title=&quot;文字、图片和表格的合理使用&quot;&gt;&lt;/a&gt;文字、图片和表格的合理使用&lt;/h1&gt;&lt;p&gt;图表可以用来表示
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（1）——SCI简介及写作顺序</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94SCI%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%86%99%E4%BD%9C%E9%A1%BA%E5%BA%8F/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（1）——SCI简介及写作顺序/</id>
    <published>2020-02-01T10:30:13.000Z</published>
    <updated>2020-02-01T12:07:17.937Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SCI论文组成"><a href="#1-SCI论文组成" class="headerlink" title="1    SCI论文组成"></a>1    SCI论文组成</h1><ul><li>Title：文章标题</li><li>Abstract：摘要</li><li>Introduction：引言</li><li>Methods：实验所使用的材料和方法</li><li>Results：实验结果</li><li>Discussion：对实验结果的讨论和分析</li><li>Reference：参考文献</li><li>Figures and Tables：图片、图表</li><li>Figure legends：图片、图表说明</li><li>Acknowledgements：致谢</li></ul><h2 id="1-1-Title"><a href="#1-1-Title" class="headerlink" title="1.1    Title"></a>1.1    Title</h2><p>简明、准确地总结文章内容，包含关键词，使文章更容易被所需要的人检索到。</p><h2 id="1-2-Abstract"><a href="#1-2-Abstract" class="headerlink" title="1.2    Abstract"></a>1.2    Abstract</h2><p>通过一段话将你的<strong>研究背景、内容、目的、结果以及研究意义</strong>告诉读者，使读者仅仅通过读Abstract就能明白这篇文献是否是其感兴趣的（或者使读者通过读Abstract就能对你的研究产生兴趣）。是用最少的词表述最重要的内容。</p><h2 id="1-3-Introduction"><a href="#1-3-Introduction" class="headerlink" title="1.3    Introduction"></a>1.3    Introduction</h2><p>Introduction是比较难写的一部分。在Introduction中，作者需要比Abstract更为详细地回答以下五个问题：</p><ol><li><p>为什么——为什么要做这个研究？</p></li><li><p>是什么——你的<strong>科学假说</strong>是什么？</p></li><li><p>做什么——你的<strong>研究方法</strong>是什么？</p></li><li><p>什么结果——你的<strong>研究发现</strong>是什么？</p></li><li><p>什么意义——为什么你的研究很重要？</p></li></ol><h2 id="1-4-Methods"><a href="#1-4-Methods" class="headerlink" title="1.4    Methods"></a>1.4    Methods</h2><p>详细叙述研究采用的方法，能够让读者参考该文献复现出实验研究。</p><h2 id="1-5-Results"><a href="#1-5-Results" class="headerlink" title="1.5     Results"></a>1.5     Results</h2><p>开门见山地告诉读者你发现了什么现象、得出了什么数据和结论。同时在这一部分，作者需要灵活地使用图片、表格等方式更加形象地展示研究成果。</p><h2 id="1-6-Discussion"><a href="#1-6-Discussion" class="headerlink" title="1.6    Discussion"></a>1.6    Discussion</h2><p>Discussion同样是比较难写的一部分。作者需要对自己的研究进行总结和归纳；同时需要讨论该研究与相关研究的关系，其结果是否具有一致性，如何去解释不一致性的产生；最后还要说明自己研究的局限性以及如何去改进这种局限性。提出研究和价值。<strong>Discussion直接决定了整篇文章的质量，对文章能否顺利发表有很大的影响</strong>。</p><h2 id="1-7-Reference"><a href="#1-7-Reference" class="headerlink" title="1.7    Reference"></a>1.7    Reference</h2><ul><li>列举出参考文献，为问题提供背景，为方法提供出处，为结论提供证据。 </li><li>对前人工作和观点的一种尊重和赞同。</li><li>为读者提供更多的信息来源。</li></ul><h1 id="2-SCI论文合理写作顺序"><a href="#2-SCI论文合理写作顺序" class="headerlink" title="2    SCI论文合理写作顺序"></a>2    SCI论文合理写作顺序</h1><ol><li>建议首先制作论文图片和表格，表格和图片就是论文的骨架！完成了图片和表格后，整个文章的基本框架便在心里有数。</li><li>完成表格和图片的说明（Figure Legend），趁热打铁。</li><li>完成论文的Results。一个图片/表格对应一个小结，合起来就是整个Results。</li><li>完成论文的Introduction。对于一件你已经完成的事情，写Introduction来解释你为什么要做这件事情。</li><li>完成Discussion。相当于Introduction的后续，对提出的工作进行总结。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-SCI论文组成&quot;&gt;&lt;a href=&quot;#1-SCI论文组成&quot; class=&quot;headerlink&quot; title=&quot;1    SCI论文组成&quot;&gt;&lt;/a&gt;1    SCI论文组成&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Title：文章标题&lt;/li&gt;
&lt;li&gt;Abstract：摘
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Detecting Depression with AI (AVEC2019)</title>
    <link href="http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/"/>
    <id>http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/</id>
    <published>2020-02-01T02:45:56.000Z</published>
    <updated>2020-03-17T15:02:36.976Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1907.11510v1" target="_blank" rel="noopener">AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression with AI, and Cross-Cultural Affect Recognition</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>视听情感挑战与工作坊(AVEC 2019)“心智状态、人工智能检测抑郁、跨文化情感识别”是第九届比赛，旨在将多媒体处理和机器学习方法用于自动视听健康和情感分析，所有参与者在相同的条件下严格竞争。该挑战的目标是为多模态信息处理提供一个通用的基准测试集，并将健康和情绪识别社区以及视听处理社区集合在一起，以比较现实生活数据中各种健康和情绪识别方法的相对优点。本文介绍了今年的主要创新点、挑战指南、使用的数据和基线系统在三个拟议任务上的表现:心理状态识别、人工智能抑郁评估和跨文化情感感知。</p><h1 id="2-Depression-Detection-with-AI"><a href="#2-Depression-Detection-with-AI" class="headerlink" title="2    Depression Detection with AI"></a>2    Depression Detection with AI</h1><p>抑郁症，尤其是重度抑郁症( major depressive disorder, MDD)，是一种常见的心理健康问题，对人的思维、感觉和行为方式有负面影响。它会导致各种情绪和身体问题，影响工作和个人生活的许多方面。世界卫生组织(WHO)在2015年宣布抑郁症是全球范围内导致疾病和残疾的主要原因:超过3亿人患有抑郁症。鉴于抑郁症的高患病率及其自杀风险，寻找新的诊断和治疗方法变得越来越重要。由于有令人信服的证据表明抑郁症和相关的精神健康障碍与行为模式的改变有关，人们越来越有兴趣使用自动人类行为分析来基于行为线索(如面部表情和说话韵律)进行计算机辅助抑郁症诊断。<strong>面部活动、手势、头部运动和表达能力</strong>等行为信号都与抑郁症密切相关。</p><p>计算机视觉可以追踪的面部表情和头部动作也是预测抑郁的好方法。据报道，更向下的凝视角度、不那么强烈的微笑、更短的平均微笑持续时间是抑郁症最显著的面部特征。此外，身体表情、手势、头部动作和语言线索也被报道为抑郁检测提供相关线索。综合所有这些证据，有人提议将情感计算技术集成到一个计算机代理中，该代理可以访问人们并识别精神疾病的语言和非语言指标。对创伤后应激障碍患者收集的数据表明，当代理人由充当WoZ的人驱动时，对其抑郁严重程度的自动评估可以实现RMSE小于5；<strong>PHQ-8 range∈[0,24] 的cutpoint分别定义为轻度、中度、中度和重度抑郁症</strong>。这些结果需要进一步研究，因为代理完全由人工智能驱动，因为向导可能会将虚拟代理驱动到一种情况，从而减轻与抑郁症相关的模式的观察，或者自主代理可能在适当地进行访谈方面存在问题。</p><h1 id="3-Distress-Analysis-Interview-Corpus"><a href="#3-Distress-Analysis-Interview-Corpus" class="headerlink" title="3    Distress Analysis Interview Corpus"></a>3    Distress Analysis Interview Corpus</h1><p>扩展遇险分析访谈语料库(E-DAIC)是WOZ-DAIC的扩展版本，包含半临床访谈，旨在支持诊断焦虑、抑郁和创伤后应激障碍等心理困扰状况。收集这些访谈是为了创建一个计算机代理来采访人们，并识别精神疾病的语言和非语言指标。收集的数据包括音频和视频记录、使用谷歌云语音识别服务自动转录的文本以及广泛的问卷回答。这些面试是由一个叫做Ellie的动画虚拟面试官进行的。在theWoZ的面试中，虚拟代理由另一个房间的人类面试官(巫师)控制，而在AI的面试中，代理以完全自主的方式使用不同的自动感知和行为生成模块。</p><p>为了达到挑战的目的，E-DAIC数据集被划分为训练、开发和测试集，同时保留了演讲者的整体多样性——在年龄、性别分布和8项患者健康问卷(PHQ8)评分方面——在这些划分内。训练和开发集包括WoZ和人工智能场景的混合，而测试集仅由自主人工智能收集的数据构成。关于扬声器在分区上的分布的详细信息见表2。</p><p><img src="https://i.loli.net/2020/02/01/teQ2Za9kod7L4M1.png" alt="K_UZ`K8@_ADCD0_THOI_~35.png"></p><h1 id="4-Baseline-Features"><a href="#4-Baseline-Features" class="headerlink" title="4    Baseline Features"></a>4    Baseline Features</h1><p>视听信号的情感识别通常依赖于特征集，这些特征集的提取基于近几十年来在视听信号处理领域的研究出来的特定技术，在语音方面有：<strong>梅尔倒频谱系数(Mel Frequency Cepstral Coefficients, MFCCs)</strong>；视觉方面则有：<strong>面部活动单元(Facial Action Units, FAUs)</strong>。</p><h2 id="4-1-Expert-knowledge"><a href="#4-1-Expert-knowledge" class="headerlink" title="4.1    Expert-knowledge"></a>4.1    Expert-knowledge</h2><p>影响感知的传统方法是在固定时间内，通过一组滑动窗口计算的统计度量，来总结随时间变化的视听信号<strong>低水平描述符(low-level descriptors, LLDs)</strong>。</p><p>在音频方面，我们计算了<strong>扩展GeMAPs特征集(extended Geneva Minimalistic Acoustic Parameter Set, eGeMAPS)</strong>，它包含88个覆盖上述声学维度，并在这里用作基线。除此之外，我们使用OpenSMILE工具包使MFCCs 1-13 的一阶和二阶差分被计算为一组声学LLDs。在视觉方面，我们使用openFace工具包提取每个视频帧的17个FAU强度，以及一个置信度。此外，还提取了姿态(pose)和凝视(gaze)的描述符。</p><h2 id="4-2-Bags-of-Words"><a href="#4-2-Bags-of-Words" class="headerlink" title="4.2    Bags-of-Words"></a>4.2    Bags-of-Words</h2><p><strong>词袋(bags-of-words, BoW)</strong>技术起源于文本处理，它代表了LLD的分布。我们使用MFCCs和eGeMAPs特征集作为声学数据，FAU的强度作为视频数据。MFCCs和eGeMAPS的LLD是经过标准化处理的。</p><p>为了生成BoW表示，声学和视觉特征都在长度为4s的音视频段上进行处理，对于SEWA数据集的每一段为100ms，对于USoM和E-DAIC数据集为1s。随机采样示例来构建字典，并从结果项频率中取对数以压缩它们的范围。整个跨模式BoW(XBoW)处理链是使用开源工具包openXBOW执行的。</p><h2 id="4-3-Deep-Representations"><a href="#4-3-Deep-Representations" class="headerlink" title="4.3    Deep Representations"></a>4.3    Deep Representations</h2><p>在去年的挑战中，我们将深度频谱特征(Deep Spectrum features)作为一种基于深度学习的音频基线特征表示。深度频谱特征的灵感来自于图像处理中常见的深度表示学习范式：将语音实例的频谱图像输入到预训练好的CNNs中，提取一组由此产生的激活值作为特征向量。</p><p>在今年的挑战中，我们使用VGG-16、AlexNet、DenseNet-121和DenseNet-201四个鲁棒的预训练模型中提取了深度频谱特征；在AVEC 2019 CES上使用AlexNet纯粹是为了与之前的AVEC 2018 CES保持一致。语音文件首先被转换成具有128个mel频段的mel谱图图像，所有挑战语料库的窗口宽度为4s, USoM和E-DAIC数据集的跳数为1s, SEWA数据集的跳数为100ms。随后，基于频谱的图像通过预先训练的网络进行转发。然后，通过激活VGG-16和AlexNet中的第二个全连接层形成4096维的特征向量，通过激活DenseNet-121和DenseNet-201网络的最后一个平均池化层分别得到1024和1920维的特征向量。</p><p>我们还提供了两个基线深度视觉表示，分别使用了VGG-16网络和ResNet-50网络，这两个网络都是使用Affwild数据集进行预训练的。首先应用openFace工具包来检测脸部区域，然后执行面部对齐。然后，我们冻结两个预先训练的模型的权重，并分别将对齐的脸部图像输入两个CNN。为了获得每个帧的深度表示，我们分别从预训练的VGG-16网络中提取第一个全连接层的输出，从预训练的ResNet-50网络中提取全局平均池化层的输出。因此，每一帧都提供了来自VGG的4096维深特征向量和来自ResNet的2048维深特征向量。</p><h1 id="5-Baseline-System"><a href="#5-Baseline-System" class="headerlink" title="5    Baseline System"></a>5    Baseline System</h1><p>对于抑郁检测基线，我们使用单层64-d GRU作为我们的递归网络，其失步正规化率为20%，然后使用64-d全连通层获得单值回归评分。为了处理偏差，我们将PHQ-8分数标签转换为浮点数，方法是在培训之前按25的倍数缩小比例。使用CCC损失函数和评价分数对网络进行训练和评价，使用原始的PHQ量表报告RMSE结果。批处理大小为15的方法得到了一致的使用，并且在不同的特性集之间优化了学习率。为了使数据适合GPU内存，为会话分配了最大的序列长度。对于MFCCs和eGeMAPS LLDs，以及诸如DeepSpectrum、ResNet和VGG等高维深表示，使用的最大序列长度为20分钟。另外，对于ResNet、VGG和深度光谱表示帧，根据维数的不同，将保留两帧中的一帧或四帧中的一帧，以便将数据加载到内存中。融合不同的视听表现是通过平均他们的分数来实现的。</p><p>DDS的基线结果见表6。结果表明，在开发集上，利用深度谱(DS-VGG)特征获取音频特征的最佳CCC评分，利用ResNet特征获取视觉特征的最佳CCC评分。这些结果表明表达的力量深层神经网络学习的大量数据时在不同的上下文中使用他们最初的设计,这是证实与ResNet视觉模型在测试集上实现最好的结果,尽管相对较低的CCC。</p><p>不同表现形式的融合在开发集上获得了最好的结果，测试集上返回的RMSE比使用AVEC 2017基线系统在DAIC-WoZ数据集上获得的RMSE稍好一些;AVEC2019年的RMSE=6.37，而AVEC 2017年的RMSE=6.97。然而，为今年的挑战开发的基线系统更加复杂，与今年的GRU-RNNs相比，它是一个简单的线性回归模型，因此，根据2017年AVEC抑郁亚挑战的最佳结果(RMSE=4.99)，应该最好地考虑相应的分数。</p><p>根据从与虚拟代理的交互中获得的抑郁程度自动感知的结果，当代理仅由人工智能驱动时，识别似乎比由人作为WoZ驱动时更具挑战性。这一观察结果为设计抑郁症诱因的设计带来了有趣的研究问题。通过强化学习，根据agent的交互方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1907.11510v1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AVEC 2019 Workshop and Challenge: State-of-Mind, Detecti
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>随机权值平均(Stochastic Weight Averaging,  SWA)</title>
    <link href="http://a-kali.github.io/2020/01/11/%E9%9A%8F%E6%9C%BA%E6%9D%83%E5%80%BC%E5%B9%B3%E5%9D%87-Stochastic-Weight-Averaging-SWA/"/>
    <id>http://a-kali.github.io/2020/01/11/随机权值平均-Stochastic-Weight-Averaging-SWA/</id>
    <published>2020-01-11T01:15:59.000Z</published>
    <updated>2020-01-23T04:57:31.951Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1803.05407" target="_blank" rel="noopener">Averaging Weights Leads to Wider Optima and Better Generalization</a></p><p>SWA是一种较为先进的模型融合方法。传统的模型融合通常使用多个模型进行预测，再使用某种方法来对预测结果取平均值得到最终的预测值。而SWA仅需要训练单个模型即可进行融合。</p><p>SWA一定程度上参考了传统的<strong>快照集成（snapshot ensembling）</strong>，即每训练 20-40 epochs，对局部最优模型保存一个权重快照，然后利用<strong>余弦退火</strong>的特性跳出局部最优，寻找另一个局部最优解；最后使用多个保存的模型权重进行预测并对预测结果进行融合。这种集成方法仅需要训练单个模型就能得出接近多模型融合的效果。</p><p>但快照集成也保留了传统模型融合的部分缺点：需要保存多个模型，较为占用存储空间；训练周期过长，每收敛一个新模型需要经过较多epochs。</p><p>而SWA的作者发现 2-4 epochs <strong>循环学习率</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Averaging Weights Leads to Wider Optima and Better General
      
    
    </summary>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模型融合" scheme="http://a-kali.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>2020年上半年论文阅读计划</title>
    <link href="http://a-kali.github.io/2020/01/06/2020%E5%B9%B4%E4%B8%8A%E5%8D%8A%E5%B9%B4%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/"/>
    <id>http://a-kali.github.io/2020/01/06/2020年上半年论文阅读计划/</id>
    <published>2020-01-06T10:58:49.000Z</published>
    <updated>2020-02-02T07:41:44.376Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。</p><p>目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标跟踪、实例分割、全景分割等），顺便再补一下去年没填的坑（比如LSTM、FPN、YOLOv3、知识蒸馏等）。</p><p><a href="https://arxiv.org/abs/1811.00116" target="_blank" rel="noopener">Face Recognition: From Traditional to Deep Learning Methods</a>：人脸识别综述</p><p><a href="https://arxiv.org/abs/1912.04977" target="_blank" rel="noopener">Advances and Open Problems in Federated Learning</a>：联邦学习综述</p><p><a href="https://arxiv.org/abs/1912.12033" target="_blank" rel="noopener">Deep Learning for 3D Point Clouds: A Survey</a>：3D点云综述</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。&lt;/p&gt;
&lt;p&gt;目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标
      
    
    </summary>
    
      <category term="计划" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="计划" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Bengali.AI Handwritten Grapheme Classification 比赛记录</title>
    <link href="http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/"/>
    <id>http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/</id>
    <published>2020-01-03T14:33:32.000Z</published>
    <updated>2020-01-16T07:02:22.977Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Please enter the password to read the blog." />    <label for="pass">Please enter the password to read the blog.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      The article has been encrypted, please enter your password to view.&lt;br&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="Kaggle" scheme="http://a-kali.github.io/tags/Kaggle/"/>
    
      <category term="比赛记录" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab全家桶（From v1 to v3+）</title>
    <link href="http://a-kali.github.io/2019/12/13/DeepLab%E5%85%A8%E5%AE%B6%E6%A1%B6/"/>
    <id>http://a-kali.github.io/2019/12/13/DeepLab全家桶/</id>
    <published>2019-12-13T15:19:07.000Z</published>
    <updated>2019-12-17T08:59:29.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DeepLabv1"><a href="#DeepLabv1" class="headerlink" title="DeepLabv1"></a>DeepLabv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1412.7062v3" target="_blank" rel="noopener">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>其实挺烦看这种远古论文的，引用的算法现在都不太常见，使用的措辞也和现在不太一样。该论文主要引入了<strong>空洞卷积(Astrous/Dilated Convolution)</strong>和<strong>条件随机场(Conditional Random Field, CRF)</strong>。</p><p>空洞卷积，顾名思义，即是在卷积核权重之间注入空洞，<strong>使用小卷积核的计算量获得大卷积核的感受野</strong>。（如理解有误请邮件指正）</p><p>空洞卷积比传统卷积多一个参数为<strong>采样率(dilation rate)</strong>，表示一个卷积核中采样的间隔。</p><p><img src="https://s2.ax1x.com/2019/12/14/Q2DWex.gif" alt="Q2DWex.gif"></p><p>条件随机场涉及到很多机器学习的知识，学起来比较耗时间，而且在后来的DeepLab版本中被取代，所以此处暂略，有机会再补上。</p><h1 id="DeepLabv2"><a href="#DeepLabv2" class="headerlink" title="DeepLabv2"></a>DeepLabv2</h1><p>论文地址：<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>比起v1，v2的主要改动是增加了<strong>带孔空间金字塔池化(ASPP)</strong>模块，其思想来源于SPPnet。但是文中对ASPP的阐述非常少，完全没有讲清楚ASPP的机制，只能通过论文中的图片和网上的博客来猜。</p><p><img src="https://i.loli.net/2019/12/15/MgRpErQ4utse9ND.png" alt="YM_ISX2ECM53ZW3_4T7HNYJ.png"></p><p><img src="https://i.loli.net/2019/12/15/6S7hpAo3ZiefQBN.png" alt="_H7RQ3P@__TYL_4_87Z0H05.png"></p><p>可以看出，ASPP使用了几种不同采样率的空洞卷积，对一张特征图得出多个分支后，最终concat到一起。我到现在也没搞明白为啥叫“空间金字塔池化”而不是“空间金字塔卷积”。</p><h1 id="DeepLabv3"><a href="#DeepLabv3" class="headerlink" title="DeepLabv3"></a>DeepLabv3</h1><p>论文地址：<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>提出了串联(cascade)和并联(parallel)两种格式，并指出并联效果更好。</p><p><img src="https://i.loli.net/2019/12/16/3PZxML24biRdpGj.png" alt="YF@__L_0M@_0RUKBG_N_O6C.png"></p><p><img src="https://i.loli.net/2019/12/15/tqhbdGpKZIwgf8A.png" alt="_C1PITC8~_S3@U_48_2_L5M.png"></p><p>网络去除了CRF，修改了一些参数，应用了一些新技术（比如批归一化）使模型更加精简。</p><p>虽然从文中看不出做了多少修改，但作者说性能得到了很大的提升。科科。</p><h1 id="DeepLabv3-1"><a href="#DeepLabv3-1" class="headerlink" title="DeepLabv3+"></a>DeepLabv3+</h1><p>论文地址：<a href="https://arxiv.org/abs/1802.02611v1" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>8102年，deeplab终于将Encoder-Decoder结构加进网络里了，之前一直用的双线性插值做上采样。</p><p><img src="https://i.loli.net/2019/12/16/bHvnI59LjUo3JcQ.png" alt="V7U32G_QEI_GOMGI97N6LAG.png"></p><p><img src="https://i.loli.net/2019/12/16/1UiclraR5NuOVvz.png" alt="__ZDD_GOD~3NX9_P_L@HSWA.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://blog.csdn.net/qq_31622015/article/details/90551107" target="_blank" rel="noopener">【语义分割系列：一】DeepLab v1 / v2 论文阅读翻译笔记</a></li><li><a href="https://blog.csdn.net/qq_21997625/article/details/87080576" target="_blank" rel="noopener">语义分割(semantic segmentation)–DeepLabV3之ASPP(Atrous Spatial Pyramid Pooling)代码详解</a></li><li><a href="https://blog.csdn.net/guo_rongxin/article/details/79842895" target="_blank" rel="noopener">deeplab v3论文翻译 Rethinking Atrous Convolution for Semantic Image Segmentation</a></li><li><a href="https://blog.csdn.net/fish_like_apple/article/details/82787705" target="_blank" rel="noopener">Deeplab相关改进的阅读记录（Deeplab V3和Deeplab V3+）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DeepLabv1&quot;&gt;&lt;a href=&quot;#DeepLabv1&quot; class=&quot;headerlink&quot; title=&quot;DeepLabv1&quot;&gt;&lt;/a&gt;DeepLabv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1412.7
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="DeepLab" scheme="http://a-kali.github.io/tags/DeepLab/"/>
    
  </entry>
  
  <entry>
    <title>SENet: Squeeze-and-Excitation Networks</title>
    <link href="http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/"/>
    <id>http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/</id>
    <published>2019-12-08T11:24:08.000Z</published>
    <updated>2019-12-17T09:00:21.295Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SENet 是北京 Momenta 公司研发团队提出的网络结构，该团队凭借SENet以极大的优势获得了 ImageNet 2017 竞赛的图像分类任务冠军。该网络至今(2019.12.12)仍然是最强力的分类网络之一。</p><p>我们从卷积网络开始说起。近些年来，卷积神经网络在很多领域上都取得了巨大的突破。而卷积核作为卷积神经网络的核心，通常被看做是在局部感受野上，将<strong>空间上（spatial）的信息和特征维度上（channel-wise）的信息</strong>进行聚合的信息聚合体。卷积神经网络由一系列卷积层、非线性层和下采样层构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述。</p><p>Inception 结构即是从空间上提取特征的典型案例，其使用不同大小的卷积核，聚合多种不同感受野的特征来提升性能。而 SENet 则是从通道特征之间的关系来考虑，采用了一种<strong>特征重标定(feature recalibration)</strong>的策略，即<strong>通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><h2 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h2><p><img src="https://i.loli.net/2019/12/12/SmgX8AJWzBUvR9s.png" alt="I8_M4J____T1XB_FTW_P_H5.png"></p><p>上图是一个 SE 模块的示意图。</p><ul><li><p><strong>Squeeze</strong>：$F_{sq}(·)$ 将特征图进行<strong>全局平均池化(global average pooling)</strong>，使每个二维的特征通道对应生成一个实数，这个实数某种程度上具有全局的感受野。这意味着当 SE 模块用于靠近输入的层时，也能获得全局的感受野，这一点在很多任务中都是非常有用的。</p></li><li><p><strong>Excitation</strong>：$F_{ex}(·,W)$ 是一些<strong>全连接层和激活函数</strong>，其输入为squeeze的结果，<strong>输出为每个特征通道的重要性</strong>。具体分为4层网络：</p><ul><li>FC1：输入长度为 C 的向量，输出长度为 C/16 的向量。</li><li>ReLU：增加非线性</li><li>FC2：输入长度为 C/16 的向量，输出长度为 C 的向量。</li><li>Sigmoid：将输出限制到(0,1)之间，作为每个通道的权重，表示通道的重要性。</li></ul><p>使用两个而不是一个全连接层的好处在于：</p><ol><li>多加了一个ReLU层，具有更多的非线性，可以更好地拟合通道间复杂的相关性</li><li>极大地减少了参数量和计算量（约为原计算量的1/8）</li></ol></li><li><p><strong>Reweight</strong>：通过乘法将 Excitation 得到的通道权重加权到先前的特征上，完成在通道维度上对原始特征的重标定。</p></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者将 SE 模块拟合到了 Inception 和 ResNet 中，对网络性能产生了较大的增益。</p><p><img src="https://i.loli.net/2019/12/12/myFb9gUAhtNZSxV.png" alt="_LW`_DHP74CK_LED0COG_1P.png"></p><p>在理论上 SE 模块仅增加了网络 1% 的计算量；在实验中，由于 GPU 的架构原因，在 GPU 上的运算时间增加了 10%，而在 CPU 上仅增加了 2%。</p><p><img src="https://i.loli.net/2019/12/12/lzBcfFuq27vDGjw.png" alt="28R__T_T_RPEF6@_EIL4_8F.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/wfei101/article/details/79672944" target="_blank" rel="noopener">Face Paper：SeNet论文详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="SENet" scheme="http://a-kali.github.io/tags/SENet/"/>
    
  </entry>
  
  <entry>
    <title>轻量级卷积神经网络综述：从SqueezeNet到MixNet</title>
    <link href="http://a-kali.github.io/2019/12/05/%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%EF%BC%9A%E4%BB%8ESqueezeNet%E5%88%B0EfficientNet/"/>
    <id>http://a-kali.github.io/2019/12/05/轻量级卷积神经网络综述：从SqueezeNet到EfficientNet/</id>
    <published>2019-12-05T10:40:16.000Z</published>
    <updated>2020-01-06T11:02:30.316Z</updated>
    
    <content type="html"><![CDATA[<p>在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗电量高则会导致汽车、手机等移动端续航能力变差，而只有轻量级的神经网络能解决这个问题。下面我将介绍近年来轻量级卷积神经网络的发展。</p><h1 id="1-SqueezeNet"><a href="#1-SqueezeNet" class="headerlink" title="1    SqueezeNet"></a>1    SqueezeNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1602.07360v2" target="_blank" rel="noopener">SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</a></p><p>SqueezeNet网络的主要亮点在于提出了<strong>Fire Module</strong>来减少参数量。Fire Module 分为两部分：<strong>Squeeze 和 Expand</strong>。Squeeze层通过 1×1 卷积对特征图进行降维，减少参数量，Expand层分别使用 1×1 和 3×3 卷积对降维后的特征图进行处理后concat到一起。比起直接用3×3卷积，这种方法减少了一定的运算量。</p><p><img src="https://i.loli.net/2019/12/05/8TYQwPWMnCU3ok4.png" alt="(R7V7PD.png"></p><p>整个网络由多个Fire Module堆叠而成，很像GoogLeNet。右边两个网络结构参考了ResNet。</p><p><img src="https://i.loli.net/2019/12/05/rW4ugsYPjLUxXVh.png" alt="`YX7Opng"></p><h1 id="2-MobileNet-v1"><a href="#2-MobileNet-v1" class="headerlink" title="2    MobileNet v1"></a>2    MobileNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p><p>MobileNet由谷歌公司提出，主要用于移动和嵌入式视觉应用，其亮点在于采用<strong>深度可分离卷积(Depth-wise Separable Convolution)</strong> 代替传统卷积。</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>深度可分离卷积分为两步：</p><ol><li><strong>Depthwise convolution</strong>：对特征图各个通道进行卷积，每个卷积核只有一个通道且只负责特征图的一个通道。</li><li><strong>Pointwise convolution</strong>：使用1×1卷积将特征图串起来，得到和普通卷积一样的输出。</li></ol><p><img src="https://i.loli.net/2019/12/05/9bdIvHnKpY3XOG8.png" alt="W8I.png"></p><h2 id="运算量对比"><a href="#运算量对比" class="headerlink" title="运算量对比"></a>运算量对比</h2><p>假设输入图像为12×12×3，输出图像为8×8×256。</p><ul><li>Convolution：<ul><li>卷积核大小 5×5×3，卷积核数量 256</li><li>数据量：5×5×3×256 = 19200</li><li>计算量：仅考虑乘法运算，每产生一个输出值就要进行5×5×3次运算，一共要产生8×8×256个输出值，故 5×5×3×256×8×8 = 1228800。</li></ul></li><li>Depthwise Separable Convolution：<ul><li>Depthwise convolution：卷积核大小 5×5×1，卷积核数量 3</li><li>Pointwise convolution：卷积核大小 1×1×3，卷积核数量 256</li><li>数据量：5×5×1×3+1×1×3×256 = 843</li><li>计算量：5×5×1×3×8×8+1×1×3×8×8×256 = 53952</li></ul></li></ul><h2 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h2><p>左边是传统卷积，右边是深度可分离卷积。</p><p><img src="https://i.loli.net/2019/12/05/K2pm9axhdokJOiR.png" alt="FT.png"></p><h2 id="实验结果对比"><a href="#实验结果对比" class="headerlink" title="实验结果对比"></a>实验结果对比</h2><p>可以看到MobileNet在只牺牲了少量精确度的情况下节约了大量的运算量和网络参数。</p><p><img src="https://i.loli.net/2019/12/05/YwsgB2EQ7cxT1XC.png" alt="O.png"></p><h1 id="3-Xception"><a href="#3-Xception" class="headerlink" title="3    Xception"></a>3    Xception</h1><p>论文地址：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolutions</a></p><p>Xception 借鉴了深度可分离卷积的思想并以此改进了Inception V3。</p><p><img src="https://i.loli.net/2019/12/05/Ht6YxQXpTvRbVA2.png" alt="Fpng"></p><p>图中是一个Xception模块，先用 1×1 卷积改变特征图的通道数，再对输出的每个通道分别进行 3×3 卷积，最后将 3×3 卷积的输出concat到一起。</p><h1 id="4-ShuffleNet-v1"><a href="#4-ShuffleNet-v1" class="headerlink" title="4    ShuffleNet v1"></a>4    ShuffleNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p><p>ShuffleNet是由旷视公司提出的轻量级网络，该网络结构主要使用了<strong>分组卷积(group convolution)</strong>和<strong>通道洗牌(channel shuffle)</strong>。</p><p><img src="https://i.loli.net/2019/12/05/tsFGvMo17V6QuIg.png" alt="MBng"></p><p>图a展示了分组卷积，即将通道均等分为多组，分别进行卷积操作（类似于深度可分离卷积）。但这样会导致组之间的信息不流通，对精度造成影响。于是使用通道洗牌的方式，对各组的通道进行交换。</p><p>下图是两种ShuffleNet单元：</p><p><img src="https://i.loli.net/2019/12/05/iNdkjWleXIDqsFK.png" alt="7EYQpng"></p><h1 id="5-MobileNet-v2"><a href="#5-MobileNet-v2" class="headerlink" title="5    MobileNet v2"></a>5    MobileNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p><h2 id="MobileNet-v1-存在问题"><a href="#MobileNet-v1-存在问题" class="headerlink" title="MobileNet v1 存在问题"></a>MobileNet v1 存在问题</h2><p>MobileNet v1 虽然很好地降低了模型运算量，但依然存在如下两个问题：</p><ol><li>MobileNet v1 的结构是类似于 VGG 的堆叠结构，而这种结构比起后来的 ResNet、GoogLeNet 来说性能不高。</li><li>Depthwise Convolution 的潜在问题：论文作者发现，由于<strong>深度残差卷积产生的特征图通道数较少，在 ReLU 的影响下很容易产生较大的信息损耗</strong>（这个故事告诉我们不要在压缩通道后用ReLU）。</li></ol><h2 id="MobileNet-v2-的创新点"><a href="#MobileNet-v2-的创新点" class="headerlink" title="MobileNet v2 的创新点"></a>MobileNet v2 的创新点</h2><p>为了解决 v1 存在的问题，v2 提出了以下改进方法：</p><ol><li><p><strong>Inverted Residual Block</strong>：首先从名字可以看出，这是从传统残差块演化而来的<strong>逆残差</strong>，两者主要的不同在于对 1×1 卷积的运用方式不同。传统的残差块使用 1×1 卷积降低特征图的通道数，减少 3×3 卷积的运算量；而逆残差则是用 1×1 卷积来提升维度，以便提升网络的准确度。可能作者觉得反正 Depthwise Convolution 运算量也不大，不如就牺牲一丢丢速度来提高一下精度吧。</p><p><img src="https://i.loli.net/2019/12/08/cBeWJVdTSU9t5Eb.png" alt="Q_P`KE8N_V3JY_L``4O6CXS.png"></p></li><li><p><strong>Linear Bottlenecks</strong>：对比 v1 和 v2 的结构可以看出，v2 使用线性函数替换了 v1 模块最后的ReLU6：</p></li></ol><p><img src="https://i.loli.net/2019/12/11/aSftLEykhVcC19I.png" alt="H_T83__0YR5Q6K_Y~V8_M_S.png"></p><h1 id="6-ShuffleNet-v2"><a href="#6-ShuffleNet-v2" class="headerlink" title="6    ShuffleNet v2"></a>6    ShuffleNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1807.11164" target="_blank" rel="noopener">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常用于神经网络的设计指导指标使用的是计算复杂度衡量指标：<strong>FLOPs</strong>，而不是更直接的评价指标：<strong>运行速度</strong>(speed)。而作者发现相同FLOPs的网络速度可能差别很大，认为FLOPs并不能作为网络性能的唯一衡量指标。</p><p>造成FLOPs和速度不成比例的原因：</p><ol><li>部分影响速度的原因没有被FLOPs包含在内：<ul><li><strong>内存访问成本</strong>(memory access cost, <strong>MAC</strong>)：这会使得强大的GPU算力受到限制。</li><li><strong>并行度</strong>(degree of parallelism)：在相同FLOPs的情况下，并行度高的网络模型速度远高于低并行度模型。</li></ul></li><li>不同的运行平台会影响FLOPs。比如说新版的CUDNN专门对 3 × 3 卷积运算进行了优化。</li></ol><p>出于这点考虑，作者提出了两点高效结构设计的指导性原则：</p><ol><li>应当使用直接的评价指标（e.g., 速度）而不是间接的（e.g., FLOPs）。</li><li>应当在规定的平台上进行评估。</li></ol><h2 id="高效卷积网络设计准测"><a href="#高效卷积网络设计准测" class="headerlink" title="高效卷积网络设计准测"></a>高效卷积网络设计准测</h2><ul><li><strong>G1: 当输入、输出channels数目相同时，conv计算所需的MAC最低。</strong>以深度可分离卷积(Depth-wise Separable Convolution)为例，其 pointwise convlution (i.e., 1×1 conv) 部分占用了其大部分复杂度。设$c_1$，$c_2$为 1 × 1 卷积的输入、输出通道数，$h$和$w$为特征图的高和宽，则 FLOPs 计算为 $B=hwc_1c_2$。内存访问操作次数为 $MAC=hw(c_1+c_2)+c_1c_2$。得出下面的不等式，仅当输入输出通道数相同时，MAC最小：<br>$$<br>MAC\geq 2\sqrt{hwB}+\frac{B}{hw}<br>$$</li><li><strong>G2: 过多的分组卷积(Group Convolution)会增大 MAC 开销。</strong>设分组数量为 $g$，从下面公式可以看出随着 $g$ 增加，MAC增加。  ：</li></ul><p>$$<br>MAC=hwc_1+\frac{Bg}{c_1}+\frac{B}{hx}<br>$$</p><ul><li><strong>G3: 网络碎片化(fragmentation)会减少并发度。</strong>这里的碎片化大概指的是模型的分支数量。比如说 NASNET-A 的分支数就高达13，而 ResNet 的分支数为2或3。作者通过实验证明，分支数量的提升会提高网络的准确率，但也会因降低GPU并行计算能力而影响效率。</li><li><strong>G4: Element-wise 操作的计算量不容忽视。</strong>element-wise包括激活、张量相加、添加偏置等，它们的共同特征就是FLOPS较小但是MAC相对较大。同时作者将 depthwise convolution 操作也算入了element-wise，因为其有着同样高的 MAC/FLOP 比率。</li></ul><p>目前的轻量级网络结构主要是是以FLOPS作为度量标准设计的，而没有考虑以上的几点属性。比如说，ShuffleNet v1使用了过多的分组卷积(与G2违背)、bottleneck-like块(与G1违背)；MobileNet v2使用倒置的bottleneck结构(与G1违背)，同时使用了深度卷积和ReLU在”thick”特征图上(与G4违背)；自动生成结构过多的使用了碎片化结构(与G3违背)</p><h2 id="ShuffleNet-V2-网络结构"><a href="#ShuffleNet-V2-网络结构" class="headerlink" title="ShuffleNet V2 网络结构"></a>ShuffleNet V2 网络结构</h2><p>为了使ShuffleNet更加高效，关键在于保持等宽的出入输出通道，以及使用密集卷积操作而不是过多的分组卷积。</p><p><img src="https://i.loli.net/2019/12/11/3YuOe1yB8zalwjL.png" alt="S9YD9ZB_OHNUS`GHFVS@9D0.png"></p><p>如图，左边两个是 ShuffleNet v1 的模块，右边两个是 ShuffleNet v2 的模块。</p><p>图c是 ShuffleNet v2 的基本模块，其首先将输入的通道随机split成两部分（这是一种变相的分组卷积，不过只分了两个组，遵守了G2和G3），一部分恒等映射到模块尾部，另一部分通过三个输入输出通道数相同的卷积前向传播（遵守了G1），之后使用concat操作（而不是add操作，遵守了G4）将两个分支结合在一起，最后进行通道洗牌(channel shuffle)。</p><p>图d为下采样模块，原理类似，stride=2缩小特征图，没有使用channel split操作，最后两个分支concat到一起使通道数翻倍。</p><p>恒等映射后concat到模块尾部，能使特征得到复用，提高准确度。这种思想来源于DenseNet。</p><h1 id="7-MnasNet（待更新）"><a href="#7-MnasNet（待更新）" class="headerlink" title="7    MnasNet（待更新）"></a>7    MnasNet（待更新）</h1><p>太复杂了，回头再看</p><h1 id="8-MobileNet-v3（待更新）"><a href="#8-MobileNet-v3（待更新）" class="headerlink" title="8    MobileNet v3（待更新）"></a>8    MobileNet v3（待更新）</h1><p>基于MnasNet</p><h1 id="9-MixNet（待更新）"><a href="#9-MixNet（待更新）" class="headerlink" title="9    MixNet（待更新）"></a>9    MixNet（待更新）</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://baijiahao.baidu.com/s?id=1589005428414488177&wfr=spider" target="_blank" rel="noopener">纵览轻量化卷积神经网络：SqueezeNet、MobileNet、ShuffleNet、Xception</a></li><li><a href="https://www.jianshu.com/p/fdd7d7353c55" target="_blank" rel="noopener">SqueezeNet | 轻量级深层神经网络</a></li><li><a href="https://www.greedyai.com/" target="_blank" rel="noopener">贪心学院</a></li><li><a href="https://blog.csdn.net/lk3030/article/details/84847879" target="_blank" rel="noopener">Xception</a></li><li><a href="https://blog.csdn.net/kangdi7547/article/details/81431572" target="_blank" rel="noopener">轻量级模型：MobileNet V2</a></li><li><a href="https://www.jianshu.com/p/71e32918ea0a?utm_source=oschina-app" target="_blank" rel="noopener">精简CNN模型系列之六：ShuffleNet v2</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/81322175" target="_blank" rel="noopener">ShuffleNet v2算法笔记</a></li><li><a href="https://blog.csdn.net/h__ang/article/details/88618089" target="_blank" rel="noopener">ShuffleNet_v2模型解读</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="轻量级网络" scheme="http://a-kali.github.io/tags/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SqueezeNet" scheme="http://a-kali.github.io/tags/SqueezeNet/"/>
    
      <category term="MobileNet" scheme="http://a-kali.github.io/tags/MobileNet/"/>
    
      <category term="Xception" scheme="http://a-kali.github.io/tags/Xception/"/>
    
      <category term="ShuffleNet" scheme="http://a-kali.github.io/tags/ShuffleNet/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
      <category term="深度可分离卷积" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>SSD: Single-shot detectors</title>
    <link href="http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/"/>
    <id>http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/</id>
    <published>2019-12-04T08:49:35.000Z</published>
    <updated>2019-12-17T09:01:49.867Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文提出了一个基于深度神经网络的<strong>单步(single shot)目标检测器SSD</strong>，其在继承了YOLO单步预测高检测速度的同时，拥有不弱于Faster R-CNN的准确度。</p><h1 id="SSD-网络结构"><a href="#SSD-网络结构" class="headerlink" title="SSD 网络结构"></a>SSD 网络结构</h1><p><img src="https://i.loli.net/2019/12/04/qIhrwjU6uRMV4Nn.png" alt="png"></p><p>从图中可以看出：</p><ul><li>不同于YOLOv1和Faster R-CNN，SSD是一个全卷积网络。</li><li>SSD的预测结果并不完全由最后一层输出，而是由其5个<strong>额外特征层(Extra Feature Layers)</strong>和 VGG16中的一层的输出综合而来。</li><li>由于SSD是个全卷积网络，所以其分类操作也由卷积层进行。上图中横向的直线即是<strong>卷积分类器</strong>，卷积核大小为3×3，channel数量为anchors×(Classes+4)。此处anchors指anchor的数量；classes为类别数，预测值为每个类置信度，这点应该会给后面的NMS作为评判标准；+4就是(x,y,w,h)。</li><li>SSD的输出特征图平均每个像素都有一组anchor，整个网络共生成8732个anchor，远多于YOLO和Faster R-CNN。（这里有个问题，根据上面一条，使用3×3卷积核作为滑动窗口是没法做到每个像素都有anchor的，所以此处应该有padding）</li></ul><p><img src="https://i.loli.net/2019/12/04/To4hNmlKez1CrqV.png" alt="2Bng"></p><h1 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h1><p>检测结果中，未被选为最终结果的样本都是负样本。这导致负样本数量远大于正样本，样本不均衡。作者采用<strong>Hard negative mining</strong>的方式，仅选用被误认为是正样本可能性更大的负样本。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>论文中还提到了损失函数和anchor的选择，但跟其它的目标检测网络差不多，就不再赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="SSD" scheme="http://a-kali.github.io/tags/SSD/"/>
    
  </entry>
  
</feed>
