<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-12-12T20:43:04.577Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FLOPs与参数量的计算</title>
    <link href="http://a-kali.github.io/2020/12/13/FLOPs%E4%B8%8E%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97/"/>
    <id>http://a-kali.github.io/2020/12/13/FLOPs与参数量的计算/</id>
    <published>2020-12-12T20:36:07.000Z</published>
    <updated>2020-12-12T20:43:04.577Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h1><p><strong>FLOPs(floating point operations)</strong>，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。注意不要和FLOPS(floating point operations per second)搞混。</p><p>拿卷积操作举例。这里需要注意，卷积的计算量如果从output map size考虑的话，是可以与input map size无关的。因为卷积的每一次矩阵相乘都一定输出output map的一个像素点，但如果从input map下手的话，还需要考虑stride和padding。</p><p>所以<strong>卷积层</strong>的FLOPs计算公式为：</p><script type="math/tex; mode=display">FLOPs=(2×C_i×K^2)×(H×W×C_o)</script><p>其中Ci和Co分别为输入输出的通道数，K为卷积核大小，H和W分别为output map的高和宽。</p><p>这个公式前半部分的运算就是每计算一个输出元素所需要的计算量，就是output map的元素数量。之所以要×2是因为一次卷积运算需要进行乘法和加法两种操作（相乘后求和），比如说两个3×3大小的矩阵相乘求和得到一个元素值，需要进行3×3次乘法操作和3×3-1次加法操作，再加上神经网络中的bias，运算量一共为2×3×3。</p><p>同理可得<strong>全连接层</strong>的FLOPs计算公式为：</p><script type="math/tex; mode=display">FLOPs=2×I×O</script><h1 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h1><p>卷积参数量的计算比较简单，这里带过一下：</p><script type="math/tex; mode=display">params=(K^2×C_i+1)×C_o</script><p>实际工程上要计算这两个值的话的话，感觉github上有挺多包的……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FLOPs&quot;&gt;&lt;a href=&quot;#FLOPs&quot; class=&quot;headerlink&quot; title=&quot;FLOPs&quot;&gt;&lt;/a&gt;FLOPs&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;FLOPs(floating point operations)&lt;/strong&gt;，意指浮点运算数，
      
    
    </summary>
    
    
      <category term="FLOPs" scheme="http://a-kali.github.io/tags/FLOPs/"/>
    
      <category term="参数量" scheme="http://a-kali.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV中的平滑、腐蚀膨胀和边缘检测</title>
    <link href="http://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/13/OpenCV中的平滑、腐蚀膨胀和边缘检测/</id>
    <published>2020-12-12T20:32:16.000Z</published>
    <updated>2020-12-12T20:33:26.691Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像平滑与滤波"><a href="#图像平滑与滤波" class="headerlink" title="图像平滑与滤波"></a>图像平滑与滤波</h1><p>图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，这些图像滤波都是一些简单的卷积操作。</p><ul><li>均值滤波：即在一个卷积核的感受野内的平均值作为该像素点的值，opencv代码为<code>cv2.blur(img, (3, 3))</code>。</li><li>高斯滤波：高斯滤波会根据距离取周围的像素点加权和，越近的像素权重越大。opencv代码为<code>cv2.GaussianBlur</code>。由于高斯滤波对中心点权重最高，使得其对椒盐类的噪声处理效果较差。</li><li>中值滤波：即在一个卷积核的感受野内的中值作为该像素点的值，opencv代码为<code>cv2.midianBlur(img, (3, 3))</code>。中值滤波对中心点的取值较为绝对化，故对椒盐类的噪声处理能力较强。</li></ul><p>下面三张图分别为原图、高斯滤波结果、中值滤波结果。</p><p><img src="https://i.loli.net/2020/12/05/tHxuK4iXhCP85cz.png" alt="image.png"></p><h1 id="图像的形态学操作"><a href="#图像的形态学操作" class="headerlink" title="图像的形态学操作"></a>图像的形态学操作</h1><h2 id="1-腐蚀与膨胀"><a href="#1-腐蚀与膨胀" class="headerlink" title="1    腐蚀与膨胀"></a>1    腐蚀与膨胀</h2><p>进行膨胀(dilation)操作时，使用卷积核遍历图像，使用内核覆盖区域的最大相素值代替锚点位置的相素。这一最大化操作将会导致图像中的亮区扩大，因此名为膨胀 。腐蚀(erosion)则与膨胀相反。在OpenCV中代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line">cv2.erode(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 腐蚀</span></span><br><span class="line">cv2.dilate(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 膨胀</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/05/XHDSuCR7yxOYIin.png" alt="image.png"></p><h2 id="2-开运算与闭运算"><a href="#2-开运算与闭运算" class="headerlink" title="2    开运算与闭运算"></a>2    开运算与闭运算</h2><p>开运算指的是先腐蚀后膨胀的操作，闭运算则是先膨胀后腐蚀的操作。通常用于消除噪点、沟壑，平滑物体轮廓。在OpenCV中代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # 开运算</span><br><span class="line">cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)  # 闭运算</span><br></pre></td></tr></table></figure><h2 id="3-梯度计算"><a href="#3-梯度计算" class="headerlink" title="3    梯度计算"></a>3    梯度计算</h2><p>使用膨胀的结果减去腐蚀的结果可以计算得到图像的梯度，但OpenCV中提供了更便捷的操作：<code>cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</code>。</p><h1 id="Sobel算子"><a href="#Sobel算子" class="headerlink" title="Sobel算子"></a>Sobel算子</h1><p>Sobel算子是计算机视觉领域的一种重要处理方法。主要用于获得数字图像的<strong>一阶梯度</strong>，常用于<strong>边缘检测</strong>。Sobel算子是把图像中每个像素的上下左右四领域的<strong>灰度值加权差</strong>，在边缘处达到极值从而检测边缘。</p><p>该算子包含两组3x3的矩阵，分别为横向及纵向，将之与图像作平面<strong>卷积</strong>，即可分别得出横向及纵向的差值。如果以A代表原始图像，Gx及Gy分别代表经横向及纵向边缘检测的图像，其公式如下：</p><p><img src="https://i.loli.net/2020/12/04/rIq3OVLCfDnWvpu.png" alt="image.png"></p><p>在OpenCV中，可通过<code>cv2.Sobel(src, ddepth, dx, dy, ksize)</code>调用Sobel算子。具体操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'pic.png'</span>, cv2.IMREAD_GRAYSCALE)  <span class="comment"># 读取灰度图</span></span><br><span class="line">sobelx = cv2.Sobel(img, cv2.CV_64F, <span class="number">1</span>, <span class="number">0</span>, ksize=<span class="number">3</span>)  <span class="comment"># x方向的梯度</span></span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)  <span class="comment"># 取绝对值将复数转为正数</span></span><br><span class="line">sobely = cv2.Sobel(img, cv2.CV_64F, <span class="number">0</span>, <span class="number">1</span>, ksize=<span class="number">3</span>)  <span class="comment"># y方向的梯度</span></span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x和y方向分别求到的梯度加权求和</span></span><br><span class="line">sobelxy = cv2.addWeighted(sobelx, <span class="number">0.5</span>, sobely, <span class="number">0.5</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>求得的结果如下所示，其中左边是原图，右边是边缘检测结果：</p><p><img src="https://i.loli.net/2020/12/04/CgwknPWiDm12b7x.png" alt="image.png"></p><p>因为直接求两个方向的梯度效果较差，所以此处是对两个方向分别求梯度再加权平均，而不是直接求两个方向的梯度。</p><h1 id="Canny边缘检测"><a href="#Canny边缘检测" class="headerlink" title="Canny边缘检测"></a>Canny边缘检测</h1><p>Canny边缘检测算法是对图像进行的一系列用于边缘检测的操作步骤：</p><ol><li>使用高斯滤波器对图像进行平滑处理，滤除噪声；</li><li>计算图像中每个像素点的梯度强度和方向；</li><li>使用非极大值抑制消除边缘检测带来的杂散效应；</li><li>应用双阈值检测找出潜在的边缘，抑制孤立弱边缘。</li></ol><p><img src="https://i.loli.net/2020/12/05/f3BTRk6cFPYgqHG.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像平滑与滤波&quot;&gt;&lt;a href=&quot;#图像平滑与滤波&quot; class=&quot;headerlink&quot; title=&quot;图像平滑与滤波&quot;&gt;&lt;/a&gt;图像平滑与滤波&lt;/h1&gt;&lt;p&gt;图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，
      
    
    </summary>
    
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】手撕卷积</title>
    <link href="http://a-kali.github.io/2020/12/03/%E6%89%8B%E6%92%95%E5%8D%B7%E7%A7%AF/"/>
    <id>http://a-kali.github.io/2020/12/03/手撕卷积/</id>
    <published>2020-12-03T09:18:27.000Z</published>
    <updated>2020-12-03T09:19:44.350Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(z, K, b, padding=<span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多通道卷积前向过程</span></span><br><span class="line"><span class="string">    :param z: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数</span></span><br><span class="line"><span class="string">    :param K: 卷积核,形状(C,D,k1,k2), C为输入通道数，D为输出通道数</span></span><br><span class="line"><span class="string">    :param b: 偏置,形状(D,)</span></span><br><span class="line"><span class="string">    :param padding: padding</span></span><br><span class="line"><span class="string">    :param strides: 步长</span></span><br><span class="line"><span class="string">    :return: 卷积结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    padding_z = np.lib.pad(z, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (padding[<span class="number">0</span>], padding[<span class="number">0</span>]), (padding[<span class="number">1</span>], padding[<span class="number">1</span>])), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)  <span class="comment"># 在H和W维度上进行padding</span></span><br><span class="line">    N, _, height, width = padding_z.shape</span><br><span class="line">    C, D, k1, k2 = K.shape</span><br><span class="line">    conv_z = np.zeros((N, D, <span class="number">1</span> + (height - k1) // strides[<span class="number">0</span>], <span class="number">1</span> + (width - k2) // strides[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> np.arange(N):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> np.arange(D):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> np.arange(height - k1 + <span class="number">1</span>)[::strides[<span class="number">0</span>]]:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> np.arange(width - k2 + <span class="number">1</span>)[::strides[<span class="number">1</span>]]:</span><br><span class="line">                    conv_z[n, d, h // strides[<span class="number">0</span>], w // strides[<span class="number">1</span>]] = np.sum(padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]</span><br><span class="line">    <span class="keyword">return</span> conv_z</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="卷积" scheme="http://a-kali.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>CenterLoss简述</title>
    <link href="http://a-kali.github.io/2020/12/03/CenterLoss%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/03/CenterLoss简述/</id>
    <published>2020-12-03T09:18:01.000Z</published>
    <updated>2020-12-12T20:43:25.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h1><p>对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图为神经网络在MNIST数据集上进行手写体数字分类任务得到的特征分布，在类与类之间能看到很明显的界限，能够便于softmax对特征进行分类。但是仍然可以看出部分不同类的样本距离很近，而同类的样本分布距离较远，这有可能对最终的分类结果有影响。</p><p><img src="https://i.loli.net/2020/12/02/4M5jQxUcpW3H6G1.png" alt="image.png"></p><p>而<strong>Center Loss</strong>的提出就是为了<strong>缩小类内(intra-class)距离</strong>。其公式表示如下：</p><script type="math/tex; mode=display">L_c=\frac{1}{2}\sum^m_{i=1}‖x_i-c_{yi}‖^2_2</script><p>其中$c_{yi}$表示第y个类别的特征中心，$x_i$表示全连接层之前的特征，$m$表示mini-batch的大小。该损失函数试图使每个样本的特征向其类别中心靠近。</p><p>至于类中心怎么更新迭代，这里就不写了（公式太复杂了yingyingying）。反正最终得到的特征分布如下：</p><p><img src="https://i.loli.net/2020/12/02/pdn8rZzMEN7OaoA.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Center-Loss&quot;&gt;&lt;a href=&quot;#Center-Loss&quot; class=&quot;headerlink&quot; title=&quot;Center Loss&quot;&gt;&lt;/a&gt;Center Loss&lt;/h1&gt;&lt;p&gt;对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图
      
    
    </summary>
    
    
      <category term="损失函数" scheme="http://a-kali.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>CycleGAN简述</title>
    <link href="http://a-kali.github.io/2020/11/19/CycleGAN%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/11/19/CycleGAN简述/</id>
    <published>2020-11-19T04:18:04.000Z</published>
    <updated>2020-11-19T04:19:12.186Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks)</p><p>PyTorch代码：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p><p>CycleGAN（或许）是首个将GAN用于风格迁移的网络。</p><p>CycleGAN的作用本质是Image2Image的风格迁移，并不是传统意义上的生成。</p><p><img src="https://i.loli.net/2020/11/16/x2lpJWfVCsqhyja.png" alt="image.png"></p><p>相比于其它Image2Image的网络，CycleGAN并不需要成对的样本数据用来训练，而仅仅需要两组无关的图片，其中目标样本通常为一组风格相似的图片集。</p><p><img src="https://i.loli.net/2020/11/16/Af6MzV79s8NCoTE.png" alt="image.png"></p><p>为了避免网络将X中所有样本映射到Y中的某一个样本，CycleGAN同时包含Y→X的映射，要求网络能够对自己生成的图片进行还原。这使得F(X)必须包含X的内容信息，因此更容易学习Y整体的风格信息（大概）。</p><p><img src="https://i.loli.net/2020/11/16/ivueyVzcxMwRrAk.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="GAN" scheme="http://a-kali.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】如何进行多标签分类</title>
    <link href="http://a-kali.github.io/2020/11/12/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/"/>
    <id>http://a-kali.github.io/2020/11/12/如何进行多标签分类/</id>
    <published>2020-11-11T16:37:46.000Z</published>
    <updated>2020-11-11T16:38:38.377Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：多标签分类怎么解决？</strong></p><p>解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。</p><p>第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这应该是最简单常见的方法，但该方法忽略了标签与标签之间的相关性。</p><p>第二种是使用CNN+RNN+Embedding的策略（参考文献：<a href="https://arxiv.org/abs/1604.04573" target="_blank" rel="noopener">CNN-RNN: A Unified Framework for Multi-label Image Classification</a>）。该方法考虑到了各个标签之间的关联性，将每个标签映射到高维空间上。每次运行预测一个标签，并根据该标签预测下一个标签。理论上该方法的准确率会比较高，但时间效率很低。</p><p><img src="https://i.loli.net/2020/11/11/S5p13cqzibZQfK2.png" alt="image.png"></p><p><a href="https://www.kaggle.com/c/imet-2019-fgvc6" target="_blank" rel="noopener">Kaggle: iMet Collection 2019 - FGVC6</a> 大都会文物分类竞赛就是多标签分类任务，几乎所有参赛者都使用第一种方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：多标签分类怎么解决？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。&lt;/p&gt;
&lt;p&gt;第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>DB：基于可微二值化的实时场景文本检测</title>
    <link href="http://a-kali.github.io/2020/11/12/DB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8F%AF%E5%BE%AE%E4%BA%8C%E5%80%BC%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/11/12/DB：基于可微二值化的实时场景文本检测/</id>
    <published>2020-11-11T16:36:11.000Z</published>
    <updated>2020-12-01T16:38:24.931Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1911.08947" target="_blank" rel="noopener">Real-time Scene Text Detection with Differentiable Binarization</a></p><p>Github：<a href="https://github.com/MhLiao/DB（代码结构很复杂）" target="_blank" rel="noopener">https://github.com/MhLiao/DB（代码结构很复杂）</a></p><p><strong>Structure</strong></p><p>DB(Differentiable Binarization)是一个轻量级的、基于分割的场景文本检测(Scene Text Detection, STD)模型。该模型的原理简洁易懂，在本文中就简单介绍一下。</p><p><img src="https://i.loli.net/2020/11/04/MF73kDbCcueraNL.png" alt="image.png"></p><p>上图展示了DB的模型结构。前半部分是常见的32倍下采样+8倍上采样+特征融合，比较特殊的是该模型使用了后四层卷积的输出concat到一起进行预测。图中的pred是3×3卷积，输出两张map分别用来表示概率和阈值。使用阈值map对概率map进行二值化后，就将输出结果分为了前景（文本域）和背景。</p><p>这个自适应阈值能把分布比较密集的文本域给隔开，避免混淆成一个文本域。</p><p>值得一提的是阈值map只在训练的时候使用，在测试时仅使用固定阈值。估计在测试时使用DB并不能得到比较有效的提升，出于运算量的考虑决定去掉这部分。</p><p><strong>Loss</strong></p><p>损失函数分为三部分：概率图损失，阈值损失，二值图损失。其中概率图和二值图都使用交叉熵损失函数，而阈值损失使用的是L1损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1911.08947&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-time Scene Text Detection with Differentiable Binariz
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本检查" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%9F%A5/"/>
    
      <category term="DB" scheme="http://a-kali.github.io/tags/DB/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Batch Normalization</title>
    <link href="http://a-kali.github.io/2020/11/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/11/12/神经网络中的归一化/</id>
    <published>2020-11-11T16:32:30.000Z</published>
    <updated>2020-12-12T22:08:33.046Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：BN解决了什么问题？</strong></p><p>解决两个问题：</p><ol><li>Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。</li><li>梯度消失：由于之前Sigmoid一类的激活函数的存在，数据在网络中传播时整体分布逐渐往非线性函数的取值区间的上下限两端靠近，导致反向传播时低层神经网络的梯度消失，神经网络收敛变慢。</li></ol><p><strong>Q：BN的运作方式</strong></p><p>通过一定的规范化手段，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布。让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。同时固定该层的输入分布，使后一层的神经元不用反复重新适应分布的变化。</p><p>但经过这一步后大部分值落入激活函数的线性区内，使得激活函数失去了其本身的非线性意义，网络表达能力下降。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了缩放平移操作(y=scale*x+shift)。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</p><p><strong>Q：手撕BN</strong></p><p>BatchNorm2D（常用于卷积神经网络）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm2D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channel, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = np.zeros(channel) <span class="comment"># 用于测试时</span></span><br><span class="line">        self.running_var = np.ones(channel)   <span class="comment"># 同上</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps                        <span class="comment"># 接近于0的数，用于避免分母为0</span></span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># input.shape: (B, C, H, W)</span></span><br><span class="line">        len_ch = input.size(<span class="number">1</span>)</span><br><span class="line">        output = np.zeros(input.size())</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len_ch):</span><br><span class="line">            in_ch = input[:, i, :, :]</span><br><span class="line">            total_elem = in_ch.numel()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">                mean = in_ch.sum() / total_elem</span><br><span class="line">                var = ((in_ch - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">                out_ch = (in_ch - mean) / (var + self.eps) ** <span class="number">0.5</span>  <span class="comment"># 归一化</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新参数</span></span><br><span class="line">                var_unbiased = ((in_ch - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">                self.running_mean[i] = self.running_mean[i] * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">                self.running_var[i] = self.running_var[i] * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_ch = (in_ch - self.running_mean[i]) / (self.running_var[i] + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">            out_ch = self.gamma * out_ch + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">            output[:, i, :, :] = out_ch</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>BatchNorm1D大概也能根据以上代码进行修改（我瞎写的，仅供参考）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm1D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = <span class="number">0</span></span><br><span class="line">        self.running_var = <span class="number">1</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps           </span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line"></span><br><span class="line">        total_elem = input.numel()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">            mean = input.sum() / total_elem</span><br><span class="line">            var = ((input - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">            output = (input - mean) / (var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            var_unbiased = ((input - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">            self.running_mean = self.running_mean * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">            self.running_var = self.running_var * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = (input - self.running_mean) / (self.running_var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">        output = self.gamma * output + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><strong>Q：BN能防止过拟合吗？为什么？</strong></p><p>BN能一定程度上缓解过拟合。BN使得模型在训练时的输出不仅仅根据当前的输入样本信息，还包含了同一batch其他样本的信息。训练时，同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的。相当于在神经网络中进行了数据增强。</p><p><strong>Q：BN 有哪些参数？</strong></p><p>可训练的参数有缩放因子和平移因子，统计参数有均值和方差，超参数有动量，2D的超参数还包含通道数。</p><p><strong>Q：BN 的反向传播推导</strong></p><ul><li><input disabled type="checkbox"> TODO</li></ul><p><strong>Q：BN 在训练和测试的区别？</strong></p><p>训练时使用的是当前batch的样本统计量进行归一化，测试时使用的是在训练过程中更新迭代计算得到的均值和方差进行归一化。</p><p><strong>Q：BN通常放在什么位置？</strong></p><p>BN通常放在激活函数前。因为BN的作用本来就是为了调整上一层的输出分布，让激活层更好地使用这些输出值。</p><p><strong>Q：BN可以防止过拟合吗？</strong></p><p>BN可以一定程度上缓解过拟合。在样本shuffle训练的情况下，某个样本在不同epoch遇到的同一个batch的其他样本都是不一样的，于是会产生不同的均值和标准差，相当于在模型内部做了数据增强。</p><p><strong>Q：BN和Dropout同时用会怎样？怎样才能同时使用？</strong></p><p>Dropout在测试阶段会根据神经元保留率来修改神经元权重，这会导致隐藏层输出值的方差跟训练时不同。而BN此时已经根据训练数据统计固定了方差参数，无法适应改变后的方差。多层累积下来产生方差偏移，影响模型效果。所以只有在Dropout在所有BN后面时能同时使用。</p><p><strong>Q：有什么其它的归一化方法？</strong></p><ul><li>IN(Instance Norm)：实例归一化。与BN的区别在于，BN使用整个batch的统计量作为参数进行归一化，而IN仅使用当前样本的统计量。IN常用于风格迁移任务中。</li><li><a href="https://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/#more">AdaIN</a>：自适应的实例归一化。在IN的基础上，将缩放和平移参数分别固定为目标风格图像的标准差和均值。在风格迁移中可以快速适应任意风格。</li><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">LN(Layer Norm)</a>：与BN的区别在于，BN是对于一个batch样本的单个通道进行归一化，LN是对单个样本的所有通道进行归一化。可用于RNN或者小batch。</li><li>GN(Group Norm)：组归一化。和LN类似，比LN多一个超参数G，G表示分组的数量。同样用来解决在小batch时BN效果较差的问题。</li></ul><p><img src="https://i.loli.net/2020/11/11/ZtDTfcHxUvyeioj.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：BN解决了什么问题？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
      <category term="BN" scheme="http://a-kali.github.io/tags/BN/"/>
    
      <category term="Normalization" scheme="http://a-kali.github.io/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】IoU和mIoU</title>
    <link href="http://a-kali.github.io/2020/11/03/%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91IoU%E5%92%8CmIoU/"/>
    <id>http://a-kali.github.io/2020/11/03/【面试题】IoU和mIoU/</id>
    <published>2020-11-03T14:53:10.000Z</published>
    <updated>2020-11-11T16:31:05.748Z</updated>
    
    <content type="html"><![CDATA[<p>害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。</p><p>虽然老夫从来没做过基于检测框的目标检测项目。</p><p><strong>Q1：啥是IoU？如何计算IoU？</strong></p><p>IoU就是交并比嘛，两个框相交的面积除以合并的面积。</p><p>定义bbox1, bbox2为两个长度为 4 的数组，用于表示两个检测框左上和右下坐标点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BoxIoU</span><span class="params">(bbox1, bbox2)</span>:</span></span><br><span class="line">x11, y11, x12, y12 = bbox1</span><br><span class="line">    x21, y21, x22, y22 = bbox2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相交面积</span></span><br><span class="line">    iw = max(min(x12, x22) - max(x11, x21), <span class="number">0</span>)</span><br><span class="line">    iy = max(min(y12, y22) - max(y11, y21), <span class="number">0</span>)</span><br><span class="line">    inter = iw * iy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算合并面积</span></span><br><span class="line">area1 = (x12 - x11) * (y12 - y11)</span><br><span class="line">    area2 = (x22 - x21) * (y22 - y21)</span><br><span class="line">uni = area1 + area2 - inter</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = inter / uni</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>其实以上应该都只是基于常规检测框的IoU计算，如果是非矩形检测框或者分割任务中的IoU则要另当别论。</p><p>对于分割任务，定义mask1, mask2为两个相同大小的二维二值numpy数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SegIoU</span><span class="params">(mask1, mask2)</span>:</span></span><br><span class="line"><span class="keyword">return</span> (mask1 &amp; mask2).sum() / (mask1 | mask2).sum()</span><br></pre></td></tr></table></figure><p><strong>Q2：啥是mIoU？如何计算mIoU？</strong></p><p>mIoU即均交并比。对于每个类计算一遍IoU后取平均就行了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。&lt;/p&gt;
&lt;p&gt;虽然老夫从来没做过基于检测框的目标检测项目。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q1：啥是IoU？如何计算IoU？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;IoU就是交并比嘛，两个框相交的面积除以合并的面积。
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="IoU" scheme="http://a-kali.github.io/tags/IoU/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】SRN：使用语义推理网络进行场景文本识别</title>
    <link href="http://a-kali.github.io/2020/10/28/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91SRN%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AF%AD%E4%B9%89%E6%8E%A8%E7%90%86%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/10/28/【论文翻译】SRN：使用语义推理网络进行场景文本识别/</id>
    <published>2020-10-28T13:27:04.000Z</published>
    <updated>2020-10-28T13:28:07.565Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2003.12294" target="_blank" rel="noopener">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></p><p>Github：<a href="https://github.com/chenjun2hao/SRN.pytorch" target="_blank" rel="noopener">https://github.com/chenjun2hao/SRN.pytorch</a> （非官方）</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>场景文本图像包含两个层次的内容：视觉纹理和语义信息</strong>。近年来，虽然已有的场景文本识别方法取得了很大的进展，但<strong>挖掘语义信息来辅助文本识别</strong>的研究却很少，只有类似RNN的结构被用来对语义信息进行隐式建模。然而，基于RNN的译码方法存在着时效性强、语义上下文单向串行传输等缺点，极大地限制了语义信息的利用和计算效率。为了缓解这些限制，我们提出了一种全新的端到端场景文本识别框架——语义推理网络(semantic reasoning network, SRN)，其中引入了一个全局语义推理模块(GSRM)，通过多路并行传输捕获全局语义。我们在常规文本、不规则文本和非拉丁文本等7个公共基准上都验证了该方法的有效性和鲁棒性。此外，相对于基于RNN的方法，SRN的速度有明显的优势，在实际应用中具有一定的价值。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>文本数据通常具有十分丰富的语义信息，这些信息已经被应用在许多计算机视觉的应用中，如自动驾驶、旅游翻译、产品检索等。场景文本识别是场景文本阅读系统的关键步骤。虽然Seq2Seq识别在过去的几十年里取得了一些显著的突破，但在现实场景中进行文本识别仍然是一个巨大的挑战，这归咎于场景文本在颜色、字体、空间布局甚至不可控的背景上都有很大的变化。</p><p>最近的大部分研究都试图从提取视觉特征的角度来提高场景文本识别的性能，如升级backbone，增加校正模块，改进注意机制等。然而对于人来说，场景文本的识别不仅依赖于视觉感知信息，还受到高层次文本语义语境理解的影响。如下图所示的一些例子，在只考虑视觉特征的情况下，很难区分这些图像中的每个字符，尤其是用红色虚线框突出的字符。相反，如果考虑到语义信息，人类很可能会根据单词的全部内容推断出正确的结果。</p><p><img src="https://i.loli.net/2020/10/26/rkAqgxsctiaTP92.png" alt="image.png"></p><p>但主流的文本识别方法对于语义信息通常采用单向串行传输的方式(RNN)，如下图(a)。这种方式有几个明显的缺点：第一，它的每个time step只能感知非常有限的语义语境；其次，当一个time step出现错误解码时，会对后面的time step产生错误累积；同时，序列模型难以并行计算，耗时且低效。</p><p><img src="https://i.loli.net/2020/10/26/RAJg5ZyfB3QKGbz.png" alt="image.png"></p><p>在本文中，我们引入了一种名为全局语义推理模块(GSRM)的子网络结构来解决这些问题。GSRM使用一种全新的多路并行传输方式将全局语义内容联系在一起。如图(b)所示，多路并行传输可以同时感知一个单词或文本行中所有字符的语义信息。单个字符的语义内容错误，对其他步骤的负面影响十分有限。</p><p>在此基础上，我们提出了一种基于语义推理网络的场景文本识别框架，该框架不仅集成了全局语义推理模块(GSRM)，还集成了并行视觉注意模块(PVAM)和视觉语义融合解码器(VSFD)。PVAM的目的是在并行注意机制中提取每个time step的视觉特征，VSFD则用于融合视觉信息和语义信息。</p><p>这篇论文的贡献主要包含三部分：首先，我们提出了一个<strong>全局语义推理模块(GSRM)</strong>来处理全局语义信息。该方法比单向串行语义传输方法具有更好的鲁棒性和有效性。其次，提出了一种新的场景文本识别框架——<strong>语义推理网络(SRN)</strong>，该框架有效地结合了视觉信息和语义信息。第三，SRN是可以端到端训练的，并在<strong>一些baseline中表现SOTA，其中包括常规文本、不规则文本和非拉丁长文本</strong>。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>不会吧不会吧，不会真有人看Related Work吧</p><h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3    Approach"></a>3    Approach</h1><p>SRN是一个端到端的可训练框架，它由四部分组成：backbone、并行视觉注意模块(PVAM)、全局语义推理模块(GSRM)、以及视觉语义融合解码器(VSFD)。对于一个给定的图像输入，首先使用backbone提取二维特征，然后使用PVAM生成$N$个对齐的一维特征$G$，其中每个特征对应于文本中的一个字符，并包含相对应的视觉信息。然后将这$N$个一维特征$G$输入到GSRM中以获取语义信息$S$。将对齐后的视觉特征$G$和语义信息$S$融合在一起，对$N$个字符进行预测。对于小于$N$的文本字符串使用’EOS’填充。SRN的详细结构如下图。</p><p><img src="https://i.loli.net/2020/10/26/73LvbolV6IqyQDH.png" alt="image.png"></p><h2 id="3-1-Backbone-Network"><a href="#3-1-Backbone-Network" class="headerlink" title="3.1    Backbone Network"></a>3.1    Backbone Network</h2><p>在backbone中，我们使用FPN来融合ResNet50的stage-3, stage-4和stage-5层的特征。因此ResNet50+FPN最终输出特征图尺寸为输入图像的$1/8$，通道数为$512$。其灵感来源于non-local mechanisms，我们还采用transformer多头注意网络和一个前馈模块捕获全局空间依赖性。将二维特征图输入到两个堆叠transformer单元中，其中多头注意力的头数为$8$，前馈输出维度为$512$。至此，我们提取出了一个（大概是）$H×W×512$的特征。</p><p><strong>总结：ResNet50 + FPN + 8 head transformer</strong></p><h2 id="3-2-Parallel-Visual-Attention-Module"><a href="#3-2-Parallel-Visual-Attention-Module" class="headerlink" title="3.2    Parallel Visual Attention Module"></a>3.2    Parallel Visual Attention Module</h2><p>注意机制在序列识别中得到了广泛的应用。它可以看作是一种特征对齐的形式，将输入中的相关信息与相应的输出进行对齐。我们使用注意机制生成$N$个特征，每个特征对应文本中的一个字符。现有的注意力方法由于存在一些时间依赖项而导致效率低下。本文提出了一种新的注意方法——<strong>平行视觉注意(PVA)</strong>，通过突破这些障碍来提高效率。</p><p>一般来说，注意机制可以描述如下：给定一个key-value集合和一个query, 计算query与所有keys的相似性。然后使values根据相似性来进行融合。在我们的研究中，key-value集合是输入的二维特征，现有的方法使用隐藏层$H_{t-1}$作为query生成第$t$个特征。为了使计算并行，我们使用读取序号作为query，而不是依赖于时间的$H_{t-1}$。文本中的第一个字符的读取序号为0，第二个字符的读取序号顺序为1，以此类推。我们的PVA可以总结为：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             e_{t,ij}=W^T_e tanh(W_of_o(O_t)+W_vv_{ij})\<br>             \alpha_{t,ij}=\frac{exp(e_{t,ij})}{\sum_{∀i,j}exp(e_{t,ij})}\<br>             \end{array}<br>\right.<br>$$<br>其中$W$均为可训练的权值。$O_t$是每个字符的读取顺序，$f_o$是embedding函数。</p><p>基于PVA的想法，我们设计了并行视觉注意模块(PVAM)用于对齐每个视觉特征和time step。对齐第$t$个time step和视觉特征的过程描述如下：<br>$$<br>g_t=\sum_{∀i,j}\alpha_{t,ij}v_{ij}<br>$$<br>由于<strong>这个计算方法具有时间无关性，PVAM可以在所有time step上并行执行对齐操作</strong>。</p><p>如图所示，所得到的注意图能够正确地注意对应字符的视觉区域，验证了PVAM的有效性。</p><p><img src="https://i.loli.net/2020/10/26/6gR1KYUsrDlXI92.png" alt="image.png"></p><h2 id="3-3-Global-Semantic-Reasoning-Module"><a href="#3-3-Global-Semantic-Reasoning-Module" class="headerlink" title="3.3    Global Semantic Reasoning Module"></a>3.3    Global Semantic Reasoning Module</h2><p>在本节中，我们提出了遵循多路并行传输思想的全局语义推理模块(GSRM)，以克服单向语义上下文传递的缺点。首先我们回顾一下典型的类RNN结构的Bahdanau注意机制中需要最大化的概率公式。可以表示为：<br>$$<br>p(y_1y_2…y_N)=\prod_{t=1}^Np(y_t|e_{t-1},H_{t-1},g_t)<br>$$<br>其中$e_t$为第$t$个label $y_t$的词嵌入。在每个time step，类RNN的方法会参考先前的labels或者预测结果。由于$e_{t-1}$、$H_{t-1}$等信息只能在time step中获取，使得这类方法的只能以序列的方式进行，限制了语义推理的能力，导致推理效率较低。</p><p>为了克服上述问题，我们使用一个时间无关的近似嵌入$e’$来代替真正的嵌入$e$。这种改进可以带来几个好处。1)首先，可以将上式中最后一步的$H_{t-1}$隐藏状态值去除，从而将串行推理过程升级为高效并行推理过程，因为所有的时间依赖项都已经被消除。2)第二，包括前后所有字符在内的全局语义信息都能用来推导当前时刻的语义状态。因此，我们将概率表达式改进如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|f_r(e_1…e_{t-1}e_{t+1}…e_N),g_t)\<br>\approx \prod^N_{t=1}p(y_t|f_r(e’<em>1…e’</em>{t-1}e’<em>{t+1}…e’_N),g_t)<br>$$<br>其中$f_r$表示用于建立全局语义和当前语义信息的函数。如果我们使$s_t=f_r(e_1…e</em>{t-1}e_{t+1}…e_N)$，$s_t$表示第$t$个语义信息的特征，上式可以简化如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|s_t, g_t)<br>$$<br>于此我们提出了GSRM。该结构分为两个关键部分：视觉语义嵌入模块(Visual-to-semantic embedding block)和语义推理模块(semantic reasoning block)。</p><p><img src="https://i.loli.net/2020/10/27/SyL1OrqVf4NGWbh.png" alt="image.png"></p><p><strong>视觉语义嵌入模块</strong>用于生成$e’$，其输入特征已经经过PVAM对每个字符进行对齐。该视觉特征首先输入到一个全连接层和softmax层，并受到交叉熵损失监督。然后使用argmax选出可能性最大的字符进行embedding，得到$e’_t$。</p><p><strong>语义推理模块</strong>用于全局语义推理，相当于上上条公式里的$f_r$。多个transformer单元使模型能够高效地感知全局上下文信息，词语的语义可以通过多个transformer单元隐式建模。最后通过该模块输出每一步的语义特征，受交叉熵损失监督。</p><p>通过交叉熵损失，从语义信息的角度对客观概率进行优化，也有助于减少收敛时间。值得注意的是，在GSRM中，全局语义是并行推理的，这使得SRN比传统的基于注意力的方法运行得更快，特别是在长文本的情况下。</p><h2 id="3-4-Visual-Semantic-Fusion-Decoder"><a href="#3-4-Visual-Semantic-Fusion-Decoder" class="headerlink" title="3.4. Visual-Semantic Fusion Decoder"></a>3.4. Visual-Semantic Fusion Decoder</h2><p>在场景文本识别的同时考虑视觉对齐特征和语义信息是非常重要的。然而视觉和语义属于不同的领域，在不同的情况下，它们在最终序列识别中的权重应该是不同的。受门控单元的启发，我们引入了一些可训练的权重来平衡VSFD中不同领域的特征贡献。其操作方式如下：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             z_t=\sigma (W_z\cdot[g_t,s_t])\<br>            f_t=z_t<em>g_t+(1-z_t)</em>s_t\<br>             \end{array}<br>\right.<br>$$<br>其中$W_z$为可训练的权值，$f_t$为第$t$次融合特征向量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2003.12294&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Towards Accurate Scene Text Recognition with Semantic Reas
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>AdaIN：基于适应性实例归一化的实时任意风格迁移</title>
    <link href="http://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    <id>http://a-kali.github.io/2020/10/12/AdaIN：基于适应性实例归一化的实时任意风格迁移/</id>
    <published>2020-10-12T15:20:11.000Z</published>
    <updated>2020-10-12T15:21:14.171Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1703.06868.pdf" target="_blank" rel="noopener">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></p><p>Github：<a href="https://github.com/xunhuang1995/AdaIN-style" target="_blank" rel="noopener">https://github.com/xunhuang1995/AdaIN-style</a></p><p>PyTorch版代码：<a href="https://github.com/naoto0804/pytorch-AdaIN" target="_blank" rel="noopener">https://github.com/naoto0804/pytorch-AdaIN</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Gatys等人最近引入了一种神经算法，可以将一幅内容(Content)图像以另一幅图像的风格(Style)呈现，实现所谓的<strong>风格迁移(Style Transfer)</strong>。然而，他们的框架需要经历一个缓慢的迭代优化过程，这限制了其实际应用。有人提出用前向神经网络进行快速逼近，以加快神经风格迁移。不幸的是，速度的提高是有代价的：网络通常与一组固定的风格绑定在一起，不能适应任意的新的风格。在本文中，我们提出了一个简单而有效的方法，首次实现了实时任意风格的传输。该方法的核心是一种新的<strong>自适应实例归一化(adaptive instance normalization, AdalN)</strong>层，它将内容特征的均值和方差与风格特征的均值和方差对齐。我们的方法达到了与现有方法相比最快的速度，并且不受预定义样式集的限制。此外，用户可以灵活调控通过这个方法训练出来的模型，如内容样式权衡、样式插值、颜色和空间控制，所有这些都仅使用一个前向神经网络。</p><p><img src="https://i.loli.net/2020/10/09/aEOytThG9LQ5q1b.png" alt="image.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p><a href="http://pdfs.semanticscholar.org/3eeb/f249182614838294f7658da7e8d20c0a1917.pdf" target="_blank" rel="noopener">Gatys等人的开创性工作</a>表明，深度神经网络(DNNs)不仅编码图像的内容，而且编码图像的风格信息。此外，图像风格和内容在某种程度上是可分离的：可以在保留内容的同时改变图像的风格。他们的风格迁移方法足够灵活，可以组合任意图像的内容和样式。但是该方法的优化过程十分缓慢。</p><p>在加速神经迁移方向已经有了许多的研究。部分研究试图训练前向神经网络，通过单次前向传递来执行风格化操作。但这些方法限制了每个网络只能训练单一的风格。最近有一些研究解决了这个问题，但它们要么仍然局限于有限的风格集合，要么比单一风格的传输方法慢得多。</p><p>在这篇论文中中，我们首次提出了能够解决速度与灵活性矛盾的风格迁移算法。<strong>我们的方法可以实时对任意新的风格进行迁移，将基于优化框架的灵活性和前向方法的速度结合在一起</strong>。我们的方法灵感来源于<strong><a href="https://arxiv.org/abs/1701.02096v1" target="_blank" rel="noopener">实例归一化(instance normalization, IN)</a></strong>层，其在前向风格迁移中非常有效。IN通过对特征统计(feature statistics)进行归一化来进行风格归一化，有些研究表明特征统计携带了图像的风格信息。根据这个解释，我们引入了一个简单的IN扩展，即自适应实例归一化(AdaIN)。对于一组内容和风格，AdaIN只需调整内容输入的平均值和方差，就能匹配风格输入的平均值和方差。通过实验，我们发现AdaIN通过传递特征统计量，有效地将前者的内容与后者的风格结合起来。然后，通过将AdaIN输出反向返回到图像空间，解码网络学会生成最终的风格化图像。在实现了高速风格迁移的同时，我们的方法提供了大量的用户控件，不需要对训练过程进行任何修改。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p><strong>Style transfer.</strong> 风格迁移问题起源于非真实感渲染，与纹理合成和转换密切相关。早期的一些方法包括线性滤波器响应的直方图匹配和非参数采样。这些方法通常依赖于低级的统计信息，并且常常无法捕获语义结构。Gatys等通过匹配DNN卷积层的特征统计量，首次展示了令人印象深刻的风格迁移结果。最近，许多研究对风格迁移的算法进行了一些改进。Li和Wand在深度特征空间中引入了一种基于马尔可夫随机场的框架来加强局部模式。Gatys等人提出了控制色彩保存、空间位置的方法。以及风格迁移的规模。Ruder等人通过添加时序约束提高了视频风格迁移的质量。</p><p>Gatys等人的框架基于一个缓慢的优化过程，迭代更新图像，以最小化由内容损失和样式损失。即使是现代GPU也需要几分钟的时间，而移动应用程序中的设备处理速度更慢，难以实现。一个常见的解决方法是用训练最小化相同目标的前馈神经网络来代替优化过程。这些前馈风格的传输方法比基于优化的替代方法大约快三个数量级，为实时应用打开了大门。Wang等人的使用多分辨率架构增强了前馈式传输的粒度。Ulyanov等人提出了提高生成样本质量和多样性的方法。然而，上述前馈方法的局限性在于，每个网络都被绑在一个固定的样式上。为了解决这个问题，Dumoulin等人引入了一个能够编码32种样式及其插值的单一网络。与我们的工作同时，Li等人提出了一种前馈架构，可以合成多达300种纹理和转移16种风格。但是，上述两种方法不能适应训练中没有观察到的任意风格。</p><p>最近，Chen和Schmidt引入了一种前馈方法，借助风格交换层可以传输任意的风格。对于给定内容和风格图像的特征激活，风格交换层将以patch-by-patch的方式将内容特征替换为最匹配的风格特征。然而，他们的风格交换层创造了一个新的计算瓶颈：超过95%的计算花费在512 x 512输入图像的样式交换上。我们的方法允许任意的风格的同时，比他们的方法快1-2个数量级。</p><p>风格迁移的另一个核心问题是使用哪种风格损失函数。Gatys等人的原始框架通过匹配Gram矩阵捕获的特征激活之间的二阶统计量来匹配风格。后来也有研究提出了其它的损失函数，如MRF损失，adversarial 损失，直方图损失，CORAL损失，MMD损失，以及信道平均和方差之间的距离。注意，以上所有的损失函数都是为了匹配风格图像和合成图像之间的一些特征统计。</p><p><strong>Deep generative image modeling.</strong> 有几个图像生成框架可供选择，包括变分自动编码器、自回归模型和生成对抗网络(GANs)。值得注意的是，GANs已经取得了最令人印象深刻的视觉质量。GAN框架的各种改进已经被提出，比如条件生成、多级处理以及更好的训练目标。GANs也被应用于风格迁移和跨域图像生成。</p><h1 id="3-Background"><a href="#3-Background" class="headerlink" title="3    Background"></a>3    Background</h1><h2 id="3-1-Batch-Normalization"><a href="#3-1-Batch-Normalization" class="headerlink" title="3.1    Batch Normalization"></a>3.1    Batch Normalization</h2><p>loffe和Szegedy的开创性地引入了批归一化(BN)层，通过对特征统计进行归一化，显著地简化了前馈网络的训练。BN层的设计初衷是为了加速识别网络的训练，但后来被发现在生成图像模型中也是有效的。对于给定批处理输入x，BN对每个特征通道的均值和标准差进行归一化：<br>$$<br>BN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>其中$\gamma$和$\beta$从数据中习得，$\mu(x)$和$\sigma$是batch的均值和标准差，由每个特征通道的batch size和空间维度独立计算而得：<br>$$<br>\mu_c(x)=\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_c(x))^2+\epsilon}<br>$$<br>BN在训练时使用mini-batch统计，在推理时使用常规的统计代替，造成了训练和推理的差异。为了解决这个问题，最近提出了批重正化，在训练期间逐步使用常规的统计数据。Li等人发现BN的另一个有趣应用：BN可以通过重新计算目标域的常规统计数据来减轻域偏移。</p><h2 id="3-2-Instance-Normalization"><a href="#3-2-Instance-Normalization" class="headerlink" title="3.2    Instance Normalization"></a>3.2    Instance Normalization</h2><p>在原始的前馈风格化方法中，风格迁移网络在每个卷积层之后包含一个BN层。令人惊讶的是，Ulyanov等发现，只需将BN层替换为IN层，就可以得到显著的改善：<br>$$<br>IN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>和BN不同的是，IN的的$\mu(x)$和$\sigma(x)$是对每个通道和每个样本独立计算的：<br>$$<br>\mu_{nc}(x)=\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_{nc}(x))^2+\epsilon}<br>$$<br>另一个区别是，在测试时应用的层不变，而BN层通常使用常规统计代替mini-batch统计。</p><h2 id="3-3-Conditional-Instance-Normalization"><a href="#3-3-Conditional-Instance-Normalization" class="headerlink" title="3.3    Conditional Instance Normalization"></a>3.3    Conditional Instance Normalization</h2><p>Dumoulin等人没有学习单一的仿射参数集$\gamma$和$\beta$，而是提出了条件实例归一化(CIN)层，该层对每种不同的风格$s$学习不同的参数集$\gamma^s$和$\beta^s$：<br>$$<br>CIN(x;s)=\gamma^s \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta^s<br>$$<br>在训练过程中，从一组固定的风格集合$s\in  {1,2,…,S}$(实验中$S = 32$)中随机选取一幅风格图像及其索引$s$。然后将内容图像输入到一个网络中，在CIN层使用对应的$\gamma^s$和$\beta^s$。令人惊讶的是，<strong>有着相同卷积参数、不同仿射参数的多个网络，可以生成完全不同风格的图像。</strong></p><p>与没有归一化层的网络相比，有CIN层的网络需要增加$2FS$的附加参数，其中$F$为网络中feature map的数量。由于附加参数的数量与样式的数量成线性关系，因此很难用这个方法生成非常多的风格。而且，每添加一种新的风格，都需要重新训练一次网络。</p><h1 id="4-Interpreting-Instance-Normalization"><a href="#4-Interpreting-Instance-Normalization" class="headerlink" title="4     Interpreting Instance Normalization"></a>4     Interpreting Instance Normalization</h1><p>尽管IN取得了巨大的成功，但它们对样式转换起作用的原因仍然是难以捉摸的。Ulyanov等人把IN的成功归于其内容图像的不变性。然而，IN发生在特征空间中，因此它应该比在像素空间中进行简单的对比归一化具有更深远的影响。也许更令人惊讶的是IN中的仿射参数可以完全改变输出图像的风格。</p><p>众所周知，DNN的卷积特征统计可以捕捉到图像的风格。Gatys等人使用二阶统计量作为其优化目标，而Li等人最近表明，包括channel-wise的均值和方差在内的其它统计量对风格迁移也是有效的。因此<strong>我们认为IN通过归一化特征统计（即均值和方差），在某种程度上执行了“风格归一化(style normalization)”。于是我们认为网络的特征统计也可以控制生成图像的风格。</strong></p><p>我们分别运行带有IN和BN层的网络来执行单一风格的转换。正如预期的那样，使用IN的模型比BN模型收敛得更快（如下图）。为了检验Ulyanov的解释，我们通过对亮度通道进行直方图均衡化，将所有训练图像归一化到相同对比度。如图(b)所示，IN仍然有效，说明Ulyanov的解释不完全。为了验证我们的假设，我们使用预训练的风格转移网络将所有的训练图像归一化为相同的风格(不同于目标风格)。从图(c)可以看出，在对图像进行了风格归一化后，IN带来的改进就小得多了。另外，使用风格归一化图像训练BN的模型和使用原始图像训练IN的模型收敛速度一样快，表明IN确实执行了一种风格归一化。</p><p><img src="https://i.loli.net/2020/09/25/BJ5SiRZen3bx2Uh.png" alt="image.png"></p><p>由于BN是在一个batch的样本上进行特征统计，可以直观地理解为将一个batch的样本围绕着单一风格进行归一化。然而每一个的样本都有不同的风格，很难将一个batch中所有样本转化成同一个风格。虽然卷积层可能会学会弥补样本之间风格的差异，但也为训练增加了难度。另一方面，IN可以将每个样本的风格归一化为目标风格，网络的其他部分可以在舍弃原有信息风格的同时专注于内容处理，提高了训练速度。CIN成功的原因也很明确：不同的仿射参数可以将特征统计值归一化到不同的值，从而将输出的图像归一化到不同的风格。</p><h1 id="5-Adaptive-Instance-Normalization"><a href="#5-Adaptive-Instance-Normalization" class="headerlink" title="5    Adaptive Instance Normalization"></a>5    Adaptive Instance Normalization</h1><p>如果将输入归一化为由仿射参数指定的单一风格，是否有可能通过自适应仿射变换使其适应任意给定的风格？我们对IN进行了一个简单的扩展。我们称之为自适应实例归一化(AdaIN)。AdaIN接收一个内容输入x和一个样式输入y，并简单地将x的通道平均值和方差与y的平均值和方差匹配。<strong>与BN、IN和CIN不同，AdaIN没有可以学习的仿射参数。相反，它自适应地从风格输入中计算仿射参数</strong>：<br>$$<br>AdaIN(x,y)=\sigma (y)\begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\mu (y)<br>$$<br>相比于IN，我们仅仅是将两个仿射参数替换成了$\sigma (y)$和$\mu (y)$，这两个统计值的依然是在空间位置上进行计算。</p><p>假设存在一个检测特定风格纹路的特征通道。具有这种纹路的风格图像将在该层产生较高的平均激活值。AdaIN产生的输出在保持内容图像的空间结构的同时，对该特征具有同样高的平均激活度。纹路特征可以通过前馈解码器转换到到图像空间。该特征通道的方差可以将更细微的风格信息传递到AdaIN输出和最终输出的图像中。</p><p>简而言之，AdaIN通过迁移特征统计量，即通道方向上的均值和方差，在特征空间中进行风格迁移。</p><h1 id="6-Experimental-Setup"><a href="#6-Experimental-Setup" class="headerlink" title="6    Experimental Setup"></a>6    Experimental Setup</h1><p>如下是我们基于AdaIN的风格迁移网络的概览图：</p><p><img src="https://i.loli.net/2020/09/29/j2ZQMDuv8HtWqhk.png" alt="image.png"></p><h2 id="6-1-Architecture"><a href="#6-1-Architecture" class="headerlink" title="6.1    Architecture"></a>6.1    Architecture</h2><p>我们的风格迁移网络$T$以一个内容图像$c$和一个任意风格的图像$s$作为输入，并合成一个输出图像，该图像重新组合前者的内容和后者的样式。我们采用一种简单的encoder-decoder架构，其中encoder $f$ 固定在预训练VGG-19的前几层（直到relu4_1）。在特征空间中对内容和风格图像进行编码后，我们将这两种特征图输入AdalN层，使内容特征图的均值和方差与风格特征图的均值和方差对齐，生成目标特征图$t$：<br>$$<br>t=AdaIN(f(c),f(s))<br>$$<br>训练一个随机初始化的decoder $g$将$t$映射回图像空间，生成风格化图像$T (c, s)$：<br>$$<br>T(c,s)=g(t)<br>$$<br>decoder大部分是encoder的镜像，所有池化层替换为最近的上采样，以减少棋盘效应。我们在$f$和$g$中使用反射填充（(reflflection padding)来避免边界失真。另一个问题是decoder应该使用IN、BN还是不使用标准化层。如第4节所述，IN将每个样本归为单个样式，而BN将一批样本归一化，以单个样式为中心。当我们希望decoder生成风格迥异的图像时，两者都是不可取的。因此，我们在decoder中不使用归一化层。</p><h2 id="6-2-Training"><a href="#6-2-Training" class="headerlink" title="6.2    Training"></a>6.2    Training</h2><ul><li>Dataset<ul><li>Content: MS-COCO</li><li>Style: WikiArt</li></ul></li><li>Sample size: 80000</li><li>Optimizer: adam</li><li>Batch size: 8 content-style image pairs</li><li>Resize: 512, RandomCrop: 256×256</li><li>Model: VGG-19</li><li>Loss: $L=L_c+\lambda L_s$</li></ul><p>损失函数为内容损失和风格损失的加权和。内容损失是目标特征与输出图像特征之间的欧氏距离。我们使用AdaIN输出$t$作为内容目标，而不是内容图像：<br>$$<br>L_c=|f(g(t))-t|<em>2<br>$$<br>由于AdaIN层只迁移了风格特征的平均值和标准差，所以我们的风格损失只与这些统计数据匹配。虽然我们发现常用的Gram矩阵损失可以产生类似的结果，但我们还是使用IN统计，因为它在概念上更清晰。<br>$$<br>L_s=\sum^L</em>{i=1}|\mu (\phi_i(g(t)))-\mu(\phi_i(s))|<em>2+\sum^L</em>{i=1}|\sigma (\phi_i(g(t)))-\sigma(\phi_i(s))|_2<br>$$<br>其中$\phi$表示VGG-19中用于计算风格损失的层。在我们的实验中，我们在relu1_1, relu2_1, relu3_1,  relu4_1中使用了相等的权重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1703.06868.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Arbitrary Style Transfer in Real-time with Adaptive In
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="风格迁移" scheme="http://a-kali.github.io/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>SynthText：用于文本定位的自然场景文本合成</title>
    <link href="http://a-kali.github.io/2020/09/28/SynthText%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D%E7%9A%84%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E5%90%88%E6%88%90/"/>
    <id>http://a-kali.github.io/2020/09/28/SynthText：用于文本定位的自然场景文本合成/</id>
    <published>2020-09-27T16:09:56.000Z</published>
    <updated>2020-09-27T16:12:36.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SynthText"><a href="#SynthText" class="headerlink" title="SynthText"></a><a href="https://github.com/ankush-me/SynthText" target="_blank" rel="noopener">SynthText</a></h1><p>论文地址：<a href="https://arxiv.org/pdf/1604.06646.pdf" target="_blank" rel="noopener">Synthetic Data for Text Localisation in Natural Images</a></p><p><img src="https://i.loli.net/2020/09/16/UVoC3yxPH8T5n4w.png" alt="image.png"></p><p>本文介绍了一种新的自然图像文本检测方法。该方法主要包括两个方面：首先，一个用于生成文本合成图片(synthetic images of text)的引擎。<strong>该引擎结合局部的三维场景几何形状，将合成文本以自然的方式叠加到现有的背景图像上。</strong>然后利用图像图像训练一个全卷积回归网络(FCRN)，在图像的任意位置多尺度地执行文本检测和边框回归。</p><p>在这里我们仅关注其生成合成图像的部分，过程如下：</p><ol><li>选择合适的文本和图像样本，根据图像局部的颜色和纹理将图像分割成连续的区域，并使用CNN进行像素级的映射；</li><li>对于每一个连续的区域，建立一个<strong>表面法线(surface normal)</strong>；</li><li>根据区域的颜色来选择文本及其轮廓的颜色；</li><li>使用随机字体渲染文本样本，并根据局部表层方向进行转换；使用泊松图像编辑(Poisson image editing)将文本混合到场景中。</li></ol><p>该生成一个场景文本图像大约需要半秒钟。项目作者建立了一个80w张生成图像的数据库：<a href="http://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/data/scenetext/</a></p><p><img src="https://i.loli.net/2020/09/14/qkvKnxTB3D9YCUt.png" alt="image.png"></p><h2 id="1-Text-and-Image-Sources"><a href="#1-Text-and-Image-Sources" class="headerlink" title="1    Text and Image Sources"></a>1    Text and Image Sources</h2><p>文本数据来源于Newsgroup20数据集，使用了三种提取方式：单词、句子(最多3行)和段落(最多7行)。该数据集中包含了丰富的英文语料。</p><p>为了增加多样性，作者从谷歌图像搜索中提取了8000幅背景图像。通过查询不同的物体/场景、室内/室外和自然/人造场所，这些图片自身不能包含文本。因此，搜索的时候会尽量避免携带大量文本的关键词，比如“路牌”、“菜单”等。包含文本的图像会在人工检查后丢弃。</p><h2 id="2-Segmentation-and-Geometry-Estimation"><a href="#2-Segmentation-and-Geometry-Estimation" class="headerlink" title="2    Segmentation and Geometry Estimation"></a>2    Segmentation and Geometry Estimation</h2><p>在真实场景中，文本往往包含在明确定义的区域中（比如一个指示牌）。本文提出的方法将文本约束在统一颜色和纹理的区域，可以防止文本跨越强图像不连续点。将gPb-UCM轮廓分层的阈值设定在0.11，通过图切割(graph-cut)获得区域。下图显示了对图像颜色和纹理敏感（左图）和直接将文本置于图像（右图）的区别。</p><p><img src="https://i.loli.net/2020/09/16/y4GfIBwsHAn9ZhC.png" alt="image.png"></p><p>在自然图像中，文本往往在物体表层的顶部(例如一个路牌或一个杯子）。为了使合成数据中也有类似的效果，作者根据局部表面法线对文本进行了详细的变换：</p><ol><li>首先通过<a href="https://arxiv.org/abs/1411.6387v1" target="_blank" rel="noopener">特定的CNN</a>对上面分割的区域预测一个深度图，然后使用<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=44efa35fada9e7afa5fa46da356fabbc&site=xueshu_se" target="_blank" rel="noopener">RANSAC</a>拟合一个平面来自动估算出一条法线；</li><li>利用估算出的平面法线将图像区域轮廓弯曲成平行面视图，将矩形拟合到额平行(fronto-parallel)区域</li><li>文本与矩形的宽对齐。当在同一区域放置多个文本实例时，检查文本mask是否相互冲突，避免叠加。</li></ol><p>并不是所有的分割区域都适合放置文本，比如区域太小、极端高宽比、或表面法向正交于视角方向的区域，这些区域都在这个阶段被过滤。此外，纹理过多的区域也被过滤，其中纹理的复杂度是由RGB图像的三阶导数的强度来衡量的。</p><h2 id="3-Text-Rendering-and-Image-Composition"><a href="#3-Text-Rendering-and-Image-Composition" class="headerlink" title="3    Text Rendering and Image Composition"></a>3    Text Rendering and Image Composition</h2><p>确定了文本的位置和方向之后，下一步是给文本上色。文本的调色板是从 IIIT5K单词数据集裁剪的单词图像中学习的。使用K-means将裁剪后的词图像的像素分割成两个集合，分别为前景（文本）和背景。在渲染新文本时，背景颜色选择与目标图像区域最匹配的颜色对(在Lab颜色空间中使用L2-norm)，并使用相应的前景色渲染文本。</p><p>随机选择大约20%的文本实例加上边框，边框颜色与前景颜色接近，或者被设为前景和背景颜色的平均值。</p><p>为了保持合成文本图像中的光照梯度(illumination gradient)，使用Poisson图像编辑将文本混合到基础图像上。</p><p>顺带再提两个比较新的文本合项目：UnrealText和SynthText3D。</p><h1 id="SynthText3D"><a href="#SynthText3D" class="headerlink" title="SynthText3D"></a><a href="https://github.com/MhLiao/SynthText3D" target="_blank" rel="noopener">SynthText3D</a></h1><p>论文地址：<a href="https://arxiv.org/abs/1907.06007" target="_blank" rel="noopener">SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds</a></p><p><img src="https://i.loli.net/2020/09/16/cvp7d8fTrCAGtby.png" alt="image.png"></p><p>本文提出从三维虚拟世界中合成场景文本图像，该方法提供了精确的场景描述、可编辑的亮度/能见度和真实的物理现象。与之前的方法不同的是，该方法可以将三维虚拟场景和文本实例作为一个整体进行渲染。该方法合成场景文本图像中能够呈现真实世界的变化，包括复杂的透视变换、光照、遮挡。此外，通过对虚拟摄像机进行随机移动和旋转，可以对同一个文本生成不同视点的实例。</p><h1 id="UnrealText"><a href="#UnrealText" class="headerlink" title="UnrealText"></a><a href="https://github.com/Jyouhou/UnrealText" target="_blank" rel="noopener">UnrealText</a></h1><p>论文地址：<a href="https://arxiv.org/abs/1907.06007" target="_blank" rel="noopener">UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World</a></p><p><img src="https://i.loli.net/2020/09/17/4vNfpg7Fte38Rwo.png" alt="image.png"></p><p>UnrealText同样是通过三维图形引擎生成逼近真实的图像。</p><p><img src="https://i.loli.net/2020/09/17/kAzgoTn9KlfeB4m.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SynthText&quot;&gt;&lt;a href=&quot;#SynthText&quot; class=&quot;headerlink&quot; title=&quot;SynthText&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/ankush-me/SynthText&quot; target=&quot;_b
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="文本合成" scheme="http://a-kali.github.io/tags/%E6%96%87%E6%9C%AC%E5%90%88%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>Mask TextSpotter v3：基于分割候选框的场景文本识别</title>
    <link href="http://a-kali.github.io/2020/09/03/Mask-TextSpotter-v3%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%88%86%E5%89%B2%E5%80%99%E9%80%89%E6%A1%86%E7%9A%84%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/09/03/Mask-TextSpotter-v3：基于分割候选框的场景文本识别/</id>
    <published>2020-09-03T14:21:50.000Z</published>
    <updated>2020-10-28T13:32:16.858Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2007.09482" target="_blank" rel="noopener">Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</a></p><p>Github：<a href="https://github.com/MhLiao/MaskTextSpotterV3" target="_blank" rel="noopener">https://github.com/MhLiao/MaskTextSpotterV3</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>最近检测与识别相结合的场景文本识别端到端模型取得了很大进展。然而，目前的任意形状场景文本识别模型大多使用RPN来生成候选框，而RPN严重依赖于手工设计的轴对称矩形anchor。这使得处理高宽比或不规则形状的文本实例时存在困难，处理密集文本时单个候选框中容易包含多个相邻实例。为了解决这些问题，我们提出了Mask TextSpotter v3，一种端到端的场景文本识别器，其<strong>采用SPN(Segmentation Proposal Network)代替RPN。我们的SPN是anchor-free的，能够精确表示任意形状的候选区域</strong>。因此，我们的Mask TextSpotter v3可以处理极端宽高比或不规则形状的文本实例，而且它的识别精度不会受到附近文本或背景噪音的影响。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在现实中阅读文本是一项非常重要的技术，有着广泛的应用，包括照片OCR，菜单阅读，地理定位等。系统针对该任务的设计一般包括文本检测和识别两个模块，其中文本检测的目标是对文本实例及其边界框进行定位，而文本识别的目标是将检测到的文本区域转换为一系列标签进行字符识别。场景文本识别/端到端识别是一个结合了这两种模块的任务，既需要检测又需要识别。</p><p>场景文本阅读的挑战主要在于场景文本实例的不同方位、宽高比和形状。因此，旋转鲁棒性，宽高比鲁棒性和形状鲁棒性在场景文本检测任务中是非常重要的。因为文本通常不会沿着图像的轴线对其，所以<strong>旋转鲁棒性</strong>非常重要。<strong>高宽比鲁棒性</strong>对于非拉丁文本尤其重要，因为这些文本通常表现为很长的文本行，而不是一个个单词。<strong>形状鲁棒性</strong>对于处理不规则形状的文本是必要的，不规则形状经常出现在logo中。</p><p>RPN的局限性主要表现在两个方面：(1)手工预先设计的锚点是使用轴对称的矩形来定义的，不易匹配高长比极端的文本实例。(2)文本实例密集时，生成矩形候选框可能包含多个相邻文本实例。如图a所示，由Mask TextSpotter v2产生的候选框相互重叠，其RoI特征包含多个相邻文本实例，导致检测和识别错误。</p><p><img src="https://i.loli.net/2020/09/02/J93otX4vVBPydT7.png" alt="image.png"></p><p>在本文中，我们提出了SPN，旨在解决RPN-based方法的局限性。<strong>SPN是anchor-free的，并给出了精确的多边形候选框，不用预先设计anchor。同时我们提出hard RoI masking应用到RoI特征中，可以抑制邻近的文本实例或背景噪声，从而充分利用该方法的精确性。</strong>我们的实验表明，Mask TextSpotter v3显著提高了对旋转、宽高比和形状的鲁棒性。在旋转的ICDAR 2013数据集上，图像以不同角度旋转，我们的方法在端到端检测和识别方面都超过了目前最先进的21.9%。在端到端识别任务中，我们的方法超出最先进方法5.9%。我们的方法还在具有极端高长宽比的文本行标记的MSRA-TD500数据集，以及包含许多低分辨率小文本实例的ICDAR 2015数据集上达到了最高水准。我们的贡献主要包括三方面：</p><ol><li>我们提出的<strong>SPN能够精确表示任意形状的候选框</strong>。Anchor-free SPN克服了RPN在处理极端长宽比或不规则形状文本时的局限性，并提供了更精确的方案来提高识别的鲁棒性。据我们所知，它是首个用于端到端可训练文本定位的任意形状建议生成器。</li><li>我们提出了 <strong>hard RoI masking 将多边形候选框应用于RoI特征</strong>，有效地抑制背景噪声或相邻的文本实例。</li><li>我们提出的Mask TextSpotter v3显著提高了对旋转、宽高比和形状的鲁棒性，在几个具有挑战性的场景文本测试中取得了最先进的结果。</li></ol><h1 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2    Methodology"></a>2    Methodology</h1><p>Mask TextSpotter v3使用ResNet-50作为backbone，SPN生成候选框，一个Fast R-CNN模块微调候选框，一个文本实例分割模块用来精确检测，一个字符分割模块和一个空间注意力模块用来识别。Mask TextSpotter v3的pipeline如图所示。</p><p><img src="https://i.loli.net/2020/09/02/DJFe94Ckndgp6QN.png" alt="image.png"></p><h2 id="2-1-Segmentation-proposal-network"><a href="#2-1-Segmentation-proposal-network" class="headerlink" title="2.1     Segmentation proposal network"></a>2.1     Segmentation proposal network</h2><p>如上图所示，我们提出的<strong>SPN采用U-Net结构使其对尺度具有鲁棒性</strong>。和基于FPN多个阶段产生不同尺度的候选框的RPN不同，<strong>SPN从分割mask生成候选框，并且使用由多个感受野产生的特征图融合而成的特征图进行预测</strong>，该特征图长宽为输入图像的1/4。分割预测模块配置见文末补充。分割模型输出的掩码概率图尺寸和输入图像相同，通道数为1，取值范围为[0,1]。</p><p><strong>Segmentation label generation.</strong> 为了分离邻近的文本实例，基于分割的场景文本检测器常用的方法是缩小文本区域。受DB模型的启发，我们采用Vatti裁剪算法，通过裁剪$d$个像素来缩小文本区域。偏移像素d可以确定为$d= A(1-r^2)/L$，其中$A$和$L$分别为文本域多边形的面积和周长，$r$为收缩比，通常设为0.4。生成label的例子如下图所示：</p><p><img src="https://i.loli.net/2020/09/02/l9RejYTKfJu7SP6.png" alt="image.png"></p><p><strong>Proposal generation.</strong> 对于一个给定的值域在[0,1]之间的分割图，我们通过阈值对其进行二值化。对掩码$B$中连通的区域进行分组，这些连通区域可以被认为是缩小后的文本区域。因此，我们使用Vatti裁剪算法扩大它们$\hat d$个像素，其中$\hat d$计算为$\hat d=\hat A×\hat r / \hat L$。其中，$\hat A$和$\hat L$是缩小后文本区域的面积和周长。根据收缩比$r$的值，我们将$\hat r$设置为3.0。</p><p>如上所述，SPN产生的候选框可以精确地表示多边形文本区域。因此，SPN具有生成极端长宽比/密集/不规则形状的文本实例候选框的能力。</p><p><strong>（概括：分割→缩小文本域→二值化→将连通的像素划分为一个文本域→恢复文本域大小）</strong></p><h2 id="2-2-Hard-RoI-masking"><a href="#2-2-Hard-RoI-masking" class="headerlink" title="2.2    Hard RoI masking"></a>2.2    Hard RoI masking</h2><p>由于自定义RoI Align操作只支持轴对称的矩形bounding boxes，因此我们使用多边形候选框中的最小的、轴对称的矩形bounding boxes来生成RoI特征，以保持RoI Align操作。</p><p>Qin等人提出了RoI masking，将掩码概率图与RoI特征相乘，其中掩码概率图由Mask R-CNN检测模块生成。然而，掩码概率图可能不准确，因为其基于RPN的候选框进行预测，一个候选框中可能包含多个密集相邻的文本。而我们为候选框设计了精确的多边形表示。因此，我们提出了 hard RoI masking 直接将这些候选框应用到RoI特征上。</p><p><strong>Hard RoI masking将二值多边形掩码与RoI特征相乘，以抑制背景噪声或邻近文本实例</strong>，大大降低了检测和识别模块的执行难度和出错概率。</p><h2 id="2-3-Detection-and-recognition"><a href="#2-3-Detection-and-recognition" class="headerlink" title="2.3    Detection and recognition"></a>2.3    Detection and recognition</h2><p>我们遵循Mask TextSpotter v2的文本检测和识别模块的主要设计，原因如下:(1)Mask TextSpotter v2是目前最先进的、具有竞争力的检测和识别模块。(2)由于Mask TextSpotter v2是基于RPN的场景文本检测中一种具有代表性的方法，我们可以控制变量，来验证我们所提出的SPN的有效性和鲁棒性。</p><p>检测时，将Hard RoI masking生成的masked RoI特征输入到Fast R-CNN模块以进一步细化定位，并使用文本实例分割模块进行精确分割。采用字符分割模块和空间注意力模块进行识别。</p><h2 id="2-4-Optimization"><a href="#2-4-Optimization" class="headerlink" title="2.4    Optimization"></a>2.4    Optimization</h2><p>损失函数$L$定义如下：<br>$$<br>L=L_s+\alpha_1L_{rcnn}+\alpha_2L_{mask}.<br>$$<br>$L_{rcnn}$和$ L_{mask}$分别在Fast R-CNN和Mask TextSpotter v2中有过明确定义。$L_{mask}$由文本分割损失、字符分割损失、空间注意力解码器损失组成。$L_s$表示SPN的损失。根据Mask TextSpotter v2，我们将$\alpha_1$和$\alpha_2$设为1.0。</p><p>我们采用dice loss计算SPN的损失。设$S$和$G$分别为分割结果mask和目标mask，则$L_s$可由如下公式计算：<br>$$<br>L_s=1-\frac{2.0×\sum(S*G)}{\sum S + \sum G}<br>$$<br>（大概意思就是两个mask重合率越高，损失越小）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2007.09482&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mask TextSpotter v3: Segmentation Proposal Network for Rob
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本识别" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】MoCo：用于无监督视觉表示学习的动量对比</title>
    <link href="http://a-kali.github.io/2020/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91MoCo%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94/"/>
    <id>http://a-kali.github.io/2020/09/03/【论文翻译】MoCo：用于无监督视觉表示学习的动量对比/</id>
    <published>2020-09-03T14:04:54.000Z</published>
    <updated>2020-09-27T16:22:31.810Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1911.05722.pdf" target="_blank" rel="noopener">Momentum Contrast for Unsupervised Visual Representation Learning</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了动量对比(Momentum Contrast, MoCo)的无监督视觉表示学习。从基于字典查找的对比学习(contrastive learning)的角度出发，我们构建了一个带有队列和移动平均编码器的动态字典：这使我们能够实时构建一个大型的、一致的字典，从而促进非监督对比学习。MoCo在ImageNet分类任务中表现优异。更重要的是，MoCo学到的表示可以很好地应用到<strong>下游任务(downstream task)</strong>中。MoCo可以在PASCAL VOC、COCO等7个数据集的检测/分割任务中超过了其监督学习的预训练模型。这表明，在许多视觉任务中，无监督和监督学习之间的差距已经很大程度上缩小了。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>无监督表示学习在NLP中的应用非常成功，如：GPT、BERT。但在CV领域，有监督的预训练仍占主导地位，而无监督的方法则普遍落后。原因可能是由于它们所属的信号空间不同。语言任务具有离散的信号空间(单词、子单词单元等)用于构建标记化词典( tokenized dictionaries)，在此基础上进行无监督学习。相反，CV更加关注字典构建，因为原始信号处于连续的高维空间。</p><p>最近的一些研究提出了使用<strong>对比损失(contrastive loss)</strong>的无监督视觉表示学习，得出了令人激动的结果。这些方法可以看作是<strong>构建动态词典(dynamic dictionaries)</strong>。数据样本通过一个encoder提取特征后得到字典中的“密钥”(keys)。无监督学习训练encoder来执行字典查询过程：一个“查询”(query)通过encoder后，得到的输出应该与其匹配的key值相近，而与其他样本的key尽可能不同。学习目的为最小化contrastive loss。</p><p>从这个角度来看，我们认为应该在训练过程中逐步建立满足以下条件的词典：<strong>大型&amp;一致</strong>。直观地说，一个<strong>更大的字典</strong>可以更好地对底层连续的、高维的视觉空间进行采样；而字典中的<strong>key应该用相同或类似的encoder表示，以便它们与query的比较是一致</strong>的。然而，使用现有的contrastive loss方法可能局限于这两个方面中的一个(稍后在下文中讨论)。</p><p>我们将动量对比(MoCo)作为一种构建大型且一致的字典的方式，用于非监督学习。我们将字典当作数据样本的队列：每当一个新的batch完成编码后进入队列，将最老的编码移出队列。队列将字典大小与batch大小解耦，如此一来就能独立建立字典（而不是依赖于batch大小），允许字典的大小比batch大很多。此外，由于字典的key来自前几个batch，为了保持一致性，我们提出了一种基于动量的encoder。</p><p><img src="https://i.loli.net/2020/08/28/cuYKIpgwVQS38v6.png" alt="image.png"></p><p>MoCo是一种建立动态对比学习词典的机制，可以与各种各样的<strong><a href="https://blog.csdn.net/u013303408/article/details/103657043" target="_blank" rel="noopener">前置任务(pretext task)</a></strong>一起使用。在本文中，我们使用一个简单的识别任务进行讲解：如果它们是同一图像(例如同一张图片的不同裁剪区域)的编码，那么query将给它们匹配同一个key。利用这个前置任务，MoCo在ImageNet数据集中展现了有力的结果。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>无监督/自监督学习方法一般包括两个方面：前置任务和损失函数。“前置”意味着正在解决的任务并不是我们最终要解决的任务，而是为了更好地学习数据的表示。损失函数通常可以独立于前置任务进行研究。MoCo专注于损失函数方面。接下来我们就这两个方面的相关研究进行讨论。</p><p><strong>损失函数</strong>。损失函数是一种用于衡量模型的预测和固定目标值之间差距的方式。其中，对比损失(Contrastive losses)用于衡量两个样本在一个空间中的相似性。相比于将输入匹配到一个固定的目标，对比损失公式的目标可以在训练期间变化。对比学习是最近几篇关于无监督学习的研究的核心内容，我们将在后面对此进行阐述。</p><p><strong>前置任务</strong>。前置任务的种类多种多样。例如修复遭到损坏的输入，包括：去噪自动编码器( denoising auto-encoders)，上下文自动编码器(context autoencoders)，或是用于着色的跨通道自动编码器(cross-channel auto-encoders)；还有一些前置任务用于形成伪标签，例如，对图像进行转换（数据增强）、patch排序、视频目标的跟踪、分割、聚类。</p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3    Method"></a>3    Method</h1><h2 id="3-1-Contrastive-Learning-as-Dictionary-Look-up"><a href="#3-1-Contrastive-Learning-as-Dictionary-Look-up" class="headerlink" title="3.1 Contrastive Learning as Dictionary Look-up"></a>3.1 Contrastive Learning as Dictionary Look-up</h2><p>对比学习(Contrastive learning)可以被看作是训练一个encoder进行字典查找任务。</p><p>假设已有一个已编码的查询$q$，和一组已编码的样本（即字典中的keys）：${k_0, k_1,  k_2, …}$，$q$匹配到了字典中的一个key(表示为k)。$q$和其对 应的$k_+$越相似且与其它的keys相差越大，对比损失函数值越小。一种和点积度量相似，被称为<strong>InfoNCE</strong>的对比损失函数在本文中被使用：<br>$$<br>L_q=-log \frac{exp(q\cdot k_+/\tau)}{\sum^K_{i=0}exp(q\cdot k_i/\tau)}<br>$$<br>其中$\tau$是一个超参数，sum运算的对象是1个正样本和K个负样本。直观地说，这种损失是基于K+1类softmax分类器的对数损失，用于将$q$分类为$k_+$类中的其中一类。对比损失函数也可以基于其他形式，如margin-based losses和NCE loss的变体。</p><p>对比损失函数作为无监督的目标函数，用于训练encoder。一般来说，query表示为$q = f_q(x^q)$（同理，$k=f_k(x^k)$），其中$f_a$是一个encoder，$x^q$是一个query样本。它们的实例化，取决于特定的前置任务。输入$x^q$和$x^k$可以是图像、patch、或者是由一组patch组成的context。网络$f_q$和$f_k$可以是相同的、部分共享的、或是完全不同的。</p><p><img src="https://i.loli.net/2020/08/31/uPQV7TlifsoB1Dy.png" alt="image.png"></p><p>上图是<strong>三种对比损失机制的概念上的比较</strong>。这里我们会解释对于一对query和key，这三种机制在如何维护keys和如何更新keys encoder方面有所不同。(a):计算query和key的encoder通过反向传播端到端更新，这两个encoder可以是不同的。(b):密钥表示从存储库中采样。(c): MoCo通过动量更新encoder对新的keys进行动态编码。并维护keys的队列(图中没有说明)。</p><h2 id="3-2-Momentum-Contrast"><a href="#3-2-Momentum-Contrast" class="headerlink" title="3.2 Momentum Contrast"></a>3.2 Momentum Contrast</h2><p>从上面的观点来看，对比学习是一种在高维连续输入(如图像)上构建离散字典的方法。这个字典是动态的，因为键是随机采样的，而且key encoder在训练过程中会进化。我们的假设是，好的特征可以通过包含大量负样本的大字典来学习。而字典key的encoder则尽可能保持一致，不管它的发展。基于这个动机，我们现在的动量对比描述如下。</p><p><strong>字典元素队列(Dictionary as a queue)</strong>。我们方法的核心是将字典作为一个数据样本队列来维护，这允许我们重用实时的keys。<strong>队列的引入将字典大小与batch大小解耦，所以字典大小可以比batch大小大得多。</strong>并且可以将字典的大小独立地设置为超参数。<br>当前batch被编入字典时，队列中最老的batch被删除。由于字典表示所有数据的抽样子集，因此维护字典的额外计算量是可控的。此外，<strong>删除最老的batch能够有效地保持一致性，因为其编码的keys是最过时的，与最新的keys最不一致。</strong></p><p><strong>动量更新(Momentum update)</strong>。使用队列可能会使字典变大，但它也使得通过反向传播(梯度应该传播到队列中的所有样本)来更新key encoder变得棘手。一个简单的解决方案是从query encoder$f_q$复制key encoder$f_k$，忽略这个梯度。但是这种解决方案在实验中产生的结果很差。我们假设这种故障是由于encoder的快速变化，降低了key的一致性造成的。于是我们提出了动量更新来解决这个问题。<br>形式上，我们将参数$f_k$、$f_q$指定为$\theta_k$、$\theta_q$，并通过如下公式更新$\theta_k$：<br>$$<br>\theta_k \leftarrow m\theta_k+(1-m)\theta_q<br>$$<br>在这个公式中，$m\in [0,1)$为动量系数。只有$\theta_q$通过反向传播更新。动量更新公式使得$\theta_k$能够比$\theta_q$更加顺滑地进化。<strong>因此，虽然字典队列中不同batch的keys是由不同的encoder编码出来的，但编码器之间的差别很小。</strong>实验中通常使用相关性较大的动量(0.999)效果会比小动量(0.9)来带的效果好很多，这表明缓慢进化的key encoder是使得队列起作用的核心。</p><p>略过一些不是很重要的部分。</p><h2 id="3-3-Pretext-Task"><a href="#3-3-Pretext-Task" class="headerlink" title="3.3 Pretext Task"></a>3.3 Pretext Task</h2><p>对比学习可以驱动各种各样的前置任务。由于本文的重点不是设计一个新的前置任务，所以我们使用了一个简单的前置任务。</p><p>我们将来自同一张图像的query和key视为正样本对，否则视为负样本对。于是我们对同一幅图像进行随机数据增强生成两个新图像，形成正样本对。query和key分别由其各自的encoder进行编码，encoder可以是任意卷积神经网络。</p><p><img src="https://i.loli.net/2020/08/31/v9jVwGRBfrMmHni.png" alt="image.png"></p><p>上图中的算法用来给前置任务生成伪标签。</p><p><strong>Technical details.</strong> 我们采用最后全局平均池化后面的全连接层的ResNet作为encoder，其具有固定维度的输出(128-D)。该输出向量使用L2-norm归一化，得到query或key的表示。动量公式中的温度$\tau$设为0.07。数据增强设置如下：随机resize后的图像中crop一个分辨率224×224的子图，然后经过随机颜色抖动(random color jittering)、随机的水平翻转(random horizontal flop)和随机的灰度转换(random grayscale conversion)，这些都可以在PyTorch的torchvision包中导入使用。</p><p><strong>Shuffling BN.</strong> 在实验中，我们发现使用BN对模型的训练有不利的影响。模型使用BN后会在前置任务中作弊，迅速找到低loss的解决方案。这可能是因为<strong>batch内的样本之间互相通信(由BN引起)，泄露了信息</strong>。<br>于是我们提出了Shuffling BN来解决这个问题。由于使用多个GPU进行训练，每个GPU独立地对样本执行BN。对于key encoder，我们先对当前batch中的样本顺序进行洗牌，然后再将其分配给GPU进行编码，将输出的编码恢复原本的顺序；而query encoder的样本顺序不变。这确保用于计算query及positive key的batch统计信息来自两个不同的子集，有效地解决了作弊的问题，并允许训练受益于BN。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://www.cnblogs.com/gaopursuit/p/12242946.html" target="_blank" rel="noopener">对比自监督学习</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/140908341" target="_blank" rel="noopener">CVPR 2020 | MoCo自监督学习或成为CV领域的启明灯</a><br>[3]<a href="https://www.cnblogs.com/xytpai/p/12575735.html" target="_blank" rel="noopener">何凯明组自监督方法MoCo开颅（1）</a><br>[4]<a href="https://blog.csdn.net/u013303408/article/details/103657043" target="_blank" rel="noopener">Self-Supervised Learning 自监督学习中Pretext task的理解</a><br>[5]<a href="https://www.thepaper.cn/newsDetail_forward_5575423" target="_blank" rel="noopener">数据太少怎么办？试试自监督学习</a><br>[6]<a href="https://zhuanlan.zhihu.com/p/150224914?utm_source=wechat_session&utm_medium=social&utm_oi=73454221000704" target="_blank" rel="noopener">自监督学习的一些思考</a>（入门向，推荐）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1911.05722.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Momentum Contrast for Unsupervised Visual Representati
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
      <category term="无监督学习" scheme="http://a-kali.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MoCo" scheme="http://a-kali.github.io/tags/MoCo/"/>
    
  </entry>
  
  <entry>
    <title>Hourglass &amp; CornerNet &amp; CenterNet</title>
    <link href="http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/"/>
    <id>http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/</id>
    <published>2020-08-10T16:26:01.000Z</published>
    <updated>2020-08-13T15:10:29.156Z</updated>
    
    <content type="html"><![CDATA[<p>最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。</p><h1 id="Hourglass"><a href="#Hourglass" class="headerlink" title="Hourglass"></a>Hourglass</h1><p>论文链接：<a href="https://arxiv.org/abs/1603.06937" target="_blank" rel="noopener">Stacked Hourglass Networks for Human Pose Estimation</a></p><p>Stacked Hourglass Neworks（以下简称Hourglass）由多个Hourglass模块堆叠而成，其模块对特征图进行下采样后，将特征图上采样到原来的大小，形似沙漏，故名Hourglass。</p><p>Hourglass原本是用于做<strong>人体姿态估计（Human pose estimation）</strong>，其中比较关键的一步就是检测人体上的关键点。人体不同的部位特征不同，所需要的特征图大小也不一样。Hourglass网络能够处理各种尺寸的人体特征，以此来捕捉人体各部位之间的空间关系。</p><p><img src="https://i.loli.net/2020/08/07/YBgedQokvwPlhHV.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/10/hvftgsOw6VFiXIE.png" alt="image.png"></p><p>每个Hourglass模块采用encoder-decoder结构，对输入图下采样提取特征后进行上采样，输出原图大小的heatmap作为关键点的标记。encoder和decoder之间使用残差结构融合前后特征。</p><p><img src="https://i.loli.net/2020/08/11/i2rt3GMlKbUakPN.png" alt="image.png"></p><p>该网络的思想和结构后被其它检测网络广泛采用。</p><h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="noopener">CornerNet: Detecting Objects as Paired Keypoints</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="noopener">https://github.com/umich-vl/CornerNet</a></p><p>CornerNet是一个通过检测对角来进行目标检测的网络。主要原理是采用Hourglass作为基本结构，输出检测框对角的Heatmap。</p><p>这个设计真的很<del>反人类</del>反人工智能，因为对角上应该是没有什么特征的。但是该网络的设计思想非常有意思，给后来的Anchor Free类型的目标检测网络提供了思路。</p><p><img src="https://i.loli.net/2020/08/10/GrVdznPJ1qT9pw5.png" alt="image.png"></p><p>CornerNet的整体结构如下图所示，其骨干部分使用Hourglass，输出部分有两个分支模块，分别表示<strong>左上角点预测分支</strong>和<strong>右下角点预测分支</strong>，每个分支模块包含一个<strong>corner pooling</strong>层和3个输出：<strong>heatmaps、embeddings和offsets</strong>。</p><p><img src="https://i.loli.net/2020/08/10/gDtnTWBj3PkXozR.png" alt="image.png"></p><h2 id="1-输出端"><a href="#1-输出端" class="headerlink" title="1    输出端"></a>1    输出端</h2><ul><li><p><strong>Heatmaps</strong>对角点的位置进行预测，最有可能是角点的位置输出值越高。其gt是基于角点的高斯值；</p></li><li><p><strong>Offset</strong>输出取整计算时丢失的精度信息（感觉用处不是很大）；</p><p><img src="https://i.loli.net/2020/08/11/cpO1Ueq6WPazGK3.png" alt="image.png"></p></li><li><p><strong>Embedding</strong>用来对左上角点和右下角点进行匹配。其输出一个vector，当两个角点的vector距离较小时，则认为这两点为成对点（这个和人脸匹配有点像）。这部分由两个损失函数实现，第一部分用来缩小成对点向量的距离，第二部分用来放大非成对点向量之间的距离。</p><p><img src="https://i.loli.net/2020/08/11/WBEQnLpIN2qCHx3.png" alt="image.png"></p></li></ul><h2 id="2-Corner-Pooling"><a href="#2-Corner-Pooling" class="headerlink" title="2    Corner Pooling"></a>2    Corner Pooling</h2><p>因为CornerNet是预测左上角和右下角两个角点，但是这两个角点在不同目标上没有相同规律可循，如果采用普通池化操作，那么在训练预测角点支路时会比较困难。考虑到对于每一个左上角点，其所有的特征都在其右下方。因此如果左上角角点经过池化操作后能有其右下方的信息，那么就有利于该点的预测。右下角点同理。</p><p>Corner Pooling便是一种对某个像素的右下方（或左上方）的所有像素进行池化的操作。</p><p><img src="https://i.loli.net/2020/08/11/2lW4Oi1k6qhzF3f.png" alt="image.png"></p><h1 id="CenterNet"><a href="#CenterNet" class="headerlink" title="CenterNet"></a>CenterNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a><br>Github：<a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p><p>CenterNet有两篇发布时间十分接近的两篇论文，这里我们讲其中开源项目Star比较多、受认可度比较高的<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a>。</p><p>单从网络的结构和损失函数来看，这个模型是非常符合当代神经网络设计思想的模型：设定简约，输入输出端到端，且能够适应多种任务。堪称神经网络中的艺术品。</p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h2><p>在普通的<strong>目标检测</strong>任务中，CenterNet的思想和CornerNet十分接近。只不过CornerNet输出的是边框两个角点的坐标，而CenterNet输出的是<strong>中心点的坐标及其到边框的距离</strong>。</p><p><img src="https://i.loli.net/2020/08/12/WhrlemckSsBZEpH.png" alt="image.png"></p><p>CenterNet还能用于<strong>3D目标检测(3D object detection)</strong>和<strong>多人人体姿态估计(multi-person human pose estimation)</strong>。在3D目标检测任务中，网络预测目标中心点的位置和深度、3D检测框的长宽以及目标的方向；在多人姿态估计任务中，网络预测中心点的位置以及各个关键的对中心点的偏置(offset)。</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2    Related work"></a>2    Related work</h2><p>CenterNet的改变和优势：</p><ul><li><p>在<strong>目标检测</strong>任务中，相比于RCNN系列网络，没有使用RPN、Anchor、NMS技术，没有使用阈值做前后景分类。仅仅提取特征图上的局部峰值点作为中心；</p></li><li><p>在<strong>通过关键点做目标检测</strong>的任务中，相比于CornerNet等网络，不需要对关键点进行配对操作；</p></li><li><p>在<strong>单目3D目标检测</strong>任务中，相比于Deep3Dbox等网络，同样更加简洁快速。</p><p><img src="https://i.loli.net/2020/08/13/svJWj2Np7aSlu3o.png" alt="image.png"></p></li></ul><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3     Method"></a>3     Method</h2><p>这部分和CornerNet差不多，主要讲了下输出gt的格式，公式比较多，结合源码看比较容易懂。</p><p>挑几个比较有看点的：</p><ol><li><p>在中心点位置生成<strong>和目标大小相同的高斯核</strong>，得到heatmap。而CornerNet仅仅在对角生成固定大小的高斯核；</p></li><li><p>损失函数是针对CenterNet的性质根据FocalLoss改进的<strong>像素级惩罚衰减逻辑回归FocalLoss(penalty-reduced pixel-wise logistic regression with focal loss)</strong>，中文是我瞎翻译的，感觉这么长的名字可以用来当日本轻小说的题目了。</p><p><img src="https://i.loli.net/2020/08/13/K2UojbPhNxGIVs3.png" alt="image.png"></p><p>其中的N是预测出的关键点的数量，剩下的都是FocalLoss里的超参数。其实就是像素级的FocalLoss没啥区别。</p></li><li><p>值得一提的是CenterNet对每个中心点做了<strong>偏差预测</strong>，为了弥补模型在下采样过程中造成的定位误差。</p></li></ol><p>后面的感觉都半斤八两了，佛系更新。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/u014380165/article/details/83032273" target="_blank" rel="noopener">CornerNet 算法笔记</a><br>[2]<a href="https://blog.csdn.net/c20081052/article/details/89358658" target="_blank" rel="noopener">论文精读——CenterNet :Objects as Points</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。&lt;/p&gt;
&lt;h1 id=&quot;Hourglass&quot;&gt;&lt;a href=&quot;#Hourglass&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关键点检测" scheme="http://a-kali.github.io/tags/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Hourglass" scheme="http://a-kali.github.io/tags/Hourglass/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SPFCN：全卷积网络实现停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/04/SPFCN%EF%BC%9A%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/04/SPFCN：全卷积网络实现停车位检测/</id>
    <published>2020-08-04T15:33:29.000Z</published>
    <updated>2020-08-07T13:00:42.460Z</updated>
    
    <content type="html"><![CDATA[<p>论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection</p><p>Github：<a href="https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection" target="_blank" rel="noopener">https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>摘要：对于配备自动停车系统的车辆，车位检测的准确性和速度是至关重要的。本文提出了一个基于FCN的检测器，在保证准确性的同时实现更快的速度和更小的模型尺寸。作者制定了一个策略来选择最佳感受野的卷积核，并在每次训练epoch结束后自动删除冗余通道。该模型能够联合检测停车位的角和线特征，并能在常规的处理器上有效地实时运行。该模型在2.3 GHz的CPU上能达到 30 FPS，车位角定位误差1.51±2.14 cm (std. err)，车位检测精度98%，总体满足车载移动终端速度和精度要求。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>该模型使用<a href="https://blog.csdn.net/wangzi371312/article/details/81174452" target="_blank" rel="noopener">Stacked Hourglass Network</a>作为基本结构，同时输出标志点、入口线、边界线的heatmap。此处原理和CenterNet类似。</p><p><img src="https://i.loli.net/2020/08/04/PQqr8x3LBTjpStO.png" alt="image.png"></p><p>该模型的主要特点在于其<strong>SP模块（Select-Prune Module）</strong>，SP模块分为Select模块和Prune模块两部分：</p><ul><li><p><strong>Select Module</strong>：Select模块主要用于选择拥有更合适的感受野的卷积核。考虑到移动端使用类Inception结构对算力要求过大，不能满足实时性。Select模块使用<strong>贡献评估网络（Contribution Evaluation Networks, CEN）</strong>对不同感受野的卷积核进行评估，选择贡献度最高的的卷积核。CEN只是一个简单的MLP模块，接收不同卷积核的输出作为输入，并输出一个贡献值，最终只留下平均贡献值最高的卷积核。即训练阶段结束后，整个模块最终会退化成一个卷积核。</p><p><img src="https://i.loli.net/2020/08/04/JGntQPWVTpEKy2H.png" alt="image.png"></p></li><li><p><strong>Prune Module</strong>：Prune模块负责修剪卷积核中的通道。网络交替进行训练和修剪，在修剪阶段会自动评估每个卷积通道的贡献度，并剔除贡献度低于定值的通道。其贡献度由通道的权重计算得出。如果某个卷积核没有低于定值的通道，则会自动剔除贡献值最低的通道。</p><p><img src="https://i.loli.net/2020/08/04/Us6pSbQlrJyvOE4.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/04/snCaumJKZ2qb9Ve.png" alt="image.png"></p></li></ul><p>最后对<strong>鸟瞰图（bird eye view, BEV）</strong>进行输入网络进行分割，输出BEV的heatmap。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3    Experiments"></a>3    Experiments</h1><p>数据集使用的是<a href="https://cslinzhang.github.io/deepps/" target="_blank" rel="noopener">DeepPS数据集</a>，该数据集中包含 9527(training)+2138(validating) 张BEV图像。作者将训练样本转化为224<em>×</em>224的灰度图作为网络的输入，点线标记作为labels。</p><p>整个训练过程分为三个阶段，分别为<strong>预训练阶段</strong>、<strong>选择阶段</strong>和<strong>修剪阶段</strong>，不同阶段使用不同的损失函数。</p><p><img src="https://i.loli.net/2020/08/05/6zqsPuD5icKgjZx.png" alt="image.png"></p><p>预训练阶段的前5个epoch只用heatmaps的FocalLoss用来预热（warm-up），随后10个epoch使用完整的损失函数；在选择阶段，从第一层到最后一层逐层进行选择，每层选择结束后微调2-3个epoch；在修剪阶段，损失函数再次恢复到只有heatmap的FocalLoss，训练100个epoch的同时进行修剪；最后再去掉两个正则化操作，微调15个epoch。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection&lt;/p&gt;
&lt;p&gt;Github：&lt;a href=&quot;https://github.co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="全卷积网络" scheme="http://a-kali.github.io/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>VPS-net：基于深度学习的停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/03/VPS-net%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/03/VPS-net：基于深度学习的停车位检测/</id>
    <published>2020-08-03T14:20:27.000Z</published>
    <updated>2020-08-03T15:05:57.491Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视器(AVM)进行空车位检测的精度仍有待提高。为了解决这个问题，本文提出了一种基于深度学习的空车位检测方法，即VPS-Net。VPS-net将空车位检测问题转化为两步问题：车位检测和占用分类。在停车位检测阶段，我们提出了一种基于YOLOv3的检测方法，该方法将停车位的分类与标记点的定位相结合，利用几何线索对各类停车位进行检测标记。在占用分类阶段，我们设计了一个自定义网络，该网络的卷积核大小和层数根据车位的特点进行调整。实验表明，VPS-Net在PS2.0数据集检测空停位任务中可以达到99.63%的精确率和99.31%的召回率。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>VPS-Net基于深度学习来检测各种空车位。如下图所示，VPS-Net可以处理三种典型的停车槽（横向、纵向、倾斜）。一个车位由四个顶点组成，其中两个顶点是入口线的成对标记点，另外两个顶点由于视觉的限制通常在全景图像中不可见。</p><p><img src="https://i.loli.net/2020/08/03/hgDlz7RiSqMwO28.png" alt="image.png"></p><p>下图显示了用于检测空闲停车槽的VPS-Net的基本原理。VPS-Net将空车位检测分为车位检测和占用分类两个步骤，结合了多目标检测网络和分类网络的优点。在停车槽检测阶段，首先使用基于yolov3的检测器同时检测标记点和停车槽头，然后利用几何线索匹配成对的标记点，确定停车槽的方向，最后通过车位的类型、方位和成对标记点推断出两个不可见的顶点，得到完整的车位。检测到停车槽后，将其在图像中的位置转移到占用分类部分，将检测到的停车位规则化为一个统一大小的120x46像素，然后通过一个DCNN来区分它是否是空置的。一旦检测到空车位，将其位置发送给PAS的决策模块进行进一步处理。</p><p><img src="https://i.loli.net/2020/08/03/NVnCFWv5o621eB3.png" alt="image.png"></p><p>车位的类型由车位头决定，车位头包含入口线的成对标点。因此，停车槽头和标记点的检测是停车槽检测的第一步，也是最重要的一步。我们将停车槽头的分类与标记点的定位结合到一个多目标检测问题中，从而可以很容易地根据检测结果推断出各种类型的停车位。为此，我们分别定义了“直角头”、“钝角头”、“尖角头”和“T/L形”四种车位头。</p><ol><li><p>使用yolov3检测出停车<strong>标志点</strong>(marking point)和<strong>车位头</strong>(parking slot heads)；（中心点、宽、高）</p><p><img src="https://i.loli.net/2020/08/03/D4Xy5ZOaG76rbU8.png" alt="image.png"></p></li><li><p>根据检测结果和几何特征推断出停车位的<strong>方向、成对点、车位类型</strong>；</p><ul><li>将包含在同一个车位头框框内的两个标志点归类为成对点(paired marking point)；</li><li>如果一个车位头内只包含了一个或零个标志点，但车位头的置信度足够高，则直接计算出缺失点所在位置；</li><li>如果一个车位头内包含了两个以上的标志点，则选取离对角最近的两个点。</li></ul><p><img src="https://i.loli.net/2020/08/03/69tSpzeVuhCqa8w.png" alt="image.png"></p></li><li><p>根据车位头类型和长宽推断出车位类型（垂直、平行、倾斜）</p><p><img src="https://i.loli.net/2020/08/03/UXGMjQbfJCVDZdE.png" alt="image.png"></p></li><li><p>根据以上特征和车位类型推断出<strong>被遮挡点的位置</strong>;</p></li></ol><p><img src="https://i.loli.net/2020/08/03/K9ns1f3hSlFaXot.png" alt="image.png"></p><p>分类部分比较简单，在此不多赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>业务数据分析师入门</title>
    <link href="http://a-kali.github.io/2020/07/17/%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%E5%85%A5%E9%97%A8/"/>
    <id>http://a-kali.github.io/2020/07/17/业务数据分析师入门/</id>
    <published>2020-07-17T05:44:12.000Z</published>
    <updated>2020-07-17T05:45:08.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据分析概念、作用和步骤"><a href="#1-数据分析概念、作用和步骤" class="headerlink" title="1    数据分析概念、作用和步骤"></a>1    数据分析概念、作用和步骤</h1><p><strong>数据分析</strong>是指用适当的统计方法对收集来的大量数据进行分析，将他们加以汇总和理解并消化，以求最大化地开发数据的功能，发挥数据的作用。</p><p>数据分析的<strong>作用</strong>：</p><ul><li>现状分析：企业运营情况、业务发展。通常使用日报、月报来完成。</li><li>原因分析：在现状上展开原因分析，通常使用专题分析来完成。</li><li>预测分析：对企业发展趋势做预测，为企业提供参考和决策依据。</li></ul><p>数据分析的<strong>步骤</strong>：</p><ol><li>明确分析目的和思路：目的、思路、分析框架（如PEST）、体系化；</li><li>数据收集：数据库、出版物、互联网、市场调查；</li><li>数据处理：整理成适合数据分析的形式；</li><li>数据分析：将数据整理分析成结论，数据挖掘是一类高级的数据分析方法；</li><li>数据展现：表格、图像；</li><li>撰写报告：分析、结论、建议或解决方法。</li></ol><h1 id="2-数据分析方法论"><a href="#2-数据分析方法论" class="headerlink" title="2    数据分析方法论"></a>2    数据分析方法论</h1><p>确定分析思路以营销、管理等理论为指导，一般把这些数据分析相关的营销、管理等理论统称为<strong>数据分析方法论</strong>。</p><h2 id="PEST分析法"><a href="#PEST分析法" class="headerlink" title="PEST分析法"></a>PEST分析法</h2><p><a href="https://baike.baidu.com/item/PEST/10805117?fr=aladdin" target="_blank" rel="noopener">PEST</a>是指对<strong>政治（Political）、经济（Economic）、技术（Technological）和社会（Social）</strong>这四类影响企业的主要外部环境因素进行分析。</p><ul><li>政治：新政策，经济政策，市场道德标准，文化宗教等；</li><li>经济：社会经济结构，经济发展水平，经济体制，经济状况等；</li><li>社会：人口，流动性，消费心理，生活方式变化，价值观等；</li><li>技术：新技术的发明、传播等。</li></ul><h2 id="5W2H分析法"><a href="#5W2H分析法" class="headerlink" title="5W2H分析法"></a>5W2H分析法</h2><p>5W2H分析法是从回答中发现解决问题的线索的方法，广泛应用于企业营销管理活动等方面。</p><p><img src="https://i.loli.net/2020/07/12/mUvrcf5yDjSgQYL.png" alt></p><h2 id="逻辑树分析法"><a href="#逻辑树分析法" class="headerlink" title="逻辑树分析法"></a>逻辑树分析法</h2><p>逻辑树分析法是将一个已知问题当成树干，然后考虑这个问题和哪些问题相关。每想到一个问题就可以在所在树干下加一个树枝，并表明树枝代表什么问题。以此来将问题逐步分解。</p><p><img src="https://i.loli.net/2020/07/12/8hiW9d2t4DN6pPg.png" alt="image.png"></p><h2 id="4P营销理论"><a href="#4P营销理论" class="headerlink" title="4P营销理论"></a>4P营销理论</h2><p>4P营销理论将营销要素概括为如下四类：<strong>产品（Product）、价格（Price）、促销（Promotion）、渠道（Place）</strong>。如果要了解公司的整体运营情况，就可以采用4P营销理论进行分析指导。</p><p><img src="https://i.loli.net/2020/07/12/rHb25QgRoEukc13.png" alt="image.png"></p><h2 id="用户使用行为理论"><a href="#用户使用行为理论" class="headerlink" title="用户使用行为理论"></a>用户使用行为理论</h2><p> 用户使用行为是指用户行为获取、使用物品或服务所采取的各种行动，一般按照以下过程：<strong>认知产品、熟悉、试用、使用、成为忠实用户</strong>。</p><p><img src="https://i.loli.net/2020/07/12/KUDOub7vPJ5ZSdh.png" alt="image.png"></p><h1 id="3-常用数据分析方法"><a href="#3-常用数据分析方法" class="headerlink" title="3    常用数据分析方法"></a>3    常用数据分析方法</h1><h2 id="对比分析法"><a href="#对比分析法" class="headerlink" title="对比分析法"></a>对比分析法</h2><p>将两个或两个以上的数据进行比较，分析其中差异。包括与目标值对比、不同时期对比、同级别对比、行业内对比、活动效果对比。</p><h2 id="分组分析法"><a href="#分组分析法" class="headerlink" title="分组分析法"></a>分组分析法</h2><p><img src="https://i.loli.net/2020/07/12/m6SlcrROhxHqi71.png" alt="image.png"></p><h2 id="结构分析法"><a href="#结构分析法" class="headerlink" title="结构分析法"></a>结构分析法</h2><p><img src="https://i.loli.net/2020/07/12/ZQfzjATtUWeupEO.png" alt="image.png"></p><h2 id="平均分析法"><a href="#平均分析法" class="headerlink" title="平均分析法"></a>平均分析法</h2><p><img src="https://i.loli.net/2020/07/12/YFLxZ91EAJrWcf5.png" alt="image.png"></p><h2 id="杜邦分析法"><a href="#杜邦分析法" class="headerlink" title="杜邦分析法"></a>杜邦分析法</h2><p>杜邦分析法是利用主要<strong>财务指标</strong>之间的内在联系，对企业财务状况以及经济效益进行综合分析评价的方法。</p><p>![image-20200712161837746](C:\Users\STEVE JOBS\AppData\Roaming\Typora\typora-user-images\image-20200712161837746.png)</p><h2 id="漏斗图分析法"><a href="#漏斗图分析法" class="headerlink" title="漏斗图分析法"></a>漏斗图分析法</h2><p>漏斗图是一个适合业务流程比较规范、周期比较长、各流程环节涉及复杂业务比较多的管理分析工具。漏斗图能很快在复杂的环节中找出主要因素。</p><p><img src="https://i.loli.net/2020/07/12/NXEUSLAfxFCyhr2.png" alt="image.png"></p><h1 id="4-数据图表讲解"><a href="#4-数据图表讲解" class="headerlink" title="4    数据图表讲解"></a>4    数据图表讲解</h1><p><strong>选择恰当的图表（重要）：</strong></p><p><img src="https://i.loli.net/2020/07/12/thQ7YKC2PgEkpRA.png" alt="image.png"></p><h2 id="展现数据的表格技巧"><a href="#展现数据的表格技巧" class="headerlink" title="展现数据的表格技巧"></a>展现数据的表格技巧</h2><ol><li>突出显示单元格：对满足条件的单元格突出显示。（Excel：开始-样式-条件格式-突出显示单元格规则）</li><li>项目选取： 同上。（Excel：开始-样式-条件格式-项目选取规则）</li><li>数据条： 在表格单元格中显示柱状图，直观显示数值大小。（Excel：开始-样式-条件格式-数据条）</li><li>图标集：对数值分类，不同类别数值显示不同图标。（Excel：开始-样式-条件格式-图标样式）</li></ol><p>效果图：</p><p><img src="https://i.loli.net/2020/07/12/yQDGZrlNw2uRk8P.png" alt="image.png"></p><h2 id="各种图表展示"><a href="#各种图表展示" class="headerlink" title="各种图表展示"></a>各种图表展示</h2><p><img src="https://i.loli.net/2020/07/12/WCUdyFXJHxpl2Sw.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/YgKNH75vTlp9UeQ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/lPu4S6imsHEaY3G.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/HeV5PzsAmir2gpR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/9DWL2VoxKpZwJsR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/cUNn8bHgLjXBJhp.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/TieQ2JuEaODytkg.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/1MjT3GmWYsHeDtl.png" alt="image.png"></p><h1 id="5-数据关键指标"><a href="#5-数据关键指标" class="headerlink" title="5    数据关键指标"></a>5    数据关键指标</h1><h2 id="5-1-产品经理常用数据指标（KPI）"><a href="#5-1-产品经理常用数据指标（KPI）" class="headerlink" title="5.1    产品经理常用数据指标（KPI）"></a>5.1    产品经理常用数据指标（KPI）</h2><p><img src="https://i.loli.net/2020/07/12/kO5VWU74otcK2Fj.png" alt="image.png"></p><h2 id="5-2-电商类网站分析指标"><a href="#5-2-电商类网站分析指标" class="headerlink" title="5.2    电商类网站分析指标"></a>5.2    电商类网站分析指标</h2><p><img src="https://i.loli.net/2020/07/12/WoRqaPCTliugHfI.png" alt="image.png"></p><h3 id="5-2-1-内容指标"><a href="#5-2-1-内容指标" class="headerlink" title="5.2.1    内容指标"></a>5.2.1    内容指标</h3><p><strong>转化率</strong>：转化率 = 进行了相应动作的访问量 / 总访问量。用于衡量网站内容对访问者的吸引程度和网站的宣传效果。</p><p><strong>回访者比率</strong>：转化率 = 回访者数 / 独立访问者数。衡量网站内容对访问者的吸引程度和网站实用性，你的网站是否有令人感兴趣的内容使访问者再次访问你的网站。</p><ul><li>积极回访者比率：积极回访者比率 = 访问超过11页的用户 / 总访问数。衡量有多少访问者度网站内容高度感兴趣。</li><li>忠实访问者比率：忠实访问者比率 = 访问时间在xx分钟以上的用户 / 总访问数。</li></ul><p><strong>忠实访问者指数</strong>：忠实访问者指数 = 大于xx分钟的访问页数 / 大于xx分钟的访问者数。衡量“忠实访问者”是否真的在浏览页面。</p><p>忠实访问者量：忠实访问者量 = 大于xx分钟的访问页数 / 总访问页数。表现该网站是否吸引了错误的访问者。</p><p>访问者参与指数：访问者参与指数 = 总访问数 / 独立访问者数。</p><p><strong>回弹率</strong>：回弹率 = 只访问了一页的访问数 / 总访问数。表示访问者只看到了一页的比率。如果这个指标高则表明可能网页布局需要优化。</p><p>浏览用户相关：</p><ul><li>浏览用户比率：少于1分钟的访问者数 / 总访问数。</li><li>浏览用户指数：少于1分钟的访问页数 / 少于1分钟的访问者数。</li><li>浏览用户量：少于1分钟的浏览页数 / 所有浏览页数。</li></ul><h3 id="5-2-2-商业指标"><a href="#5-2-2-商业指标" class="headerlink" title="5.2.2    商业指标"></a>5.2.2    商业指标</h3><p><img src="https://i.loli.net/2020/07/12/IBUNDniwFX2gm59.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/tGmynaOofjDg14A.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/X4LSfPsdbxJ1EzB.png" alt="image.png"></p><h3 id="5-2-3-其它指标"><a href="#5-2-3-其它指标" class="headerlink" title="5.2.3    其它指标"></a>5.2.3    其它指标</h3><p><img src="https://i.loli.net/2020/07/12/rsoViW5gzMTZwqx.png" alt="image.png"></p><h1 id="6-数据分析报告"><a href="#6-数据分析报告" class="headerlink" title="6    数据分析报告"></a>6    数据分析报告</h1><p><strong>数据分析报告</strong>是根据数据分析方法，运用数据来反映某项事物的结论，并提出解决办法的分析应用文体。</p><p>数据分析报告的<strong>原则</strong>：</p><ul><li>规范性：术语规范；</li><li>重要性：选取重要的、关键的指标；</li><li>谨慎性：真实、严谨、实事求是；</li><li>创新性：特征挖掘。</li></ul><p>数据分析报告的<strong>作用</strong>：展示分析结果、验证分析质量、提供决策参考。</p><p>数据分析报告的<strong>种类</strong>：专题分析报告（单一性、深入性）、综合分析报告（全面性、联系性）、日常数据通报（进度性、规范性、时效性）。</p><p><strong>数据分析报告的构成</strong>：</p><ul><li>标题页：简洁的标题；</li><li>目录；</li><li>前言：分析背景、分析目的、分析思路；</li><li>正文：通过图表和文字系统全面地表述分析的过程和结果，表明所有数据分析事实和观点，各部分之间具有逻辑关系。</li><li>结论和建议；</li><li>附录：名词解释、计算方法、原始数据等。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-数据分析概念、作用和步骤&quot;&gt;&lt;a href=&quot;#1-数据分析概念、作用和步骤&quot; class=&quot;headerlink&quot; title=&quot;1    数据分析概念、作用和步骤&quot;&gt;&lt;/a&gt;1    数据分析概念、作用和步骤&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;数据分析&lt;/st
      
    
    </summary>
    
      <category term="数据分析" scheme="http://a-kali.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>证券投资概述</title>
    <link href="http://a-kali.github.io/2020/05/04/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/05/04/证券投资概述/</id>
    <published>2020-05-04T05:14:24.000Z</published>
    <updated>2020-05-04T05:15:28.716Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-证券家族四兄弟"><a href="#1-证券家族四兄弟" class="headerlink" title="1    证券家族四兄弟"></a>1    证券家族四兄弟</h1><h2 id="1-1-债券"><a href="#1-1-债券" class="headerlink" title="1.1    债券"></a>1.1    债券</h2><p><strong>定义：政府和企业借债，向投资人开具的借条。规定了期限，还本付息。</strong></p><p>现代债券出现于十二世纪的意大利，国家<strong>将巨额债务拆分成标准化的小块出售，向百姓筹集资金，并定期还本付息。</strong></p><p>债券的种类有上百种，但可以进行以下几种归类。债券根据期限可以分为<strong>短期债券、中期债券、长期债券</strong>，划分界限为一年和十年。通常来说债券偿还期越长风险越大，收益也越高。</p><p>债券根据发行人的不同可以分为<strong>政府债券、金融债券、企业债券</strong>。政府债券是信用最好、收益最稳定、风险最小的债券，由政府的权力进行保障。政府债券可分为<strong>中央政府债券</strong>和<strong>地方政府债券</strong>，中央政府债券也被称为<strong>金边债券</strong>。金融债券的风险和收益通常高于政府债券，金融企业通常包括商业银行、证券公司、保险公司。这些企业资金雄厚、实力强大。在中国这类企业通常有国资背景。企业债券为在证券交易所上市的企业发行的债券，风险较大。</p><p>债券的风险小，收益较低。在股市熊市的时候可以作为一种不错的投资选项。<strong>债券基金</strong>相比于债券流动性更强，购买三天后即可赎回。</p><p>债券按照信用等级可以被分为<strong>垃圾债</strong>和<strong>投资债</strong>。</p><h2 id="1-2-股票"><a href="#1-2-股票" class="headerlink" title="1.2    股票"></a>1.2    股票</h2><p><strong>定义：企业向投资人发行的共担风险、共享收益的所有权凭证。</strong></p><p>普通股（A股）的三大性质：</p><ol><li><strong>有限责任</strong>：投资人所需承担的风险取决于投资的金额。</li><li><strong>分红</strong>：按照股份分红，企业挣的越多分红越多。</li><li><strong>投票权</strong>：股东享有企业决策的投票权，股份越多权重越大。</li></ol><p>优先股：优先比普通股的股东分红，但股东不能参与公司决策。</p><p>决策权股（B股）：企业授权给管理层的股票，其决策权重高于普通股。</p><p>股票收益来源于两个方面：<strong>分红</strong>和<strong>资本利得</strong>。分红为从企业收益中分享给投资者的红利；资本利得为股票买卖的差价。</p><h2 id="1-3-基金"><a href="#1-3-基金" class="headerlink" title="1.3    基金"></a>1.3    基金</h2><p><strong>定义：基金是代人理财的机构。代人投资证券的机构为证券投资基金。</strong></p><p>特征：</p><ol><li>共担风险，共享收益。</li><li>专家理财：将散户的钱统一交给专家投资。</li><li>分散风险：一个基金可以投资上百个股票。</li></ol><p>按照投资对象对基金分类：</p><ol><li><strong>债券基金</strong></li><li><strong>股票基金</strong></li><li><strong>混合基金</strong>：既投资债券又投资股票的基金。</li><li><strong>货币基金</strong>：投资货币性金融资产（即一年期以内的金融资产，流动性强，风险小）的基金。余额宝是一种货币基金。</li></ol><p>在股市熊市的时候可购买货币基金，在牛市的时候如果不知道该买哪只股票可全仓股票基金和混合基金。</p><p>按照基金风格分类：</p><ol><li><strong>主动型基金</strong>：只挑选少量股票且不断换股，以更高收益为目的。对基金经理要求高，交易手续费较高，为1.5%。</li><li><strong>被动型基金</strong>：挑选大量股票，分散风险，不换股。手续费低，为0.5%。其中以指数基金最为典型。指数基金是以某特定指数（比如沪深300）的成分股为投资对象的被动型基金。在证券市场可直接交易的指数基金称为<strong>ETF</strong>。</li></ol><p>大部分主动型基金跑不过指数基金。</p><p>基金还可以被分为<strong>私募基金（对冲基金）</strong>和<strong>公募基金（共同基金）</strong>。公募基金在公共场合公开募集资金，而私募基金在公开场合是买不到的。公募基金门槛低，通常散户就能投资；而私募基金则对投资金额、投资人财产情况、收入等有要求。公募基金通常收到政府和企业的严格监管，有严格的限制和流程。公募基金经理收入来源于手续费，私募基金经理来源于投资收入的一部分（投资负收益则没有收入）。私募基金收到的监管少，通常良莠不齐。</p><p>万金油投资——指数基金定投。门槛低，成本低，避免追涨杀跌。</p><p>指数基金可分为<strong>宽基指数基金</strong>和<strong>行业指数基金</strong>。宽基即面向各行各业（如沪深300），行业指数基金则对应单一行业（如白酒ETF）。指数基金还可以被分为<strong>场内基金</strong>和<strong>场外基金</strong>。场内基金在证券交易所和股票软件购买，可以像股票一样实时交易；场外基金可以在银行、支付宝、股票软件购买，不能实时交易。</p><p>比较知名的ETF：</p><ol><li>沪深300：从上海和深圳证券市场中选取最优的300只大型企业A股作为样本编制而成的成份股指数,原则上每半年调整一次样本股。</li><li>中证500：排在沪深300之后的500只中小企业A股。</li><li>上证50：上海证券市场前50，包括茅台、中国平安和招商银行等。</li><li>上证指数：上海证券综合指数，这个指数的样本是在上海证券交易所全部上市股票，这里包括A股和B股。（2600-2700低买，2900-3000高卖）</li><li>创业50：创业板前50。</li><li>创业指数：创业板前100。 </li></ol><h2 id="1-4-衍生工具"><a href="#1-4-衍生工具" class="headerlink" title="1.4    衍生工具"></a>1.4    衍生工具</h2><p><strong>定义：在股票、债券、 商品、货币的买卖合同上加上新的买卖合同，就是衍生工具。主要包括远期合约、期货、期权、互换。</strong></p><p><strong>远期合约</strong>：易双方约定在未来的某一确定时间，以确定的价格买卖一定数量的某种金融资产的合约。远期合约通常是大合同，由于未来的不确定性，远期合约有较高的被毁约的风险。</p><p><strong>期货</strong>：以某种大众产品如棉花、大豆、石油等及金融资产如股票、债券等为标的标准化可交易合约。具有多次交易（保证金+实际金额）、杠杆、高收益高风险等特性。</p><p><strong>期权</strong>：合约赋予持有人在某一特定日期或该日之前的任何时间以固定价格购进或售出一种资产的权利。需要支付权利金购买。</p><p><strong>互换</strong>：互换两个或两个以上的当事人按照共同商定的条件，在约定的时间内定期交换现金流的金融交易，可分为货币互换、利率互换、股权互换、信用互换等类别。如人民币换美元。</p><p>期货的交易为跨期交易，共分两次交易。第一次交易叫做<strong>开仓</strong>，只需交保证金；第二次交易叫<strong>平仓</strong>，需支付全额。由于期货第一次交易只需要交少量保证金，以小博大，所以期货是加杠杆的。认为货物涨价，先买后卖的订单叫<strong>多单</strong>；认为货物降价，先卖后买的订单叫<strong>空单</strong>。空单时可以向期货交易所借货卖出，并在一定期限后买回归还。</p><p>期货的意义：</p><ol><li>对企业来说可以锁住风险，安心经营。</li><li>期货能直观反映一个货物的涨跌。</li><li>具有国家战略意义（比如可以用人民币结算石油期货）。</li></ol><h2 id="加餐：证券玩家、投资者与投机者"><a href="#加餐：证券玩家、投资者与投机者" class="headerlink" title="加餐：证券玩家、投资者与投机者"></a>加餐：证券玩家、投资者与投机者</h2><p>证券市场上存在三种人：证券玩家、投资者和投机者。</p><p>证券玩家对市场上的风吹草动十分敏感，每天都在追涨杀跌做短线。这类人通常挣不到钱。</p><p>投资者通常研究优质企业，进行长线投资。</p><p>投机者介于投资者和证券玩家之间，关心大趋势。比如贸易战、加息、汇率变动等。</p><h1 id="2-证券市场"><a href="#2-证券市场" class="headerlink" title="2    证券市场"></a>2    证券市场</h1><p>证券市场根据职能可以分为<strong>证券发行市场</strong>和<strong>证券交易市场</strong>，根据交易点可分为<strong>场内市场</strong>和<strong>场外市场</strong>，场内即证券交易所内，场外通常是在商业银行柜台进行交易。证券市场根据服务对象可分为<strong>一板、二板、三板</strong>。一板通常给大型企业上市，二板是创业板，三板是场外市场，被称为<strong>股权代办交易系统</strong>，证券公司在三板代办不能在主板上市的公司股票。</p><h2 id="2-1-证券发行市场"><a href="#2-1-证券发行市场" class="headerlink" title="2.1    证券发行市场"></a>2.1    证券发行市场</h2><p>证券发行的方式包括<strong>私募</strong>和<strong>公募</strong>。私募为私下发行证券，向特定投资人发行新证券；公募在市场上公开募集，需要向市场和社会公开信息，比如发行人的经济状况、证券的特点等。股票发行需要公开<strong>招股说明书</strong>，内容包括企业的经营状况、主营业务、未来收入等。</p><p>证券发行还可以被分为<strong>直接发行</strong>和<strong>间接发行</strong>。直接发行是直接将证券发给投资人，间接发行是将证券通过<strong>证券公司</strong>（又称<strong>券商</strong>、<strong>投行</strong>）发行。间接发行分为<strong>代销</strong>和<strong>包销</strong>。代销时，证券公司仅作为销售平台，不负责任；包销时，证券公司收购所有证券并加手续费卖出，卖不出去的证券只能证券公司自己留着。</p><p>发行定价的方式：</p><ol><li><strong>市盈率(PER)定价</strong>：市盈率指股票价格除以每股盈利的比率。成熟的市场国家市盈率通常为15倍，我国通常按照20倍市盈率定价。</li><li><strong>净资产定价</strong>：净资产指指企业的资产总额减去负债以后的净额。股票的价格按照每股净资产的一定倍数定价。</li><li><strong>竞价</strong>。</li></ol><p>在我国主要是市盈率定价。</p><h3 id="注册制"><a href="#注册制" class="headerlink" title="注册制"></a>注册制</h3><p>注册制由<strong>核准制</strong>发展而来。核准制下监管部门对上市企业制定相应标准，并对符合标准的企业进行实质性审核；而注册制下监管部门制定严格的标准并审核相关材料，剩下的由证券交易所和证券公司进行安排。注册制启用后，上市企业将变得远多于核准制。注册制时成熟的市场经济国家的证券市场常规的办法。注册制目前在我国创业板试点。</p><p>注册制的影响：</p><ol><li>新股不会受到严格的审查监管，股民对炒新股会更加谨慎。</li><li>大浪淘沙，留住好企业。市场的入口和出口都会放得更开，仙股（股价低于1元）更容易退市。</li><li>注册制将留住好企业，引导市场变得成熟，牛市更长熊市更短。</li></ol><h3 id="加餐：上市公司割韭菜的套路"><a href="#加餐：上市公司割韭菜的套路" class="headerlink" title="加餐：上市公司割韭菜的套路"></a>加餐：上市公司割韭菜的套路</h3><ol><li>假装回购（回购：上市公司将股票的一部分买回来注销，减少股票量来抬高股价），实际上并没有回购或者回购数量没有预定的那么多，吸引散户抬高股价。</li><li>无意义的增发股票圈钱，但并没有用获得的钱做利于公司发展的事。</li><li>不分红。</li></ol><h2 id="2-2-证券交易市场"><a href="#2-2-证券交易市场" class="headerlink" title="2.2    证券交易市场"></a>2.2    证券交易市场</h2><p><strong>证券交易所</strong>是以拍卖形式交易旧证券的场所。证券交易所里有两类人：<strong>内部管理人员</strong>和<strong>交易所会员</strong>。交易所会员的注册要求非常高，上海证券交易所的注册资本为500万元。交易所会员有两类人：<strong>经纪人</strong>和<strong>交易商</strong>。经纪人即是证券公司，普通人通过证券公司进行投资；交易商是有大型证券订单的大公司。</p><p>证券市场交易时间为每周一到周五上午时段9:30-11:30，下午时段13:00-15:00。周六、周日及上海证券交易所、深圳证券交易所公告的休市日不交易。由于我国市场太不成熟，所以股票交易由原先的T+0制度改为T+1制度，并设定了涨跌停板。</p><p>在上市公司股票价格出现重大波动，上市公司有重大决策时，为了引起投资者注意，停止股票交易，称为<strong>停牌</strong>；而<strong>摘牌</strong>指将不符合条件的上市公司踢出证券市场。</p><p>我国为了一定程度上维护投资者利益，实施了<strong>ST(Special Treatment)制度</strong>。当一个企业变成仙股，就会被扣上ST的帽子。如果企业继续下行，ST会变成*ST，临近退市边缘。ST制度会让部分垃圾企业成为巨婴，如今我国正严格退市制度，将垃圾企业清理出市。</p><h1 id="3-证券基本分析"><a href="#3-证券基本分析" class="headerlink" title="3    证券基本分析"></a>3    证券基本分析</h1><h2 id="3-1-宏观环境"><a href="#3-1-宏观环境" class="headerlink" title="3.1    宏观环境"></a>3.1    宏观环境</h2><p>宏观环境主要包括三个方向：<strong>社会大环境、大政方针、经济周期与反周期</strong>。</p><p>社会大环境：人口老龄化（医疗）、人民收入提高（消费）等。</p><p>大政方针：中国制造2025、一带一路、雄安新区等。</p><p>经济周期：人们在预料到经济周期前通常会对股票进行买入或卖出，所以股市通常是经济周期的先行指标。部分行业不受经济周期影响，比如食品行业、烟草行业、医疗行业等；部分行业受经济周期影响较大，比如钢铁、冶金、建材行业。</p><p>反周期的政策：经济周期处于萧条期时不利于经济发展，此时政府会推出反周期的政策来刺激经济。</p><h2 id="3-2-行业对股市的影响"><a href="#3-2-行业对股市的影响" class="headerlink" title="3.2    行业对股市的影响"></a>3.2    行业对股市的影响</h2><p>行业：指一组提供同类相互密切替代商品或服务的公司。</p><p>判断一个行业的时候一定要知道其挣钱模式，即其面向的客户、出售的产品等。</p><p>行业和人一样有从幼年、青年、中年到老年的生命周期。幼年期的企业处于创业期，以亏损为主；青年期的企业竞争对手多，风险大；中年企业产品定型、市场成熟，可以作为主要投资方向。</p><p>周期股与防御股：珠宝、建材、钢铁、有色金属受经济影响大，建议在牛市时作为主要买入对象；食品、烟酒、医药、调料等受经济影响小，可以在熊市的时候投资。</p><h2 id="3-3-企业对股市的影响"><a href="#3-3-企业对股市的影响" class="headerlink" title="3.3    企业对股市的影响"></a>3.3    企业对股市的影响</h2><p><strong>得天独厚的自然条件、品牌技术专利、研发投入、顾客的转化成本</strong>是企业无法被替代的重要条件。举例：</p><ul><li>茅台在赤水河旁边，所处气候适合酿造高品质的白酒；</li><li>茅台有很多酱香酒的专利，无法被模仿；</li><li>华为麒麟980芯片研发投入3亿美元，令普通公司望而却步;</li><li>微软和苹果的PC端操作系统用户、腾讯QQ和微信用户转化成本非常高；</li><li>滴滴网约车、摩拜共享单车的模式门槛低，容易产生竞争对手。</li></ul><h2 id="加餐：粗看股票"><a href="#加餐：粗看股票" class="headerlink" title="加餐：粗看股票"></a>加餐：粗看股票</h2><ol><li>看F10中的<strong>每股收益、市盈率、企业主营业务、竞争对手、竞争对手的股价</strong>。其中以市盈率为主。如果是高成长的行业，30-40倍的市盈率是可以接收的；如果是成熟行业，市盈率通常在15倍以下为佳；如果是衰退行业，市盈率尽量选择在10倍以下。（股票中的F10的意思是指在分析软件中按键盘上的F10可以查看该公司基本面的文本资料，指向该股的基本公开信息，包括股本、股东、财务数据、公司概况和沿革、公司公告、媒体信息等等，都可快速查到。）</li><li>看<strong>毛利率</strong>（销售利润/销售收入）：毛利率＞20%为良，＞40%为优；</li><li>看<strong>净资产收益率</strong>（(资产-负债)/收益）：净资产＞15%为优。</li></ol><p>这三个指标不能只看某一年的数据，某一年的数据不能作为常态，应当看近几年的数据。</p><p>K线图可以看是否处于历史低位或者单日U型、V型底部。</p><h1 id="4-证券技术分析"><a href="#4-证券技术分析" class="headerlink" title="4    证券技术分析"></a>4    证券技术分析</h1><h2 id="4-1-图形识别"><a href="#4-1-图形识别" class="headerlink" title="4.1    图形识别"></a>4.1    图形识别</h2><p><strong>压力（阻力）与支撑</strong>：支撑位是指在股价下跌时遇到买方大量买入，从而止跌回稳的价位；其对应的是阻力位。</p><p>压力位与支撑位的转换：压力位意味着股价上方有一群套牢盘，许多人之前在该点位买入并随着股价下跌而亏损，想在该点位卖出。如果股价突破了压力位继续上涨，在压力位卖出的人又会后悔，想在该点买入，此时压力位就转换成了支撑位。支撑位同理。</p><p>当股价多次试图突破压力位时没成功时，就会在压力位套牢一大批人，留下多个压力位，股价转跌。支撑位同理。</p><p><img src="https://i.loli.net/2020/05/03/nV26gKahL9QOcUP.png" alt="ISBW11D8MS_@P29_PR_QST1.png"></p><p><strong>趋势线</strong>就是由压力点或支撑点组成的呈上涨/下跌/水平趋势的<strong>上下轨线</strong>，通过趋势线可以看出股价的趋势；当趋势不明显或者水平的时候，趋近上轨时卖出，趋近下轨时买入。</p><p><img src="https://i.loli.net/2020/05/03/b8owYM2zQTdnDSP.png" alt="DU5`FA_HCC80SGLI16_D__3.png"></p><p>根据压力和支撑理论，可以衍生出头肩底、W底、圆弧底等图形理论，在此不多赘述。</p><p><strong>缺口理论</strong>：</p><p><img src="https://i.loli.net/2020/05/03/cR98qnYl2w4iQaj.png" alt="V@N_B9_P_UL0MLZ_O2UB4_V.png"></p><h2 id="加餐：蜡烛图与K线形态组合"><a href="#加餐：蜡烛图与K线形态组合" class="headerlink" title="加餐：蜡烛图与K线形态组合"></a>加餐：蜡烛图与K线形态组合</h2><p>蜡烛图：</p><p><img src="https://i.loli.net/2020/05/04/75FuKJWsMceBUCf.png" alt="_`8Y2IE_MQA_8QNJE_FKGXF.png"></p><p>连续行情+同价线=行情结束</p><p>连续行情+同价线+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/gKlEu63dLDshIbW.png" alt="5X__V`G@_RN1@ZE3V_JC_C5.png"></p><p>连续行情+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/BAm2JskVGLyQh4e.png" alt="_KI4__0FV8A2M_S4_`PEQPS.png"></p><p><img src="https://i.loli.net/2020/05/04/t8AsOuDdEZ5aTpM.png" alt="DM0KUNY_9SN65H_V__~_`PL.png"></p><p>成交密集区的跳空缺口=横盘结束</p><p><img src="https://i.loli.net/2020/05/04/xiguQbB4oXSypm1.png" alt="7_MJWHJ___N_@1_1`R_XLMF.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-证券家族四兄弟&quot;&gt;&lt;a href=&quot;#1-证券家族四兄弟&quot; class=&quot;headerlink&quot; title=&quot;1    证券家族四兄弟&quot;&gt;&lt;/a&gt;1    证券家族四兄弟&lt;/h1&gt;&lt;h2 id=&quot;1-1-债券&quot;&gt;&lt;a href=&quot;#1-1-债券&quot; class
      
    
    </summary>
    
      <category term="经济与金融" scheme="http://a-kali.github.io/categories/%E7%BB%8F%E6%B5%8E%E4%B8%8E%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="证券投资" scheme="http://a-kali.github.io/tags/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：使用RNN进行情感识别</title>
    <link href="http://a-kali.github.io/2020/03/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/03/03/论文笔记：使用RNN进行情感识别/</id>
    <published>2020-03-03T08:31:11.000Z</published>
    <updated>2020-03-03T09:22:57.794Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1701.08071" target="_blank" rel="noopener">Emotion Recognition From Speech With Recurrent Neural Networks</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在<strong>自动语音识别(Automated Speech Recognition, ASR)</strong>任务中，语音被模型转化成文字。但是在人们的对话过程中除了文本以外还有其它重要的信息，比如<strong>语调、情感、响度</strong>。这些信息在语音理解中亦扮演者十分重要的角色。本文将围绕其中“情感”部分进行概述，即<strong>语音情感识别(Speech Emotion Recognition, SER)</strong>。</p><p>在语音情感识别方面存在一些难点：</p><ol><li><strong>情感是主观的</strong>，不同人对于同一段语音，理解出的情感可能不同。</li><li><strong>同一段语音可能包含多种情感</strong>。（可以通过<a href="https://zhuanlan.zhihu.com/p/86745594" target="_blank" rel="noopener">CTC损失函数</a>解决）</li><li><strong>数据来源</strong>：从电影中截取的语音可能和现实中存在偏差。通常会找专业演员来演绎各种情感来制造数据。</li></ol><h1 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2    Related works"></a>2    Related works</h1><p>大部分文献将语音情感识别视为一个分类问题，对每一个utterance分配一个label。utterance即为一小段语音，是语音的最小单元。</p><p>在深度学习之前，大多研究提取底层的手工特征，用传统分类器进行分类，比如HMM（隐马尔可夫模型）或GMM（高斯混合模型）。</p><p>深度学习出现后，有人把utterance分帧计算低层特征，用三层全连接层，对输出概率聚合成utterance水平的特征（用简单的统计量，比如最大值，最小值，平均值等），最后用ELM（Extreme Learning Machine）分类。</p><p>后面出现了纯深度学习和端到端的架构模型。有人使用Attention CNN，有人用DBN，还有人用迁移学习把语音识别的任务（数据集）迁移到语音情感识别中。</p><h1 id="3-Data-and-preprocessing"><a href="#3-Data-and-preprocessing" class="headerlink" title="3    Data and preprocessing"></a>3    Data and preprocessing</h1><p>IEMOCAP（Interactive Emotional Dyadic Motion Capture）被选作数据集，因为它有详尽的获取方法，免费的学术许可，较长的语音时长和良好的标注。</p><p>大约包括12个小时，含有视频，音频和人脸关键点的数据。由南加利福尼亚大学戏剧系的10位专业演员表演所得。评估者对每个utterance给出评价（10个情感选项），当一半以上的评估者对某个utterance的评估一致时，该utterance才分配到评估的感情。本文中选取其中4种情感用于分析（生气，兴奋，中立和伤心），只有这些样本才被考虑到本文工作中，下图是标签分布。</p><p><img src="https://img2018.cnblogs.com/blog/1160281/201811/1160281-20181113191458804-356625669.png" alt="img"></p><p>原始信号的采样率是16kHz，直接使用计算量很大，需要尽量保持信息的同时减小计算量。本文对utterance进行分帧，帧长为200ms，帧移为100ms，在帧上计算声学特征（用了哪些声学特征见下文介绍），然后把这些特征合在一起作为utterance的特征输入到模型。关于帧长的选取，论文从30ms到200ms都做过实验发现效果差别不大，而较长的帧可以导致比较少的帧，能减小计算量，所以使用了200ms。</p><p>对于语音信号的特征主要有三种，一是声学特征，也就是声波的一些属性；二是音律特征，指的是停用词，韵律（押韵，平仄），响度，这个特征依赖于说话人，所以没有用这类特征；三是语义学特征，就是语音对应的文字内容的信息。</p><p>本文只使用了声学特征，使用的是python库PyAudioAnalysis的API提供的34个特征，主要包括3个时域特征（过零率，能量，能量熵），5个谱特征，13个MFCC特征，13个音阶特征。也就是一帧的声音用34维的向量来表示。</p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4    Approach"></a>4    Approach</h1><p>因为一个utterance只对应一个标签，但是有很多帧，有些帧是不包含情感的，所以输入序列和输出序列难以一一对应，为了应对这个问题，可以使用CTC（Connectionist Temporal Classification）的方法。</p><p>CTC模型中的LSTM的输入时间步和输出时间步T为78，因为每个语音样本划分成了78帧。情感标签有4个，加上空白符，得到大小为5的字符集合。真实输出只有一个标签，所以在这些长度为78的输出序列中，经过B转换后能得到一个真实情感标签的那些序列才是我们要的序列，用CTC的方法来使得这些序列产生的概率最大。</p><p>注：文本大量参考这篇文章<a href="https://www.cnblogs.com/liaohuiqiang/p/9954088.html" target="_blank" rel="noopener">论文笔记：Emotion Recognition From Speech With Recurrent Neural Networks</a>，以自己的语言整理一遍笔记，旨在巩固记忆加深理解，并非完全原创。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1701.08071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Emotion Recognition From Speech With Recurrent Neural Netw
      
    
    </summary>
    
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="RNN" scheme="http://a-kali.github.io/tags/RNN/"/>
    
  </entry>
  
</feed>
