<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-11-03T15:04:10.865Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>【面试题】IoU和mIoU</title>
    <link href="http://a-kali.github.io/2020/11/03/%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91IoU%E5%92%8CmIoU/"/>
    <id>http://a-kali.github.io/2020/11/03/【面试题】IoU和mIoU/</id>
    <published>2020-11-03T14:53:10.000Z</published>
    <updated>2020-11-03T15:04:10.865Z</updated>
    
    <content type="html"><![CDATA[<p>害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。</p><p>虽然老夫从来没做过基于检测框的目标检测项目。</p><p><strong>Q1：啥是IoU？</strong></p><p>IoU就是交并比嘛，两个框相交的面积除以合并的面积。</p><p><strong>Q2：手撕IoU。</strong></p><p>定义pred和gt为两个长度为 4 的numpy数组，用于表示两个检测框左上和右下坐标点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IoU</span><span class="params">(bbox1, bbox2)</span>:</span></span><br><span class="line">x11, y11, x12, y12 = bbox1</span><br><span class="line">    x21, y21, x22, y22 = bbox2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相交面积</span></span><br><span class="line">    iw = max(min(x12, x22) - max(x11, x21), <span class="number">0</span>)</span><br><span class="line">    iy = max(min(y12, y22) - max(y11, y21), <span class="number">0</span>)</span><br><span class="line">    inter = iw * iy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算合并面积</span></span><br><span class="line">area1 = (x12 - x11) * (y12 - y11)</span><br><span class="line">    area2 = (x22 - x21) * (y22 - y21)</span><br><span class="line">uni = area1 + area2 - inter</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = inter / uni</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>其实以上应该都只是基于常规检测框的IoU计算，如果是非矩形检测框或者分割任务中的IoU则要另当别论。</p><p>mIoU待更新</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。&lt;/p&gt;
&lt;p&gt;虽然老夫从来没做过基于检测框的目标检测项目。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q1：啥是IoU？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;IoU就是交并比嘛，两个框相交的面积除以合并的面积。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="IoU" scheme="http://a-kali.github.io/tags/IoU/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】SRN：使用语义推理网络进行场景文本识别</title>
    <link href="http://a-kali.github.io/2020/10/28/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91SRN%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%AF%AD%E4%B9%89%E6%8E%A8%E7%90%86%E7%BD%91%E7%BB%9C%E8%BF%9B%E8%A1%8C%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/10/28/【论文翻译】SRN：使用语义推理网络进行场景文本识别/</id>
    <published>2020-10-28T13:27:04.000Z</published>
    <updated>2020-10-28T13:28:07.565Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2003.12294" target="_blank" rel="noopener">Towards Accurate Scene Text Recognition with Semantic Reasoning Networks</a></p><p>Github：<a href="https://github.com/chenjun2hao/SRN.pytorch" target="_blank" rel="noopener">https://github.com/chenjun2hao/SRN.pytorch</a> （非官方）</p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>场景文本图像包含两个层次的内容：视觉纹理和语义信息</strong>。近年来，虽然已有的场景文本识别方法取得了很大的进展，但<strong>挖掘语义信息来辅助文本识别</strong>的研究却很少，只有类似RNN的结构被用来对语义信息进行隐式建模。然而，基于RNN的译码方法存在着时效性强、语义上下文单向串行传输等缺点，极大地限制了语义信息的利用和计算效率。为了缓解这些限制，我们提出了一种全新的端到端场景文本识别框架——语义推理网络(semantic reasoning network, SRN)，其中引入了一个全局语义推理模块(GSRM)，通过多路并行传输捕获全局语义。我们在常规文本、不规则文本和非拉丁文本等7个公共基准上都验证了该方法的有效性和鲁棒性。此外，相对于基于RNN的方法，SRN的速度有明显的优势，在实际应用中具有一定的价值。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>文本数据通常具有十分丰富的语义信息，这些信息已经被应用在许多计算机视觉的应用中，如自动驾驶、旅游翻译、产品检索等。场景文本识别是场景文本阅读系统的关键步骤。虽然Seq2Seq识别在过去的几十年里取得了一些显著的突破，但在现实场景中进行文本识别仍然是一个巨大的挑战，这归咎于场景文本在颜色、字体、空间布局甚至不可控的背景上都有很大的变化。</p><p>最近的大部分研究都试图从提取视觉特征的角度来提高场景文本识别的性能，如升级backbone，增加校正模块，改进注意机制等。然而对于人来说，场景文本的识别不仅依赖于视觉感知信息，还受到高层次文本语义语境理解的影响。如下图所示的一些例子，在只考虑视觉特征的情况下，很难区分这些图像中的每个字符，尤其是用红色虚线框突出的字符。相反，如果考虑到语义信息，人类很可能会根据单词的全部内容推断出正确的结果。</p><p><img src="https://i.loli.net/2020/10/26/rkAqgxsctiaTP92.png" alt="image.png"></p><p>但主流的文本识别方法对于语义信息通常采用单向串行传输的方式(RNN)，如下图(a)。这种方式有几个明显的缺点：第一，它的每个time step只能感知非常有限的语义语境；其次，当一个time step出现错误解码时，会对后面的time step产生错误累积；同时，序列模型难以并行计算，耗时且低效。</p><p><img src="https://i.loli.net/2020/10/26/RAJg5ZyfB3QKGbz.png" alt="image.png"></p><p>在本文中，我们引入了一种名为全局语义推理模块(GSRM)的子网络结构来解决这些问题。GSRM使用一种全新的多路并行传输方式将全局语义内容联系在一起。如图(b)所示，多路并行传输可以同时感知一个单词或文本行中所有字符的语义信息。单个字符的语义内容错误，对其他步骤的负面影响十分有限。</p><p>在此基础上，我们提出了一种基于语义推理网络的场景文本识别框架，该框架不仅集成了全局语义推理模块(GSRM)，还集成了并行视觉注意模块(PVAM)和视觉语义融合解码器(VSFD)。PVAM的目的是在并行注意机制中提取每个time step的视觉特征，VSFD则用于融合视觉信息和语义信息。</p><p>这篇论文的贡献主要包含三部分：首先，我们提出了一个<strong>全局语义推理模块(GSRM)</strong>来处理全局语义信息。该方法比单向串行语义传输方法具有更好的鲁棒性和有效性。其次，提出了一种新的场景文本识别框架——<strong>语义推理网络(SRN)</strong>，该框架有效地结合了视觉信息和语义信息。第三，SRN是可以端到端训练的，并在<strong>一些baseline中表现SOTA，其中包括常规文本、不规则文本和非拉丁长文本</strong>。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>不会吧不会吧，不会真有人看Related Work吧</p><h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3    Approach"></a>3    Approach</h1><p>SRN是一个端到端的可训练框架，它由四部分组成：backbone、并行视觉注意模块(PVAM)、全局语义推理模块(GSRM)、以及视觉语义融合解码器(VSFD)。对于一个给定的图像输入，首先使用backbone提取二维特征，然后使用PVAM生成$N$个对齐的一维特征$G$，其中每个特征对应于文本中的一个字符，并包含相对应的视觉信息。然后将这$N$个一维特征$G$输入到GSRM中以获取语义信息$S$。将对齐后的视觉特征$G$和语义信息$S$融合在一起，对$N$个字符进行预测。对于小于$N$的文本字符串使用’EOS’填充。SRN的详细结构如下图。</p><p><img src="https://i.loli.net/2020/10/26/73LvbolV6IqyQDH.png" alt="image.png"></p><h2 id="3-1-Backbone-Network"><a href="#3-1-Backbone-Network" class="headerlink" title="3.1    Backbone Network"></a>3.1    Backbone Network</h2><p>在backbone中，我们使用FPN来融合ResNet50的stage-3, stage-4和stage-5层的特征。因此ResNet50+FPN最终输出特征图尺寸为输入图像的$1/8$，通道数为$512$。其灵感来源于non-local mechanisms，我们还采用transformer多头注意网络和一个前馈模块捕获全局空间依赖性。将二维特征图输入到两个堆叠transformer单元中，其中多头注意力的头数为$8$，前馈输出维度为$512$。至此，我们提取出了一个（大概是）$H×W×512$的特征。</p><p><strong>总结：ResNet50 + FPN + 8 head transformer</strong></p><h2 id="3-2-Parallel-Visual-Attention-Module"><a href="#3-2-Parallel-Visual-Attention-Module" class="headerlink" title="3.2    Parallel Visual Attention Module"></a>3.2    Parallel Visual Attention Module</h2><p>注意机制在序列识别中得到了广泛的应用。它可以看作是一种特征对齐的形式，将输入中的相关信息与相应的输出进行对齐。我们使用注意机制生成$N$个特征，每个特征对应文本中的一个字符。现有的注意力方法由于存在一些时间依赖项而导致效率低下。本文提出了一种新的注意方法——<strong>平行视觉注意(PVA)</strong>，通过突破这些障碍来提高效率。</p><p>一般来说，注意机制可以描述如下：给定一个key-value集合和一个query, 计算query与所有keys的相似性。然后使values根据相似性来进行融合。在我们的研究中，key-value集合是输入的二维特征，现有的方法使用隐藏层$H_{t-1}$作为query生成第$t$个特征。为了使计算并行，我们使用读取序号作为query，而不是依赖于时间的$H_{t-1}$。文本中的第一个字符的读取序号为0，第二个字符的读取序号顺序为1，以此类推。我们的PVA可以总结为：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             e_{t,ij}=W^T_e tanh(W_of_o(O_t)+W_vv_{ij})\<br>             \alpha_{t,ij}=\frac{exp(e_{t,ij})}{\sum_{∀i,j}exp(e_{t,ij})}\<br>             \end{array}<br>\right.<br>$$<br>其中$W$均为可训练的权值。$O_t$是每个字符的读取顺序，$f_o$是embedding函数。</p><p>基于PVA的想法，我们设计了并行视觉注意模块(PVAM)用于对齐每个视觉特征和time step。对齐第$t$个time step和视觉特征的过程描述如下：<br>$$<br>g_t=\sum_{∀i,j}\alpha_{t,ij}v_{ij}<br>$$<br>由于<strong>这个计算方法具有时间无关性，PVAM可以在所有time step上并行执行对齐操作</strong>。</p><p>如图所示，所得到的注意图能够正确地注意对应字符的视觉区域，验证了PVAM的有效性。</p><p><img src="https://i.loli.net/2020/10/26/6gR1KYUsrDlXI92.png" alt="image.png"></p><h2 id="3-3-Global-Semantic-Reasoning-Module"><a href="#3-3-Global-Semantic-Reasoning-Module" class="headerlink" title="3.3    Global Semantic Reasoning Module"></a>3.3    Global Semantic Reasoning Module</h2><p>在本节中，我们提出了遵循多路并行传输思想的全局语义推理模块(GSRM)，以克服单向语义上下文传递的缺点。首先我们回顾一下典型的类RNN结构的Bahdanau注意机制中需要最大化的概率公式。可以表示为：<br>$$<br>p(y_1y_2…y_N)=\prod_{t=1}^Np(y_t|e_{t-1},H_{t-1},g_t)<br>$$<br>其中$e_t$为第$t$个label $y_t$的词嵌入。在每个time step，类RNN的方法会参考先前的labels或者预测结果。由于$e_{t-1}$、$H_{t-1}$等信息只能在time step中获取，使得这类方法的只能以序列的方式进行，限制了语义推理的能力，导致推理效率较低。</p><p>为了克服上述问题，我们使用一个时间无关的近似嵌入$e’$来代替真正的嵌入$e$。这种改进可以带来几个好处。1)首先，可以将上式中最后一步的$H_{t-1}$隐藏状态值去除，从而将串行推理过程升级为高效并行推理过程，因为所有的时间依赖项都已经被消除。2)第二，包括前后所有字符在内的全局语义信息都能用来推导当前时刻的语义状态。因此，我们将概率表达式改进如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|f_r(e_1…e_{t-1}e_{t+1}…e_N),g_t)\<br>\approx \prod^N_{t=1}p(y_t|f_r(e’<em>1…e’</em>{t-1}e’<em>{t+1}…e’_N),g_t)<br>$$<br>其中$f_r$表示用于建立全局语义和当前语义信息的函数。如果我们使$s_t=f_r(e_1…e</em>{t-1}e_{t+1}…e_N)$，$s_t$表示第$t$个语义信息的特征，上式可以简化如下：<br>$$<br>p(y_1y_2…y_N)=\prod^N_{t=1}p(y_t|s_t, g_t)<br>$$<br>于此我们提出了GSRM。该结构分为两个关键部分：视觉语义嵌入模块(Visual-to-semantic embedding block)和语义推理模块(semantic reasoning block)。</p><p><img src="https://i.loli.net/2020/10/27/SyL1OrqVf4NGWbh.png" alt="image.png"></p><p><strong>视觉语义嵌入模块</strong>用于生成$e’$，其输入特征已经经过PVAM对每个字符进行对齐。该视觉特征首先输入到一个全连接层和softmax层，并受到交叉熵损失监督。然后使用argmax选出可能性最大的字符进行embedding，得到$e’_t$。</p><p><strong>语义推理模块</strong>用于全局语义推理，相当于上上条公式里的$f_r$。多个transformer单元使模型能够高效地感知全局上下文信息，词语的语义可以通过多个transformer单元隐式建模。最后通过该模块输出每一步的语义特征，受交叉熵损失监督。</p><p>通过交叉熵损失，从语义信息的角度对客观概率进行优化，也有助于减少收敛时间。值得注意的是，在GSRM中，全局语义是并行推理的，这使得SRN比传统的基于注意力的方法运行得更快，特别是在长文本的情况下。</p><h2 id="3-4-Visual-Semantic-Fusion-Decoder"><a href="#3-4-Visual-Semantic-Fusion-Decoder" class="headerlink" title="3.4. Visual-Semantic Fusion Decoder"></a>3.4. Visual-Semantic Fusion Decoder</h2><p>在场景文本识别的同时考虑视觉对齐特征和语义信息是非常重要的。然而视觉和语义属于不同的领域，在不同的情况下，它们在最终序列识别中的权重应该是不同的。受门控单元的启发，我们引入了一些可训练的权重来平衡VSFD中不同领域的特征贡献。其操作方式如下：<br>$$<br>B_{i,j}=\left{<br>             \begin{array}{}<br>             z_t=\sigma (W_z\cdot[g_t,s_t])\<br>            f_t=z_t<em>g_t+(1-z_t)</em>s_t\<br>             \end{array}<br>\right.<br>$$<br>其中$W_z$为可训练的权值，$f_t$为第$t$次融合特征向量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2003.12294&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Towards Accurate Scene Text Recognition with Semantic Reas
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>AdaIN：基于适应性实例归一化的实时任意风格迁移</title>
    <link href="http://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    <id>http://a-kali.github.io/2020/10/12/AdaIN：基于适应性实例归一化的实时任意风格迁移/</id>
    <published>2020-10-12T15:20:11.000Z</published>
    <updated>2020-10-12T15:21:14.171Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1703.06868.pdf" target="_blank" rel="noopener">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></p><p>Github：<a href="https://github.com/xunhuang1995/AdaIN-style" target="_blank" rel="noopener">https://github.com/xunhuang1995/AdaIN-style</a></p><p>PyTorch版代码：<a href="https://github.com/naoto0804/pytorch-AdaIN" target="_blank" rel="noopener">https://github.com/naoto0804/pytorch-AdaIN</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Gatys等人最近引入了一种神经算法，可以将一幅内容(Content)图像以另一幅图像的风格(Style)呈现，实现所谓的<strong>风格迁移(Style Transfer)</strong>。然而，他们的框架需要经历一个缓慢的迭代优化过程，这限制了其实际应用。有人提出用前向神经网络进行快速逼近，以加快神经风格迁移。不幸的是，速度的提高是有代价的：网络通常与一组固定的风格绑定在一起，不能适应任意的新的风格。在本文中，我们提出了一个简单而有效的方法，首次实现了实时任意风格的传输。该方法的核心是一种新的<strong>自适应实例归一化(adaptive instance normalization, AdalN)</strong>层，它将内容特征的均值和方差与风格特征的均值和方差对齐。我们的方法达到了与现有方法相比最快的速度，并且不受预定义样式集的限制。此外，用户可以灵活调控通过这个方法训练出来的模型，如内容样式权衡、样式插值、颜色和空间控制，所有这些都仅使用一个前向神经网络。</p><p><img src="https://i.loli.net/2020/10/09/aEOytThG9LQ5q1b.png" alt="image.png"></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p><a href="http://pdfs.semanticscholar.org/3eeb/f249182614838294f7658da7e8d20c0a1917.pdf" target="_blank" rel="noopener">Gatys等人的开创性工作</a>表明，深度神经网络(DNNs)不仅编码图像的内容，而且编码图像的风格信息。此外，图像风格和内容在某种程度上是可分离的：可以在保留内容的同时改变图像的风格。他们的风格迁移方法足够灵活，可以组合任意图像的内容和样式。但是该方法的优化过程十分缓慢。</p><p>在加速神经迁移方向已经有了许多的研究。部分研究试图训练前向神经网络，通过单次前向传递来执行风格化操作。但这些方法限制了每个网络只能训练单一的风格。最近有一些研究解决了这个问题，但它们要么仍然局限于有限的风格集合，要么比单一风格的传输方法慢得多。</p><p>在这篇论文中中，我们首次提出了能够解决速度与灵活性矛盾的风格迁移算法。<strong>我们的方法可以实时对任意新的风格进行迁移，将基于优化框架的灵活性和前向方法的速度结合在一起</strong>。我们的方法灵感来源于<strong><a href="https://arxiv.org/abs/1701.02096v1" target="_blank" rel="noopener">实例归一化(instance normalization, IN)</a></strong>层，其在前向风格迁移中非常有效。IN通过对特征统计(feature statistics)进行归一化来进行风格归一化，有些研究表明特征统计携带了图像的风格信息。根据这个解释，我们引入了一个简单的IN扩展，即自适应实例归一化(AdaIN)。对于一组内容和风格，AdaIN只需调整内容输入的平均值和方差，就能匹配风格输入的平均值和方差。通过实验，我们发现AdaIN通过传递特征统计量，有效地将前者的内容与后者的风格结合起来。然后，通过将AdaIN输出反向返回到图像空间，解码网络学会生成最终的风格化图像。在实现了高速风格迁移的同时，我们的方法提供了大量的用户控件，不需要对训练过程进行任何修改。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p><strong>Style transfer.</strong> 风格迁移问题起源于非真实感渲染，与纹理合成和转换密切相关。早期的一些方法包括线性滤波器响应的直方图匹配和非参数采样。这些方法通常依赖于低级的统计信息，并且常常无法捕获语义结构。Gatys等通过匹配DNN卷积层的特征统计量，首次展示了令人印象深刻的风格迁移结果。最近，许多研究对风格迁移的算法进行了一些改进。Li和Wand在深度特征空间中引入了一种基于马尔可夫随机场的框架来加强局部模式。Gatys等人提出了控制色彩保存、空间位置的方法。以及风格迁移的规模。Ruder等人通过添加时序约束提高了视频风格迁移的质量。</p><p>Gatys等人的框架基于一个缓慢的优化过程，迭代更新图像，以最小化由内容损失和样式损失。即使是现代GPU也需要几分钟的时间，而移动应用程序中的设备处理速度更慢，难以实现。一个常见的解决方法是用训练最小化相同目标的前馈神经网络来代替优化过程。这些前馈风格的传输方法比基于优化的替代方法大约快三个数量级，为实时应用打开了大门。Wang等人的使用多分辨率架构增强了前馈式传输的粒度。Ulyanov等人提出了提高生成样本质量和多样性的方法。然而，上述前馈方法的局限性在于，每个网络都被绑在一个固定的样式上。为了解决这个问题，Dumoulin等人引入了一个能够编码32种样式及其插值的单一网络。与我们的工作同时，Li等人提出了一种前馈架构，可以合成多达300种纹理和转移16种风格。但是，上述两种方法不能适应训练中没有观察到的任意风格。</p><p>最近，Chen和Schmidt引入了一种前馈方法，借助风格交换层可以传输任意的风格。对于给定内容和风格图像的特征激活，风格交换层将以patch-by-patch的方式将内容特征替换为最匹配的风格特征。然而，他们的风格交换层创造了一个新的计算瓶颈：超过95%的计算花费在512 x 512输入图像的样式交换上。我们的方法允许任意的风格的同时，比他们的方法快1-2个数量级。</p><p>风格迁移的另一个核心问题是使用哪种风格损失函数。Gatys等人的原始框架通过匹配Gram矩阵捕获的特征激活之间的二阶统计量来匹配风格。后来也有研究提出了其它的损失函数，如MRF损失，adversarial 损失，直方图损失，CORAL损失，MMD损失，以及信道平均和方差之间的距离。注意，以上所有的损失函数都是为了匹配风格图像和合成图像之间的一些特征统计。</p><p><strong>Deep generative image modeling.</strong> 有几个图像生成框架可供选择，包括变分自动编码器、自回归模型和生成对抗网络(GANs)。值得注意的是，GANs已经取得了最令人印象深刻的视觉质量。GAN框架的各种改进已经被提出，比如条件生成、多级处理以及更好的训练目标。GANs也被应用于风格迁移和跨域图像生成。</p><h1 id="3-Background"><a href="#3-Background" class="headerlink" title="3    Background"></a>3    Background</h1><h2 id="3-1-Batch-Normalization"><a href="#3-1-Batch-Normalization" class="headerlink" title="3.1    Batch Normalization"></a>3.1    Batch Normalization</h2><p>loffe和Szegedy的开创性地引入了批归一化(BN)层，通过对特征统计进行归一化，显著地简化了前馈网络的训练。BN层的设计初衷是为了加速识别网络的训练，但后来被发现在生成图像模型中也是有效的。对于给定批处理输入x，BN对每个特征通道的均值和标准差进行归一化：<br>$$<br>BN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>其中$\gamma$和$\beta$从数据中习得，$\mu(x)$和$\sigma$是batch的均值和标准差，由每个特征通道的batch size和空间维度独立计算而得：<br>$$<br>\mu_c(x)=\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{NHW}\sum^N_{n=1}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_c(x))^2+\epsilon}<br>$$<br>BN在训练时使用mini-batch统计，在推理时使用常规的统计代替，造成了训练和推理的差异。为了解决这个问题，最近提出了批重正化，在训练期间逐步使用常规的统计数据。Li等人发现BN的另一个有趣应用：BN可以通过重新计算目标域的常规统计数据来减轻域偏移。</p><h2 id="3-2-Instance-Normalization"><a href="#3-2-Instance-Normalization" class="headerlink" title="3.2    Instance Normalization"></a>3.2    Instance Normalization</h2><p>在原始的前馈风格化方法中，风格迁移网络在每个卷积层之后包含一个BN层。令人惊讶的是，Ulyanov等发现，只需将BN层替换为IN层，就可以得到显著的改善：<br>$$<br>IN(x)=\gamma \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta<br>$$<br>和BN不同的是，IN的的$\mu(x)$和$\sigma(x)$是对每个通道和每个样本独立计算的：<br>$$<br>\mu_{nc}(x)=\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}x_{nchw}\<br>\sigma_c(x)=\sqrt{\frac{1}{HW}\sum^H_{h=1}\sum^W_{w=1}(x_{nchw}-\mu_{nc}(x))^2+\epsilon}<br>$$<br>另一个区别是，在测试时应用的层不变，而BN层通常使用常规统计代替mini-batch统计。</p><h2 id="3-3-Conditional-Instance-Normalization"><a href="#3-3-Conditional-Instance-Normalization" class="headerlink" title="3.3    Conditional Instance Normalization"></a>3.3    Conditional Instance Normalization</h2><p>Dumoulin等人没有学习单一的仿射参数集$\gamma$和$\beta$，而是提出了条件实例归一化(CIN)层，该层对每种不同的风格$s$学习不同的参数集$\gamma^s$和$\beta^s$：<br>$$<br>CIN(x;s)=\gamma^s \begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\beta^s<br>$$<br>在训练过程中，从一组固定的风格集合$s\in  {1,2,…,S}$(实验中$S = 32$)中随机选取一幅风格图像及其索引$s$。然后将内容图像输入到一个网络中，在CIN层使用对应的$\gamma^s$和$\beta^s$。令人惊讶的是，<strong>有着相同卷积参数、不同仿射参数的多个网络，可以生成完全不同风格的图像。</strong></p><p>与没有归一化层的网络相比，有CIN层的网络需要增加$2FS$的附加参数，其中$F$为网络中feature map的数量。由于附加参数的数量与样式的数量成线性关系，因此很难用这个方法生成非常多的风格。而且，每添加一种新的风格，都需要重新训练一次网络。</p><h1 id="4-Interpreting-Instance-Normalization"><a href="#4-Interpreting-Instance-Normalization" class="headerlink" title="4     Interpreting Instance Normalization"></a>4     Interpreting Instance Normalization</h1><p>尽管IN取得了巨大的成功，但它们对样式转换起作用的原因仍然是难以捉摸的。Ulyanov等人把IN的成功归于其内容图像的不变性。然而，IN发生在特征空间中，因此它应该比在像素空间中进行简单的对比归一化具有更深远的影响。也许更令人惊讶的是IN中的仿射参数可以完全改变输出图像的风格。</p><p>众所周知，DNN的卷积特征统计可以捕捉到图像的风格。Gatys等人使用二阶统计量作为其优化目标，而Li等人最近表明，包括channel-wise的均值和方差在内的其它统计量对风格迁移也是有效的。因此<strong>我们认为IN通过归一化特征统计（即均值和方差），在某种程度上执行了“风格归一化(style normalization)”。于是我们认为网络的特征统计也可以控制生成图像的风格。</strong></p><p>我们分别运行带有IN和BN层的网络来执行单一风格的转换。正如预期的那样，使用IN的模型比BN模型收敛得更快（如下图）。为了检验Ulyanov的解释，我们通过对亮度通道进行直方图均衡化，将所有训练图像归一化到相同对比度。如图(b)所示，IN仍然有效，说明Ulyanov的解释不完全。为了验证我们的假设，我们使用预训练的风格转移网络将所有的训练图像归一化为相同的风格(不同于目标风格)。从图(c)可以看出，在对图像进行了风格归一化后，IN带来的改进就小得多了。另外，使用风格归一化图像训练BN的模型和使用原始图像训练IN的模型收敛速度一样快，表明IN确实执行了一种风格归一化。</p><p><img src="https://i.loli.net/2020/09/25/BJ5SiRZen3bx2Uh.png" alt="image.png"></p><p>由于BN是在一个batch的样本上进行特征统计，可以直观地理解为将一个batch的样本围绕着单一风格进行归一化。然而每一个的样本都有不同的风格，很难将一个batch中所有样本转化成同一个风格。虽然卷积层可能会学会弥补样本之间风格的差异，但也为训练增加了难度。另一方面，IN可以将每个样本的风格归一化为目标风格，网络的其他部分可以在舍弃原有信息风格的同时专注于内容处理，提高了训练速度。CIN成功的原因也很明确：不同的仿射参数可以将特征统计值归一化到不同的值，从而将输出的图像归一化到不同的风格。</p><h1 id="5-Adaptive-Instance-Normalization"><a href="#5-Adaptive-Instance-Normalization" class="headerlink" title="5    Adaptive Instance Normalization"></a>5    Adaptive Instance Normalization</h1><p>如果将输入归一化为由仿射参数指定的单一风格，是否有可能通过自适应仿射变换使其适应任意给定的风格？我们对IN进行了一个简单的扩展。我们称之为自适应实例归一化(AdaIN)。AdaIN接收一个内容输入x和一个样式输入y，并简单地将x的通道平均值和方差与y的平均值和方差匹配。<strong>与BN、IN和CIN不同，AdaIN没有可以学习的仿射参数。相反，它自适应地从风格输入中计算仿射参数</strong>：<br>$$<br>AdaIN(x,y)=\sigma (y)\begin{pmatrix} \frac{x-\mu (x)}{\sigma (x)}\end{pmatrix}+\mu (y)<br>$$<br>相比于IN，我们仅仅是将两个仿射参数替换成了$\sigma (y)$和$\mu (y)$，这两个统计值的依然是在空间位置上进行计算。</p><p>假设存在一个检测特定风格纹路的特征通道。具有这种纹路的风格图像将在该层产生较高的平均激活值。AdaIN产生的输出在保持内容图像的空间结构的同时，对该特征具有同样高的平均激活度。纹路特征可以通过前馈解码器转换到到图像空间。该特征通道的方差可以将更细微的风格信息传递到AdaIN输出和最终输出的图像中。</p><p>简而言之，AdaIN通过迁移特征统计量，即通道方向上的均值和方差，在特征空间中进行风格迁移。</p><h1 id="6-Experimental-Setup"><a href="#6-Experimental-Setup" class="headerlink" title="6    Experimental Setup"></a>6    Experimental Setup</h1><p>如下是我们基于AdaIN的风格迁移网络的概览图：</p><p><img src="https://i.loli.net/2020/09/29/j2ZQMDuv8HtWqhk.png" alt="image.png"></p><h2 id="6-1-Architecture"><a href="#6-1-Architecture" class="headerlink" title="6.1    Architecture"></a>6.1    Architecture</h2><p>我们的风格迁移网络$T$以一个内容图像$c$和一个任意风格的图像$s$作为输入，并合成一个输出图像，该图像重新组合前者的内容和后者的样式。我们采用一种简单的encoder-decoder架构，其中encoder $f$ 固定在预训练VGG-19的前几层（直到relu4_1）。在特征空间中对内容和风格图像进行编码后，我们将这两种特征图输入AdalN层，使内容特征图的均值和方差与风格特征图的均值和方差对齐，生成目标特征图$t$：<br>$$<br>t=AdaIN(f(c),f(s))<br>$$<br>训练一个随机初始化的decoder $g$将$t$映射回图像空间，生成风格化图像$T (c, s)$：<br>$$<br>T(c,s)=g(t)<br>$$<br>decoder大部分是encoder的镜像，所有池化层替换为最近的上采样，以减少棋盘效应。我们在$f$和$g$中使用反射填充（(reflflection padding)来避免边界失真。另一个问题是decoder应该使用IN、BN还是不使用标准化层。如第4节所述，IN将每个样本归为单个样式，而BN将一批样本归一化，以单个样式为中心。当我们希望decoder生成风格迥异的图像时，两者都是不可取的。因此，我们在decoder中不使用归一化层。</p><h2 id="6-2-Training"><a href="#6-2-Training" class="headerlink" title="6.2    Training"></a>6.2    Training</h2><ul><li>Dataset<ul><li>Content: MS-COCO</li><li>Style: WikiArt</li></ul></li><li>Sample size: 80000</li><li>Optimizer: adam</li><li>Batch size: 8 content-style image pairs</li><li>Resize: 512, RandomCrop: 256×256</li><li>Model: VGG-19</li><li>Loss: $L=L_c+\lambda L_s$</li></ul><p>损失函数为内容损失和风格损失的加权和。内容损失是目标特征与输出图像特征之间的欧氏距离。我们使用AdaIN输出$t$作为内容目标，而不是内容图像：<br>$$<br>L_c=|f(g(t))-t|<em>2<br>$$<br>由于AdaIN层只迁移了风格特征的平均值和标准差，所以我们的风格损失只与这些统计数据匹配。虽然我们发现常用的Gram矩阵损失可以产生类似的结果，但我们还是使用IN统计，因为它在概念上更清晰。<br>$$<br>L_s=\sum^L</em>{i=1}|\mu (\phi_i(g(t)))-\mu(\phi_i(s))|<em>2+\sum^L</em>{i=1}|\sigma (\phi_i(g(t)))-\sigma(\phi_i(s))|_2<br>$$<br>其中$\phi$表示VGG-19中用于计算风格损失的层。在我们的实验中，我们在relu1_1, relu2_1, relu3_1,  relu4_1中使用了相等的权重。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1703.06868.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Arbitrary Style Transfer in Real-time with Adaptive In
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="风格迁移" scheme="http://a-kali.github.io/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
  </entry>
  
  <entry>
    <title>SynthText：用于文本定位的自然场景文本合成</title>
    <link href="http://a-kali.github.io/2020/09/28/SynthText%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%96%87%E6%9C%AC%E5%AE%9A%E4%BD%8D%E7%9A%84%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E5%90%88%E6%88%90/"/>
    <id>http://a-kali.github.io/2020/09/28/SynthText：用于文本定位的自然场景文本合成/</id>
    <published>2020-09-27T16:09:56.000Z</published>
    <updated>2020-09-27T16:12:36.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SynthText"><a href="#SynthText" class="headerlink" title="SynthText"></a><a href="https://github.com/ankush-me/SynthText" target="_blank" rel="noopener">SynthText</a></h1><p>论文地址：<a href="https://arxiv.org/pdf/1604.06646.pdf" target="_blank" rel="noopener">Synthetic Data for Text Localisation in Natural Images</a></p><p><img src="https://i.loli.net/2020/09/16/UVoC3yxPH8T5n4w.png" alt="image.png"></p><p>本文介绍了一种新的自然图像文本检测方法。该方法主要包括两个方面：首先，一个用于生成文本合成图片(synthetic images of text)的引擎。<strong>该引擎结合局部的三维场景几何形状，将合成文本以自然的方式叠加到现有的背景图像上。</strong>然后利用图像图像训练一个全卷积回归网络(FCRN)，在图像的任意位置多尺度地执行文本检测和边框回归。</p><p>在这里我们仅关注其生成合成图像的部分，过程如下：</p><ol><li>选择合适的文本和图像样本，根据图像局部的颜色和纹理将图像分割成连续的区域，并使用CNN进行像素级的映射；</li><li>对于每一个连续的区域，建立一个<strong>表面法线(surface normal)</strong>；</li><li>根据区域的颜色来选择文本及其轮廓的颜色；</li><li>使用随机字体渲染文本样本，并根据局部表层方向进行转换；使用泊松图像编辑(Poisson image editing)将文本混合到场景中。</li></ol><p>该生成一个场景文本图像大约需要半秒钟。项目作者建立了一个80w张生成图像的数据库：<a href="http://www.robots.ox.ac.uk/~vgg/data/scenetext/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/data/scenetext/</a></p><p><img src="https://i.loli.net/2020/09/14/qkvKnxTB3D9YCUt.png" alt="image.png"></p><h2 id="1-Text-and-Image-Sources"><a href="#1-Text-and-Image-Sources" class="headerlink" title="1    Text and Image Sources"></a>1    Text and Image Sources</h2><p>文本数据来源于Newsgroup20数据集，使用了三种提取方式：单词、句子(最多3行)和段落(最多7行)。该数据集中包含了丰富的英文语料。</p><p>为了增加多样性，作者从谷歌图像搜索中提取了8000幅背景图像。通过查询不同的物体/场景、室内/室外和自然/人造场所，这些图片自身不能包含文本。因此，搜索的时候会尽量避免携带大量文本的关键词，比如“路牌”、“菜单”等。包含文本的图像会在人工检查后丢弃。</p><h2 id="2-Segmentation-and-Geometry-Estimation"><a href="#2-Segmentation-and-Geometry-Estimation" class="headerlink" title="2    Segmentation and Geometry Estimation"></a>2    Segmentation and Geometry Estimation</h2><p>在真实场景中，文本往往包含在明确定义的区域中（比如一个指示牌）。本文提出的方法将文本约束在统一颜色和纹理的区域，可以防止文本跨越强图像不连续点。将gPb-UCM轮廓分层的阈值设定在0.11，通过图切割(graph-cut)获得区域。下图显示了对图像颜色和纹理敏感（左图）和直接将文本置于图像（右图）的区别。</p><p><img src="https://i.loli.net/2020/09/16/y4GfIBwsHAn9ZhC.png" alt="image.png"></p><p>在自然图像中，文本往往在物体表层的顶部(例如一个路牌或一个杯子）。为了使合成数据中也有类似的效果，作者根据局部表面法线对文本进行了详细的变换：</p><ol><li>首先通过<a href="https://arxiv.org/abs/1411.6387v1" target="_blank" rel="noopener">特定的CNN</a>对上面分割的区域预测一个深度图，然后使用<a href="https://xueshu.baidu.com/usercenter/paper/show?paperid=44efa35fada9e7afa5fa46da356fabbc&site=xueshu_se" target="_blank" rel="noopener">RANSAC</a>拟合一个平面来自动估算出一条法线；</li><li>利用估算出的平面法线将图像区域轮廓弯曲成平行面视图，将矩形拟合到额平行(fronto-parallel)区域</li><li>文本与矩形的宽对齐。当在同一区域放置多个文本实例时，检查文本mask是否相互冲突，避免叠加。</li></ol><p>并不是所有的分割区域都适合放置文本，比如区域太小、极端高宽比、或表面法向正交于视角方向的区域，这些区域都在这个阶段被过滤。此外，纹理过多的区域也被过滤，其中纹理的复杂度是由RGB图像的三阶导数的强度来衡量的。</p><h2 id="3-Text-Rendering-and-Image-Composition"><a href="#3-Text-Rendering-and-Image-Composition" class="headerlink" title="3    Text Rendering and Image Composition"></a>3    Text Rendering and Image Composition</h2><p>确定了文本的位置和方向之后，下一步是给文本上色。文本的调色板是从 IIIT5K单词数据集裁剪的单词图像中学习的。使用K-means将裁剪后的词图像的像素分割成两个集合，分别为前景（文本）和背景。在渲染新文本时，背景颜色选择与目标图像区域最匹配的颜色对(在Lab颜色空间中使用L2-norm)，并使用相应的前景色渲染文本。</p><p>随机选择大约20%的文本实例加上边框，边框颜色与前景颜色接近，或者被设为前景和背景颜色的平均值。</p><p>为了保持合成文本图像中的光照梯度(illumination gradient)，使用Poisson图像编辑将文本混合到基础图像上。</p><p>顺带再提两个比较新的文本合项目：UnrealText和SynthText3D。</p><h1 id="SynthText3D"><a href="#SynthText3D" class="headerlink" title="SynthText3D"></a><a href="https://github.com/MhLiao/SynthText3D" target="_blank" rel="noopener">SynthText3D</a></h1><p>论文地址：<a href="https://arxiv.org/abs/1907.06007" target="_blank" rel="noopener">SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds</a></p><p><img src="https://i.loli.net/2020/09/16/cvp7d8fTrCAGtby.png" alt="image.png"></p><p>本文提出从三维虚拟世界中合成场景文本图像，该方法提供了精确的场景描述、可编辑的亮度/能见度和真实的物理现象。与之前的方法不同的是，该方法可以将三维虚拟场景和文本实例作为一个整体进行渲染。该方法合成场景文本图像中能够呈现真实世界的变化，包括复杂的透视变换、光照、遮挡。此外，通过对虚拟摄像机进行随机移动和旋转，可以对同一个文本生成不同视点的实例。</p><h1 id="UnrealText"><a href="#UnrealText" class="headerlink" title="UnrealText"></a><a href="https://github.com/Jyouhou/UnrealText" target="_blank" rel="noopener">UnrealText</a></h1><p>论文地址：<a href="https://arxiv.org/abs/1907.06007" target="_blank" rel="noopener">UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World</a></p><p><img src="https://i.loli.net/2020/09/17/4vNfpg7Fte38Rwo.png" alt="image.png"></p><p>UnrealText同样是通过三维图形引擎生成逼近真实的图像。</p><p><img src="https://i.loli.net/2020/09/17/kAzgoTn9KlfeB4m.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SynthText&quot;&gt;&lt;a href=&quot;#SynthText&quot; class=&quot;headerlink&quot; title=&quot;SynthText&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/ankush-me/SynthText&quot; target=&quot;_b
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="文本合成" scheme="http://a-kali.github.io/tags/%E6%96%87%E6%9C%AC%E5%90%88%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>Mask TextSpotter v3：基于分割候选框的场景文本识别</title>
    <link href="http://a-kali.github.io/2020/09/03/Mask-TextSpotter-v3%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%88%86%E5%89%B2%E5%80%99%E9%80%89%E6%A1%86%E7%9A%84%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/09/03/Mask-TextSpotter-v3：基于分割候选框的场景文本识别/</id>
    <published>2020-09-03T14:21:50.000Z</published>
    <updated>2020-10-28T13:32:16.858Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2007.09482" target="_blank" rel="noopener">Mask TextSpotter v3: Segmentation Proposal Network for Robust Scene Text Spotting</a></p><p>Github：<a href="https://github.com/MhLiao/MaskTextSpotterV3" target="_blank" rel="noopener">https://github.com/MhLiao/MaskTextSpotterV3</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>最近检测与识别相结合的场景文本识别端到端模型取得了很大进展。然而，目前的任意形状场景文本识别模型大多使用RPN来生成候选框，而RPN严重依赖于手工设计的轴对称矩形anchor。这使得处理高宽比或不规则形状的文本实例时存在困难，处理密集文本时单个候选框中容易包含多个相邻实例。为了解决这些问题，我们提出了Mask TextSpotter v3，一种端到端的场景文本识别器，其<strong>采用SPN(Segmentation Proposal Network)代替RPN。我们的SPN是anchor-free的，能够精确表示任意形状的候选区域</strong>。因此，我们的Mask TextSpotter v3可以处理极端宽高比或不规则形状的文本实例，而且它的识别精度不会受到附近文本或背景噪音的影响。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在现实中阅读文本是一项非常重要的技术，有着广泛的应用，包括照片OCR，菜单阅读，地理定位等。系统针对该任务的设计一般包括文本检测和识别两个模块，其中文本检测的目标是对文本实例及其边界框进行定位，而文本识别的目标是将检测到的文本区域转换为一系列标签进行字符识别。场景文本识别/端到端识别是一个结合了这两种模块的任务，既需要检测又需要识别。</p><p>场景文本阅读的挑战主要在于场景文本实例的不同方位、宽高比和形状。因此，旋转鲁棒性，宽高比鲁棒性和形状鲁棒性在场景文本检测任务中是非常重要的。因为文本通常不会沿着图像的轴线对其，所以<strong>旋转鲁棒性</strong>非常重要。<strong>高宽比鲁棒性</strong>对于非拉丁文本尤其重要，因为这些文本通常表现为很长的文本行，而不是一个个单词。<strong>形状鲁棒性</strong>对于处理不规则形状的文本是必要的，不规则形状经常出现在logo中。</p><p>RPN的局限性主要表现在两个方面：(1)手工预先设计的锚点是使用轴对称的矩形来定义的，不易匹配高长比极端的文本实例。(2)文本实例密集时，生成矩形候选框可能包含多个相邻文本实例。如图a所示，由Mask TextSpotter v2产生的候选框相互重叠，其RoI特征包含多个相邻文本实例，导致检测和识别错误。</p><p><img src="https://i.loli.net/2020/09/02/J93otX4vVBPydT7.png" alt="image.png"></p><p>在本文中，我们提出了SPN，旨在解决RPN-based方法的局限性。<strong>SPN是anchor-free的，并给出了精确的多边形候选框，不用预先设计anchor。同时我们提出hard RoI masking应用到RoI特征中，可以抑制邻近的文本实例或背景噪声，从而充分利用该方法的精确性。</strong>我们的实验表明，Mask TextSpotter v3显著提高了对旋转、宽高比和形状的鲁棒性。在旋转的ICDAR 2013数据集上，图像以不同角度旋转，我们的方法在端到端检测和识别方面都超过了目前最先进的21.9%。在端到端识别任务中，我们的方法超出最先进方法5.9%。我们的方法还在具有极端高长宽比的文本行标记的MSRA-TD500数据集，以及包含许多低分辨率小文本实例的ICDAR 2015数据集上达到了最高水准。我们的贡献主要包括三方面：</p><ol><li>我们提出的<strong>SPN能够精确表示任意形状的候选框</strong>。Anchor-free SPN克服了RPN在处理极端长宽比或不规则形状文本时的局限性，并提供了更精确的方案来提高识别的鲁棒性。据我们所知，它是首个用于端到端可训练文本定位的任意形状建议生成器。</li><li>我们提出了 <strong>hard RoI masking 将多边形候选框应用于RoI特征</strong>，有效地抑制背景噪声或相邻的文本实例。</li><li>我们提出的Mask TextSpotter v3显著提高了对旋转、宽高比和形状的鲁棒性，在几个具有挑战性的场景文本测试中取得了最先进的结果。</li></ol><h1 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2    Methodology"></a>2    Methodology</h1><p>Mask TextSpotter v3使用ResNet-50作为backbone，SPN生成候选框，一个Fast R-CNN模块微调候选框，一个文本实例分割模块用来精确检测，一个字符分割模块和一个空间注意力模块用来识别。Mask TextSpotter v3的pipeline如图所示。</p><p><img src="https://i.loli.net/2020/09/02/DJFe94Ckndgp6QN.png" alt="image.png"></p><h2 id="2-1-Segmentation-proposal-network"><a href="#2-1-Segmentation-proposal-network" class="headerlink" title="2.1     Segmentation proposal network"></a>2.1     Segmentation proposal network</h2><p>如上图所示，我们提出的<strong>SPN采用U-Net结构使其对尺度具有鲁棒性</strong>。和基于FPN多个阶段产生不同尺度的候选框的RPN不同，<strong>SPN从分割mask生成候选框，并且使用由多个感受野产生的特征图融合而成的特征图进行预测</strong>，该特征图长宽为输入图像的1/4。分割预测模块配置见文末补充。分割模型输出的掩码概率图尺寸和输入图像相同，通道数为1，取值范围为[0,1]。</p><p><strong>Segmentation label generation.</strong> 为了分离邻近的文本实例，基于分割的场景文本检测器常用的方法是缩小文本区域。受DB模型的启发，我们采用Vatti裁剪算法，通过裁剪$d$个像素来缩小文本区域。偏移像素d可以确定为$d= A(1-r^2)/L$，其中$A$和$L$分别为文本域多边形的面积和周长，$r$为收缩比，通常设为0.4。生成label的例子如下图所示：</p><p><img src="https://i.loli.net/2020/09/02/l9RejYTKfJu7SP6.png" alt="image.png"></p><p><strong>Proposal generation.</strong> 对于一个给定的值域在[0,1]之间的分割图，我们通过阈值对其进行二值化。对掩码$B$中连通的区域进行分组，这些连通区域可以被认为是缩小后的文本区域。因此，我们使用Vatti裁剪算法扩大它们$\hat d$个像素，其中$\hat d$计算为$\hat d=\hat A×\hat r / \hat L$。其中，$\hat A$和$\hat L$是缩小后文本区域的面积和周长。根据收缩比$r$的值，我们将$\hat r$设置为3.0。</p><p>如上所述，SPN产生的候选框可以精确地表示多边形文本区域。因此，SPN具有生成极端长宽比/密集/不规则形状的文本实例候选框的能力。</p><p><strong>（概括：分割→缩小文本域→二值化→将连通的像素划分为一个文本域→恢复文本域大小）</strong></p><h2 id="2-2-Hard-RoI-masking"><a href="#2-2-Hard-RoI-masking" class="headerlink" title="2.2    Hard RoI masking"></a>2.2    Hard RoI masking</h2><p>由于自定义RoI Align操作只支持轴对称的矩形bounding boxes，因此我们使用多边形候选框中的最小的、轴对称的矩形bounding boxes来生成RoI特征，以保持RoI Align操作。</p><p>Qin等人提出了RoI masking，将掩码概率图与RoI特征相乘，其中掩码概率图由Mask R-CNN检测模块生成。然而，掩码概率图可能不准确，因为其基于RPN的候选框进行预测，一个候选框中可能包含多个密集相邻的文本。而我们为候选框设计了精确的多边形表示。因此，我们提出了 hard RoI masking 直接将这些候选框应用到RoI特征上。</p><p><strong>Hard RoI masking将二值多边形掩码与RoI特征相乘，以抑制背景噪声或邻近文本实例</strong>，大大降低了检测和识别模块的执行难度和出错概率。</p><h2 id="2-3-Detection-and-recognition"><a href="#2-3-Detection-and-recognition" class="headerlink" title="2.3    Detection and recognition"></a>2.3    Detection and recognition</h2><p>我们遵循Mask TextSpotter v2的文本检测和识别模块的主要设计，原因如下:(1)Mask TextSpotter v2是目前最先进的、具有竞争力的检测和识别模块。(2)由于Mask TextSpotter v2是基于RPN的场景文本检测中一种具有代表性的方法，我们可以控制变量，来验证我们所提出的SPN的有效性和鲁棒性。</p><p>检测时，将Hard RoI masking生成的masked RoI特征输入到Fast R-CNN模块以进一步细化定位，并使用文本实例分割模块进行精确分割。采用字符分割模块和空间注意力模块进行识别。</p><h2 id="2-4-Optimization"><a href="#2-4-Optimization" class="headerlink" title="2.4    Optimization"></a>2.4    Optimization</h2><p>损失函数$L$定义如下：<br>$$<br>L=L_s+\alpha_1L_{rcnn}+\alpha_2L_{mask}.<br>$$<br>$L_{rcnn}$和$ L_{mask}$分别在Fast R-CNN和Mask TextSpotter v2中有过明确定义。$L_{mask}$由文本分割损失、字符分割损失、空间注意力解码器损失组成。$L_s$表示SPN的损失。根据Mask TextSpotter v2，我们将$\alpha_1$和$\alpha_2$设为1.0。</p><p>我们采用dice loss计算SPN的损失。设$S$和$G$分别为分割结果mask和目标mask，则$L_s$可由如下公式计算：<br>$$<br>L_s=1-\frac{2.0×\sum(S*G)}{\sum S + \sum G}<br>$$<br>（大概意思就是两个mask重合率越高，损失越小）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2007.09482&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Mask TextSpotter v3: Segmentation Proposal Network for Rob
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本识别" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>【论文翻译】MoCo：用于无监督视觉表示学习的动量对比</title>
    <link href="http://a-kali.github.io/2020/09/03/%E3%80%90%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91MoCo%EF%BC%9A%E7%94%A8%E4%BA%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E8%A7%86%E8%A7%89%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8A%A8%E9%87%8F%E5%AF%B9%E6%AF%94/"/>
    <id>http://a-kali.github.io/2020/09/03/【论文翻译】MoCo：用于无监督视觉表示学习的动量对比/</id>
    <published>2020-09-03T14:04:54.000Z</published>
    <updated>2020-09-27T16:22:31.810Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1911.05722.pdf" target="_blank" rel="noopener">Momentum Contrast for Unsupervised Visual Representation Learning</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>我们提出了动量对比(Momentum Contrast, MoCo)的无监督视觉表示学习。从基于字典查找的对比学习(contrastive learning)的角度出发，我们构建了一个带有队列和移动平均编码器的动态字典：这使我们能够实时构建一个大型的、一致的字典，从而促进非监督对比学习。MoCo在ImageNet分类任务中表现优异。更重要的是，MoCo学到的表示可以很好地应用到<strong>下游任务(downstream task)</strong>中。MoCo可以在PASCAL VOC、COCO等7个数据集的检测/分割任务中超过了其监督学习的预训练模型。这表明，在许多视觉任务中，无监督和监督学习之间的差距已经很大程度上缩小了。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>无监督表示学习在NLP中的应用非常成功，如：GPT、BERT。但在CV领域，有监督的预训练仍占主导地位，而无监督的方法则普遍落后。原因可能是由于它们所属的信号空间不同。语言任务具有离散的信号空间(单词、子单词单元等)用于构建标记化词典( tokenized dictionaries)，在此基础上进行无监督学习。相反，CV更加关注字典构建，因为原始信号处于连续的高维空间。</p><p>最近的一些研究提出了使用<strong>对比损失(contrastive loss)</strong>的无监督视觉表示学习，得出了令人激动的结果。这些方法可以看作是<strong>构建动态词典(dynamic dictionaries)</strong>。数据样本通过一个encoder提取特征后得到字典中的“密钥”(keys)。无监督学习训练encoder来执行字典查询过程：一个“查询”(query)通过encoder后，得到的输出应该与其匹配的key值相近，而与其他样本的key尽可能不同。学习目的为最小化contrastive loss。</p><p>从这个角度来看，我们认为应该在训练过程中逐步建立满足以下条件的词典：<strong>大型&amp;一致</strong>。直观地说，一个<strong>更大的字典</strong>可以更好地对底层连续的、高维的视觉空间进行采样；而字典中的<strong>key应该用相同或类似的encoder表示，以便它们与query的比较是一致</strong>的。然而，使用现有的contrastive loss方法可能局限于这两个方面中的一个(稍后在下文中讨论)。</p><p>我们将动量对比(MoCo)作为一种构建大型且一致的字典的方式，用于非监督学习。我们将字典当作数据样本的队列：每当一个新的batch完成编码后进入队列，将最老的编码移出队列。队列将字典大小与batch大小解耦，如此一来就能独立建立字典（而不是依赖于batch大小），允许字典的大小比batch大很多。此外，由于字典的key来自前几个batch，为了保持一致性，我们提出了一种基于动量的encoder。</p><p><img src="https://i.loli.net/2020/08/28/cuYKIpgwVQS38v6.png" alt="image.png"></p><p>MoCo是一种建立动态对比学习词典的机制，可以与各种各样的<strong><a href="https://blog.csdn.net/u013303408/article/details/103657043" target="_blank" rel="noopener">前置任务(pretext task)</a></strong>一起使用。在本文中，我们使用一个简单的识别任务进行讲解：如果它们是同一图像(例如同一张图片的不同裁剪区域)的编码，那么query将给它们匹配同一个key。利用这个前置任务，MoCo在ImageNet数据集中展现了有力的结果。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>无监督/自监督学习方法一般包括两个方面：前置任务和损失函数。“前置”意味着正在解决的任务并不是我们最终要解决的任务，而是为了更好地学习数据的表示。损失函数通常可以独立于前置任务进行研究。MoCo专注于损失函数方面。接下来我们就这两个方面的相关研究进行讨论。</p><p><strong>损失函数</strong>。损失函数是一种用于衡量模型的预测和固定目标值之间差距的方式。其中，对比损失(Contrastive losses)用于衡量两个样本在一个空间中的相似性。相比于将输入匹配到一个固定的目标，对比损失公式的目标可以在训练期间变化。对比学习是最近几篇关于无监督学习的研究的核心内容，我们将在后面对此进行阐述。</p><p><strong>前置任务</strong>。前置任务的种类多种多样。例如修复遭到损坏的输入，包括：去噪自动编码器( denoising auto-encoders)，上下文自动编码器(context autoencoders)，或是用于着色的跨通道自动编码器(cross-channel auto-encoders)；还有一些前置任务用于形成伪标签，例如，对图像进行转换（数据增强）、patch排序、视频目标的跟踪、分割、聚类。</p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3    Method"></a>3    Method</h1><h2 id="3-1-Contrastive-Learning-as-Dictionary-Look-up"><a href="#3-1-Contrastive-Learning-as-Dictionary-Look-up" class="headerlink" title="3.1 Contrastive Learning as Dictionary Look-up"></a>3.1 Contrastive Learning as Dictionary Look-up</h2><p>对比学习(Contrastive learning)可以被看作是训练一个encoder进行字典查找任务。</p><p>假设已有一个已编码的查询$q$，和一组已编码的样本（即字典中的keys）：${k_0, k_1,  k_2, …}$，$q$匹配到了字典中的一个key(表示为k)。$q$和其对 应的$k_+$越相似且与其它的keys相差越大，对比损失函数值越小。一种和点积度量相似，被称为<strong>InfoNCE</strong>的对比损失函数在本文中被使用：<br>$$<br>L_q=-log \frac{exp(q\cdot k_+/\tau)}{\sum^K_{i=0}exp(q\cdot k_i/\tau)}<br>$$<br>其中$\tau$是一个超参数，sum运算的对象是1个正样本和K个负样本。直观地说，这种损失是基于K+1类softmax分类器的对数损失，用于将$q$分类为$k_+$类中的其中一类。对比损失函数也可以基于其他形式，如margin-based losses和NCE loss的变体。</p><p>对比损失函数作为无监督的目标函数，用于训练encoder。一般来说，query表示为$q = f_q(x^q)$（同理，$k=f_k(x^k)$），其中$f_a$是一个encoder，$x^q$是一个query样本。它们的实例化，取决于特定的前置任务。输入$x^q$和$x^k$可以是图像、patch、或者是由一组patch组成的context。网络$f_q$和$f_k$可以是相同的、部分共享的、或是完全不同的。</p><p><img src="https://i.loli.net/2020/08/31/uPQV7TlifsoB1Dy.png" alt="image.png"></p><p>上图是<strong>三种对比损失机制的概念上的比较</strong>。这里我们会解释对于一对query和key，这三种机制在如何维护keys和如何更新keys encoder方面有所不同。(a):计算query和key的encoder通过反向传播端到端更新，这两个encoder可以是不同的。(b):密钥表示从存储库中采样。(c): MoCo通过动量更新encoder对新的keys进行动态编码。并维护keys的队列(图中没有说明)。</p><h2 id="3-2-Momentum-Contrast"><a href="#3-2-Momentum-Contrast" class="headerlink" title="3.2 Momentum Contrast"></a>3.2 Momentum Contrast</h2><p>从上面的观点来看，对比学习是一种在高维连续输入(如图像)上构建离散字典的方法。这个字典是动态的，因为键是随机采样的，而且key encoder在训练过程中会进化。我们的假设是，好的特征可以通过包含大量负样本的大字典来学习。而字典key的encoder则尽可能保持一致，不管它的发展。基于这个动机，我们现在的动量对比描述如下。</p><p><strong>字典元素队列(Dictionary as a queue)</strong>。我们方法的核心是将字典作为一个数据样本队列来维护，这允许我们重用实时的keys。<strong>队列的引入将字典大小与batch大小解耦，所以字典大小可以比batch大小大得多。</strong>并且可以将字典的大小独立地设置为超参数。<br>当前batch被编入字典时，队列中最老的batch被删除。由于字典表示所有数据的抽样子集，因此维护字典的额外计算量是可控的。此外，<strong>删除最老的batch能够有效地保持一致性，因为其编码的keys是最过时的，与最新的keys最不一致。</strong></p><p><strong>动量更新(Momentum update)</strong>。使用队列可能会使字典变大，但它也使得通过反向传播(梯度应该传播到队列中的所有样本)来更新key encoder变得棘手。一个简单的解决方案是从query encoder$f_q$复制key encoder$f_k$，忽略这个梯度。但是这种解决方案在实验中产生的结果很差。我们假设这种故障是由于encoder的快速变化，降低了key的一致性造成的。于是我们提出了动量更新来解决这个问题。<br>形式上，我们将参数$f_k$、$f_q$指定为$\theta_k$、$\theta_q$，并通过如下公式更新$\theta_k$：<br>$$<br>\theta_k \leftarrow m\theta_k+(1-m)\theta_q<br>$$<br>在这个公式中，$m\in [0,1)$为动量系数。只有$\theta_q$通过反向传播更新。动量更新公式使得$\theta_k$能够比$\theta_q$更加顺滑地进化。<strong>因此，虽然字典队列中不同batch的keys是由不同的encoder编码出来的，但编码器之间的差别很小。</strong>实验中通常使用相关性较大的动量(0.999)效果会比小动量(0.9)来带的效果好很多，这表明缓慢进化的key encoder是使得队列起作用的核心。</p><p>略过一些不是很重要的部分。</p><h2 id="3-3-Pretext-Task"><a href="#3-3-Pretext-Task" class="headerlink" title="3.3 Pretext Task"></a>3.3 Pretext Task</h2><p>对比学习可以驱动各种各样的前置任务。由于本文的重点不是设计一个新的前置任务，所以我们使用了一个简单的前置任务。</p><p>我们将来自同一张图像的query和key视为正样本对，否则视为负样本对。于是我们对同一幅图像进行随机数据增强生成两个新图像，形成正样本对。query和key分别由其各自的encoder进行编码，encoder可以是任意卷积神经网络。</p><p><img src="https://i.loli.net/2020/08/31/v9jVwGRBfrMmHni.png" alt="image.png"></p><p>上图中的算法用来给前置任务生成伪标签。</p><p><strong>Technical details.</strong> 我们采用最后全局平均池化后面的全连接层的ResNet作为encoder，其具有固定维度的输出(128-D)。该输出向量使用L2-norm归一化，得到query或key的表示。动量公式中的温度$\tau$设为0.07。数据增强设置如下：随机resize后的图像中crop一个分辨率224×224的子图，然后经过随机颜色抖动(random color jittering)、随机的水平翻转(random horizontal flop)和随机的灰度转换(random grayscale conversion)，这些都可以在PyTorch的torchvision包中导入使用。</p><p><strong>Shuffling BN.</strong> 在实验中，我们发现使用BN对模型的训练有不利的影响。模型使用BN后会在前置任务中作弊，迅速找到低loss的解决方案。这可能是因为<strong>batch内的样本之间互相通信(由BN引起)，泄露了信息</strong>。<br>于是我们提出了Shuffling BN来解决这个问题。由于使用多个GPU进行训练，每个GPU独立地对样本执行BN。对于key encoder，我们先对当前batch中的样本顺序进行洗牌，然后再将其分配给GPU进行编码，将输出的编码恢复原本的顺序；而query encoder的样本顺序不变。这确保用于计算query及positive key的batch统计信息来自两个不同的子集，有效地解决了作弊的问题，并允许训练受益于BN。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://www.cnblogs.com/gaopursuit/p/12242946.html" target="_blank" rel="noopener">对比自监督学习</a><br>[2]<a href="https://zhuanlan.zhihu.com/p/140908341" target="_blank" rel="noopener">CVPR 2020 | MoCo自监督学习或成为CV领域的启明灯</a><br>[3]<a href="https://www.cnblogs.com/xytpai/p/12575735.html" target="_blank" rel="noopener">何凯明组自监督方法MoCo开颅（1）</a><br>[4]<a href="https://blog.csdn.net/u013303408/article/details/103657043" target="_blank" rel="noopener">Self-Supervised Learning 自监督学习中Pretext task的理解</a><br>[5]<a href="https://www.thepaper.cn/newsDetail_forward_5575423" target="_blank" rel="noopener">数据太少怎么办？试试自监督学习</a><br>[6]<a href="https://zhuanlan.zhihu.com/p/150224914?utm_source=wechat_session&utm_medium=social&utm_oi=73454221000704" target="_blank" rel="noopener">自监督学习的一些思考</a>（入门向，推荐）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1911.05722.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Momentum Contrast for Unsupervised Visual Representati
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
      <category term="无监督学习" scheme="http://a-kali.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MoCo" scheme="http://a-kali.github.io/tags/MoCo/"/>
    
  </entry>
  
  <entry>
    <title>Hourglass &amp; CornerNet &amp; CenterNet</title>
    <link href="http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/"/>
    <id>http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/</id>
    <published>2020-08-10T16:26:01.000Z</published>
    <updated>2020-08-13T15:10:29.156Z</updated>
    
    <content type="html"><![CDATA[<p>最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。</p><h1 id="Hourglass"><a href="#Hourglass" class="headerlink" title="Hourglass"></a>Hourglass</h1><p>论文链接：<a href="https://arxiv.org/abs/1603.06937" target="_blank" rel="noopener">Stacked Hourglass Networks for Human Pose Estimation</a></p><p>Stacked Hourglass Neworks（以下简称Hourglass）由多个Hourglass模块堆叠而成，其模块对特征图进行下采样后，将特征图上采样到原来的大小，形似沙漏，故名Hourglass。</p><p>Hourglass原本是用于做<strong>人体姿态估计（Human pose estimation）</strong>，其中比较关键的一步就是检测人体上的关键点。人体不同的部位特征不同，所需要的特征图大小也不一样。Hourglass网络能够处理各种尺寸的人体特征，以此来捕捉人体各部位之间的空间关系。</p><p><img src="https://i.loli.net/2020/08/07/YBgedQokvwPlhHV.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/10/hvftgsOw6VFiXIE.png" alt="image.png"></p><p>每个Hourglass模块采用encoder-decoder结构，对输入图下采样提取特征后进行上采样，输出原图大小的heatmap作为关键点的标记。encoder和decoder之间使用残差结构融合前后特征。</p><p><img src="https://i.loli.net/2020/08/11/i2rt3GMlKbUakPN.png" alt="image.png"></p><p>该网络的思想和结构后被其它检测网络广泛采用。</p><h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="noopener">CornerNet: Detecting Objects as Paired Keypoints</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="noopener">https://github.com/umich-vl/CornerNet</a></p><p>CornerNet是一个通过检测对角来进行目标检测的网络。主要原理是采用Hourglass作为基本结构，输出检测框对角的Heatmap。</p><p>这个设计真的很<del>反人类</del>反人工智能，因为对角上应该是没有什么特征的。但是该网络的设计思想非常有意思，给后来的Anchor Free类型的目标检测网络提供了思路。</p><p><img src="https://i.loli.net/2020/08/10/GrVdznPJ1qT9pw5.png" alt="image.png"></p><p>CornerNet的整体结构如下图所示，其骨干部分使用Hourglass，输出部分有两个分支模块，分别表示<strong>左上角点预测分支</strong>和<strong>右下角点预测分支</strong>，每个分支模块包含一个<strong>corner pooling</strong>层和3个输出：<strong>heatmaps、embeddings和offsets</strong>。</p><p><img src="https://i.loli.net/2020/08/10/gDtnTWBj3PkXozR.png" alt="image.png"></p><h2 id="1-输出端"><a href="#1-输出端" class="headerlink" title="1    输出端"></a>1    输出端</h2><ul><li><p><strong>Heatmaps</strong>对角点的位置进行预测，最有可能是角点的位置输出值越高。其gt是基于角点的高斯值；</p></li><li><p><strong>Offset</strong>输出取整计算时丢失的精度信息（感觉用处不是很大）；</p><p><img src="https://i.loli.net/2020/08/11/cpO1Ueq6WPazGK3.png" alt="image.png"></p></li><li><p><strong>Embedding</strong>用来对左上角点和右下角点进行匹配。其输出一个vector，当两个角点的vector距离较小时，则认为这两点为成对点（这个和人脸匹配有点像）。这部分由两个损失函数实现，第一部分用来缩小成对点向量的距离，第二部分用来放大非成对点向量之间的距离。</p><p><img src="https://i.loli.net/2020/08/11/WBEQnLpIN2qCHx3.png" alt="image.png"></p></li></ul><h2 id="2-Corner-Pooling"><a href="#2-Corner-Pooling" class="headerlink" title="2    Corner Pooling"></a>2    Corner Pooling</h2><p>因为CornerNet是预测左上角和右下角两个角点，但是这两个角点在不同目标上没有相同规律可循，如果采用普通池化操作，那么在训练预测角点支路时会比较困难。考虑到对于每一个左上角点，其所有的特征都在其右下方。因此如果左上角角点经过池化操作后能有其右下方的信息，那么就有利于该点的预测。右下角点同理。</p><p>Corner Pooling便是一种对某个像素的右下方（或左上方）的所有像素进行池化的操作。</p><p><img src="https://i.loli.net/2020/08/11/2lW4Oi1k6qhzF3f.png" alt="image.png"></p><h1 id="CenterNet"><a href="#CenterNet" class="headerlink" title="CenterNet"></a>CenterNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a><br>Github：<a href="https://github.com/xingyizhou/CenterNet" target="_blank" rel="noopener">https://github.com/xingyizhou/CenterNet</a></p><p>CenterNet有两篇发布时间十分接近的两篇论文，这里我们讲其中开源项目Star比较多、受认可度比较高的<a href="https://arxiv.org/abs/1904.07850" target="_blank" rel="noopener">Objects as Points</a>。</p><p>单从网络的结构和损失函数来看，这个模型是非常符合当代神经网络设计思想的模型：设定简约，输入输出端到端，且能够适应多种任务。堪称神经网络中的艺术品。</p><h2 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h2><p>在普通的<strong>目标检测</strong>任务中，CenterNet的思想和CornerNet十分接近。只不过CornerNet输出的是边框两个角点的坐标，而CenterNet输出的是<strong>中心点的坐标及其到边框的距离</strong>。</p><p><img src="https://i.loli.net/2020/08/12/WhrlemckSsBZEpH.png" alt="image.png"></p><p>CenterNet还能用于<strong>3D目标检测(3D object detection)</strong>和<strong>多人人体姿态估计(multi-person human pose estimation)</strong>。在3D目标检测任务中，网络预测目标中心点的位置和深度、3D检测框的长宽以及目标的方向；在多人姿态估计任务中，网络预测中心点的位置以及各个关键的对中心点的偏置(offset)。</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2    Related work"></a>2    Related work</h2><p>CenterNet的改变和优势：</p><ul><li><p>在<strong>目标检测</strong>任务中，相比于RCNN系列网络，没有使用RPN、Anchor、NMS技术，没有使用阈值做前后景分类。仅仅提取特征图上的局部峰值点作为中心；</p></li><li><p>在<strong>通过关键点做目标检测</strong>的任务中，相比于CornerNet等网络，不需要对关键点进行配对操作；</p></li><li><p>在<strong>单目3D目标检测</strong>任务中，相比于Deep3Dbox等网络，同样更加简洁快速。</p><p><img src="https://i.loli.net/2020/08/13/svJWj2Np7aSlu3o.png" alt="image.png"></p></li></ul><h2 id="3-Method"><a href="#3-Method" class="headerlink" title="3     Method"></a>3     Method</h2><p>这部分和CornerNet差不多，主要讲了下输出gt的格式，公式比较多，结合源码看比较容易懂。</p><p>挑几个比较有看点的：</p><ol><li><p>在中心点位置生成<strong>和目标大小相同的高斯核</strong>，得到heatmap。而CornerNet仅仅在对角生成固定大小的高斯核；</p></li><li><p>损失函数是针对CenterNet的性质根据FocalLoss改进的<strong>像素级惩罚衰减逻辑回归FocalLoss(penalty-reduced pixel-wise logistic regression with focal loss)</strong>，中文是我瞎翻译的，感觉这么长的名字可以用来当日本轻小说的题目了。</p><p><img src="https://i.loli.net/2020/08/13/K2UojbPhNxGIVs3.png" alt="image.png"></p><p>其中的N是预测出的关键点的数量，剩下的都是FocalLoss里的超参数。其实就是像素级的FocalLoss没啥区别。</p></li><li><p>值得一提的是CenterNet对每个中心点做了<strong>偏差预测</strong>，为了弥补模型在下采样过程中造成的定位误差。</p></li></ol><p>后面的感觉都半斤八两了，佛系更新。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/u014380165/article/details/83032273" target="_blank" rel="noopener">CornerNet 算法笔记</a><br>[2]<a href="https://blog.csdn.net/c20081052/article/details/89358658" target="_blank" rel="noopener">论文精读——CenterNet :Objects as Points</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。&lt;/p&gt;
&lt;h1 id=&quot;Hourglass&quot;&gt;&lt;a href=&quot;#Hourglass&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关键点检测" scheme="http://a-kali.github.io/tags/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Hourglass" scheme="http://a-kali.github.io/tags/Hourglass/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>SPFCN：全卷积网络实现停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/04/SPFCN%EF%BC%9A%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/04/SPFCN：全卷积网络实现停车位检测/</id>
    <published>2020-08-04T15:33:29.000Z</published>
    <updated>2020-08-07T13:00:42.460Z</updated>
    
    <content type="html"><![CDATA[<p>论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection</p><p>Github：<a href="https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection" target="_blank" rel="noopener">https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>摘要：对于配备自动停车系统的车辆，车位检测的准确性和速度是至关重要的。本文提出了一个基于FCN的检测器，在保证准确性的同时实现更快的速度和更小的模型尺寸。作者制定了一个策略来选择最佳感受野的卷积核，并在每次训练epoch结束后自动删除冗余通道。该模型能够联合检测停车位的角和线特征，并能在常规的处理器上有效地实时运行。该模型在2.3 GHz的CPU上能达到 30 FPS，车位角定位误差1.51±2.14 cm (std. err)，车位检测精度98%，总体满足车载移动终端速度和精度要求。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>该模型使用<a href="https://blog.csdn.net/wangzi371312/article/details/81174452" target="_blank" rel="noopener">Stacked Hourglass Network</a>作为基本结构，同时输出标志点、入口线、边界线的heatmap。此处原理和CenterNet类似。</p><p><img src="https://i.loli.net/2020/08/04/PQqr8x3LBTjpStO.png" alt="image.png"></p><p>该模型的主要特点在于其<strong>SP模块（Select-Prune Module）</strong>，SP模块分为Select模块和Prune模块两部分：</p><ul><li><p><strong>Select Module</strong>：Select模块主要用于选择拥有更合适的感受野的卷积核。考虑到移动端使用类Inception结构对算力要求过大，不能满足实时性。Select模块使用<strong>贡献评估网络（Contribution Evaluation Networks, CEN）</strong>对不同感受野的卷积核进行评估，选择贡献度最高的的卷积核。CEN只是一个简单的MLP模块，接收不同卷积核的输出作为输入，并输出一个贡献值，最终只留下平均贡献值最高的卷积核。即训练阶段结束后，整个模块最终会退化成一个卷积核。</p><p><img src="https://i.loli.net/2020/08/04/JGntQPWVTpEKy2H.png" alt="image.png"></p></li><li><p><strong>Prune Module</strong>：Prune模块负责修剪卷积核中的通道。网络交替进行训练和修剪，在修剪阶段会自动评估每个卷积通道的贡献度，并剔除贡献度低于定值的通道。其贡献度由通道的权重计算得出。如果某个卷积核没有低于定值的通道，则会自动剔除贡献值最低的通道。</p><p><img src="https://i.loli.net/2020/08/04/Us6pSbQlrJyvOE4.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/04/snCaumJKZ2qb9Ve.png" alt="image.png"></p></li></ul><p>最后对<strong>鸟瞰图（bird eye view, BEV）</strong>进行输入网络进行分割，输出BEV的heatmap。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3    Experiments"></a>3    Experiments</h1><p>数据集使用的是<a href="https://cslinzhang.github.io/deepps/" target="_blank" rel="noopener">DeepPS数据集</a>，该数据集中包含 9527(training)+2138(validating) 张BEV图像。作者将训练样本转化为224<em>×</em>224的灰度图作为网络的输入，点线标记作为labels。</p><p>整个训练过程分为三个阶段，分别为<strong>预训练阶段</strong>、<strong>选择阶段</strong>和<strong>修剪阶段</strong>，不同阶段使用不同的损失函数。</p><p><img src="https://i.loli.net/2020/08/05/6zqsPuD5icKgjZx.png" alt="image.png"></p><p>预训练阶段的前5个epoch只用heatmaps的FocalLoss用来预热（warm-up），随后10个epoch使用完整的损失函数；在选择阶段，从第一层到最后一层逐层进行选择，每层选择结束后微调2-3个epoch；在修剪阶段，损失函数再次恢复到只有heatmap的FocalLoss，训练100个epoch的同时进行修剪；最后再去掉两个正则化操作，微调15个epoch。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection&lt;/p&gt;
&lt;p&gt;Github：&lt;a href=&quot;https://github.co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="全卷积网络" scheme="http://a-kali.github.io/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>VPS-net：基于深度学习的停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/03/VPS-net%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/03/VPS-net：基于深度学习的停车位检测/</id>
    <published>2020-08-03T14:20:27.000Z</published>
    <updated>2020-08-03T15:05:57.491Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视器(AVM)进行空车位检测的精度仍有待提高。为了解决这个问题，本文提出了一种基于深度学习的空车位检测方法，即VPS-Net。VPS-net将空车位检测问题转化为两步问题：车位检测和占用分类。在停车位检测阶段，我们提出了一种基于YOLOv3的检测方法，该方法将停车位的分类与标记点的定位相结合，利用几何线索对各类停车位进行检测标记。在占用分类阶段，我们设计了一个自定义网络，该网络的卷积核大小和层数根据车位的特点进行调整。实验表明，VPS-Net在PS2.0数据集检测空停位任务中可以达到99.63%的精确率和99.31%的召回率。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>VPS-Net基于深度学习来检测各种空车位。如下图所示，VPS-Net可以处理三种典型的停车槽（横向、纵向、倾斜）。一个车位由四个顶点组成，其中两个顶点是入口线的成对标记点，另外两个顶点由于视觉的限制通常在全景图像中不可见。</p><p><img src="https://i.loli.net/2020/08/03/hgDlz7RiSqMwO28.png" alt="image.png"></p><p>下图显示了用于检测空闲停车槽的VPS-Net的基本原理。VPS-Net将空车位检测分为车位检测和占用分类两个步骤，结合了多目标检测网络和分类网络的优点。在停车槽检测阶段，首先使用基于yolov3的检测器同时检测标记点和停车槽头，然后利用几何线索匹配成对的标记点，确定停车槽的方向，最后通过车位的类型、方位和成对标记点推断出两个不可见的顶点，得到完整的车位。检测到停车槽后，将其在图像中的位置转移到占用分类部分，将检测到的停车位规则化为一个统一大小的120x46像素，然后通过一个DCNN来区分它是否是空置的。一旦检测到空车位，将其位置发送给PAS的决策模块进行进一步处理。</p><p><img src="https://i.loli.net/2020/08/03/NVnCFWv5o621eB3.png" alt="image.png"></p><p>车位的类型由车位头决定，车位头包含入口线的成对标点。因此，停车槽头和标记点的检测是停车槽检测的第一步，也是最重要的一步。我们将停车槽头的分类与标记点的定位结合到一个多目标检测问题中，从而可以很容易地根据检测结果推断出各种类型的停车位。为此，我们分别定义了“直角头”、“钝角头”、“尖角头”和“T/L形”四种车位头。</p><ol><li><p>使用yolov3检测出停车<strong>标志点</strong>(marking point)和<strong>车位头</strong>(parking slot heads)；（中心点、宽、高）</p><p><img src="https://i.loli.net/2020/08/03/D4Xy5ZOaG76rbU8.png" alt="image.png"></p></li><li><p>根据检测结果和几何特征推断出停车位的<strong>方向、成对点、车位类型</strong>；</p><ul><li>将包含在同一个车位头框框内的两个标志点归类为成对点(paired marking point)；</li><li>如果一个车位头内只包含了一个或零个标志点，但车位头的置信度足够高，则直接计算出缺失点所在位置；</li><li>如果一个车位头内包含了两个以上的标志点，则选取离对角最近的两个点。</li></ul><p><img src="https://i.loli.net/2020/08/03/69tSpzeVuhCqa8w.png" alt="image.png"></p></li><li><p>根据车位头类型和长宽推断出车位类型（垂直、平行、倾斜）</p><p><img src="https://i.loli.net/2020/08/03/UXGMjQbfJCVDZdE.png" alt="image.png"></p></li><li><p>根据以上特征和车位类型推断出<strong>被遮挡点的位置</strong>;</p></li></ol><p><img src="https://i.loli.net/2020/08/03/K9ns1f3hSlFaXot.png" alt="image.png"></p><p>分类部分比较简单，在此不多赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>业务数据分析师入门</title>
    <link href="http://a-kali.github.io/2020/07/17/%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%E5%85%A5%E9%97%A8/"/>
    <id>http://a-kali.github.io/2020/07/17/业务数据分析师入门/</id>
    <published>2020-07-17T05:44:12.000Z</published>
    <updated>2020-07-17T05:45:08.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据分析概念、作用和步骤"><a href="#1-数据分析概念、作用和步骤" class="headerlink" title="1    数据分析概念、作用和步骤"></a>1    数据分析概念、作用和步骤</h1><p><strong>数据分析</strong>是指用适当的统计方法对收集来的大量数据进行分析，将他们加以汇总和理解并消化，以求最大化地开发数据的功能，发挥数据的作用。</p><p>数据分析的<strong>作用</strong>：</p><ul><li>现状分析：企业运营情况、业务发展。通常使用日报、月报来完成。</li><li>原因分析：在现状上展开原因分析，通常使用专题分析来完成。</li><li>预测分析：对企业发展趋势做预测，为企业提供参考和决策依据。</li></ul><p>数据分析的<strong>步骤</strong>：</p><ol><li>明确分析目的和思路：目的、思路、分析框架（如PEST）、体系化；</li><li>数据收集：数据库、出版物、互联网、市场调查；</li><li>数据处理：整理成适合数据分析的形式；</li><li>数据分析：将数据整理分析成结论，数据挖掘是一类高级的数据分析方法；</li><li>数据展现：表格、图像；</li><li>撰写报告：分析、结论、建议或解决方法。</li></ol><h1 id="2-数据分析方法论"><a href="#2-数据分析方法论" class="headerlink" title="2    数据分析方法论"></a>2    数据分析方法论</h1><p>确定分析思路以营销、管理等理论为指导，一般把这些数据分析相关的营销、管理等理论统称为<strong>数据分析方法论</strong>。</p><h2 id="PEST分析法"><a href="#PEST分析法" class="headerlink" title="PEST分析法"></a>PEST分析法</h2><p><a href="https://baike.baidu.com/item/PEST/10805117?fr=aladdin" target="_blank" rel="noopener">PEST</a>是指对<strong>政治（Political）、经济（Economic）、技术（Technological）和社会（Social）</strong>这四类影响企业的主要外部环境因素进行分析。</p><ul><li>政治：新政策，经济政策，市场道德标准，文化宗教等；</li><li>经济：社会经济结构，经济发展水平，经济体制，经济状况等；</li><li>社会：人口，流动性，消费心理，生活方式变化，价值观等；</li><li>技术：新技术的发明、传播等。</li></ul><h2 id="5W2H分析法"><a href="#5W2H分析法" class="headerlink" title="5W2H分析法"></a>5W2H分析法</h2><p>5W2H分析法是从回答中发现解决问题的线索的方法，广泛应用于企业营销管理活动等方面。</p><p><img src="https://i.loli.net/2020/07/12/mUvrcf5yDjSgQYL.png" alt></p><h2 id="逻辑树分析法"><a href="#逻辑树分析法" class="headerlink" title="逻辑树分析法"></a>逻辑树分析法</h2><p>逻辑树分析法是将一个已知问题当成树干，然后考虑这个问题和哪些问题相关。每想到一个问题就可以在所在树干下加一个树枝，并表明树枝代表什么问题。以此来将问题逐步分解。</p><p><img src="https://i.loli.net/2020/07/12/8hiW9d2t4DN6pPg.png" alt="image.png"></p><h2 id="4P营销理论"><a href="#4P营销理论" class="headerlink" title="4P营销理论"></a>4P营销理论</h2><p>4P营销理论将营销要素概括为如下四类：<strong>产品（Product）、价格（Price）、促销（Promotion）、渠道（Place）</strong>。如果要了解公司的整体运营情况，就可以采用4P营销理论进行分析指导。</p><p><img src="https://i.loli.net/2020/07/12/rHb25QgRoEukc13.png" alt="image.png"></p><h2 id="用户使用行为理论"><a href="#用户使用行为理论" class="headerlink" title="用户使用行为理论"></a>用户使用行为理论</h2><p> 用户使用行为是指用户行为获取、使用物品或服务所采取的各种行动，一般按照以下过程：<strong>认知产品、熟悉、试用、使用、成为忠实用户</strong>。</p><p><img src="https://i.loli.net/2020/07/12/KUDOub7vPJ5ZSdh.png" alt="image.png"></p><h1 id="3-常用数据分析方法"><a href="#3-常用数据分析方法" class="headerlink" title="3    常用数据分析方法"></a>3    常用数据分析方法</h1><h2 id="对比分析法"><a href="#对比分析法" class="headerlink" title="对比分析法"></a>对比分析法</h2><p>将两个或两个以上的数据进行比较，分析其中差异。包括与目标值对比、不同时期对比、同级别对比、行业内对比、活动效果对比。</p><h2 id="分组分析法"><a href="#分组分析法" class="headerlink" title="分组分析法"></a>分组分析法</h2><p><img src="https://i.loli.net/2020/07/12/m6SlcrROhxHqi71.png" alt="image.png"></p><h2 id="结构分析法"><a href="#结构分析法" class="headerlink" title="结构分析法"></a>结构分析法</h2><p><img src="https://i.loli.net/2020/07/12/ZQfzjATtUWeupEO.png" alt="image.png"></p><h2 id="平均分析法"><a href="#平均分析法" class="headerlink" title="平均分析法"></a>平均分析法</h2><p><img src="https://i.loli.net/2020/07/12/YFLxZ91EAJrWcf5.png" alt="image.png"></p><h2 id="杜邦分析法"><a href="#杜邦分析法" class="headerlink" title="杜邦分析法"></a>杜邦分析法</h2><p>杜邦分析法是利用主要<strong>财务指标</strong>之间的内在联系，对企业财务状况以及经济效益进行综合分析评价的方法。</p><p>![image-20200712161837746](C:\Users\STEVE JOBS\AppData\Roaming\Typora\typora-user-images\image-20200712161837746.png)</p><h2 id="漏斗图分析法"><a href="#漏斗图分析法" class="headerlink" title="漏斗图分析法"></a>漏斗图分析法</h2><p>漏斗图是一个适合业务流程比较规范、周期比较长、各流程环节涉及复杂业务比较多的管理分析工具。漏斗图能很快在复杂的环节中找出主要因素。</p><p><img src="https://i.loli.net/2020/07/12/NXEUSLAfxFCyhr2.png" alt="image.png"></p><h1 id="4-数据图表讲解"><a href="#4-数据图表讲解" class="headerlink" title="4    数据图表讲解"></a>4    数据图表讲解</h1><p><strong>选择恰当的图表（重要）：</strong></p><p><img src="https://i.loli.net/2020/07/12/thQ7YKC2PgEkpRA.png" alt="image.png"></p><h2 id="展现数据的表格技巧"><a href="#展现数据的表格技巧" class="headerlink" title="展现数据的表格技巧"></a>展现数据的表格技巧</h2><ol><li>突出显示单元格：对满足条件的单元格突出显示。（Excel：开始-样式-条件格式-突出显示单元格规则）</li><li>项目选取： 同上。（Excel：开始-样式-条件格式-项目选取规则）</li><li>数据条： 在表格单元格中显示柱状图，直观显示数值大小。（Excel：开始-样式-条件格式-数据条）</li><li>图标集：对数值分类，不同类别数值显示不同图标。（Excel：开始-样式-条件格式-图标样式）</li></ol><p>效果图：</p><p><img src="https://i.loli.net/2020/07/12/yQDGZrlNw2uRk8P.png" alt="image.png"></p><h2 id="各种图表展示"><a href="#各种图表展示" class="headerlink" title="各种图表展示"></a>各种图表展示</h2><p><img src="https://i.loli.net/2020/07/12/WCUdyFXJHxpl2Sw.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/YgKNH75vTlp9UeQ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/lPu4S6imsHEaY3G.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/HeV5PzsAmir2gpR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/9DWL2VoxKpZwJsR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/cUNn8bHgLjXBJhp.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/TieQ2JuEaODytkg.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/1MjT3GmWYsHeDtl.png" alt="image.png"></p><h1 id="5-数据关键指标"><a href="#5-数据关键指标" class="headerlink" title="5    数据关键指标"></a>5    数据关键指标</h1><h2 id="5-1-产品经理常用数据指标（KPI）"><a href="#5-1-产品经理常用数据指标（KPI）" class="headerlink" title="5.1    产品经理常用数据指标（KPI）"></a>5.1    产品经理常用数据指标（KPI）</h2><p><img src="https://i.loli.net/2020/07/12/kO5VWU74otcK2Fj.png" alt="image.png"></p><h2 id="5-2-电商类网站分析指标"><a href="#5-2-电商类网站分析指标" class="headerlink" title="5.2    电商类网站分析指标"></a>5.2    电商类网站分析指标</h2><p><img src="https://i.loli.net/2020/07/12/WoRqaPCTliugHfI.png" alt="image.png"></p><h3 id="5-2-1-内容指标"><a href="#5-2-1-内容指标" class="headerlink" title="5.2.1    内容指标"></a>5.2.1    内容指标</h3><p><strong>转化率</strong>：转化率 = 进行了相应动作的访问量 / 总访问量。用于衡量网站内容对访问者的吸引程度和网站的宣传效果。</p><p><strong>回访者比率</strong>：转化率 = 回访者数 / 独立访问者数。衡量网站内容对访问者的吸引程度和网站实用性，你的网站是否有令人感兴趣的内容使访问者再次访问你的网站。</p><ul><li>积极回访者比率：积极回访者比率 = 访问超过11页的用户 / 总访问数。衡量有多少访问者度网站内容高度感兴趣。</li><li>忠实访问者比率：忠实访问者比率 = 访问时间在xx分钟以上的用户 / 总访问数。</li></ul><p><strong>忠实访问者指数</strong>：忠实访问者指数 = 大于xx分钟的访问页数 / 大于xx分钟的访问者数。衡量“忠实访问者”是否真的在浏览页面。</p><p>忠实访问者量：忠实访问者量 = 大于xx分钟的访问页数 / 总访问页数。表现该网站是否吸引了错误的访问者。</p><p>访问者参与指数：访问者参与指数 = 总访问数 / 独立访问者数。</p><p><strong>回弹率</strong>：回弹率 = 只访问了一页的访问数 / 总访问数。表示访问者只看到了一页的比率。如果这个指标高则表明可能网页布局需要优化。</p><p>浏览用户相关：</p><ul><li>浏览用户比率：少于1分钟的访问者数 / 总访问数。</li><li>浏览用户指数：少于1分钟的访问页数 / 少于1分钟的访问者数。</li><li>浏览用户量：少于1分钟的浏览页数 / 所有浏览页数。</li></ul><h3 id="5-2-2-商业指标"><a href="#5-2-2-商业指标" class="headerlink" title="5.2.2    商业指标"></a>5.2.2    商业指标</h3><p><img src="https://i.loli.net/2020/07/12/IBUNDniwFX2gm59.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/tGmynaOofjDg14A.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/X4LSfPsdbxJ1EzB.png" alt="image.png"></p><h3 id="5-2-3-其它指标"><a href="#5-2-3-其它指标" class="headerlink" title="5.2.3    其它指标"></a>5.2.3    其它指标</h3><p><img src="https://i.loli.net/2020/07/12/rsoViW5gzMTZwqx.png" alt="image.png"></p><h1 id="6-数据分析报告"><a href="#6-数据分析报告" class="headerlink" title="6    数据分析报告"></a>6    数据分析报告</h1><p><strong>数据分析报告</strong>是根据数据分析方法，运用数据来反映某项事物的结论，并提出解决办法的分析应用文体。</p><p>数据分析报告的<strong>原则</strong>：</p><ul><li>规范性：术语规范；</li><li>重要性：选取重要的、关键的指标；</li><li>谨慎性：真实、严谨、实事求是；</li><li>创新性：特征挖掘。</li></ul><p>数据分析报告的<strong>作用</strong>：展示分析结果、验证分析质量、提供决策参考。</p><p>数据分析报告的<strong>种类</strong>：专题分析报告（单一性、深入性）、综合分析报告（全面性、联系性）、日常数据通报（进度性、规范性、时效性）。</p><p><strong>数据分析报告的构成</strong>：</p><ul><li>标题页：简洁的标题；</li><li>目录；</li><li>前言：分析背景、分析目的、分析思路；</li><li>正文：通过图表和文字系统全面地表述分析的过程和结果，表明所有数据分析事实和观点，各部分之间具有逻辑关系。</li><li>结论和建议；</li><li>附录：名词解释、计算方法、原始数据等。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-数据分析概念、作用和步骤&quot;&gt;&lt;a href=&quot;#1-数据分析概念、作用和步骤&quot; class=&quot;headerlink&quot; title=&quot;1    数据分析概念、作用和步骤&quot;&gt;&lt;/a&gt;1    数据分析概念、作用和步骤&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;数据分析&lt;/st
      
    
    </summary>
    
      <category term="数据分析" scheme="http://a-kali.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>证券投资概述</title>
    <link href="http://a-kali.github.io/2020/05/04/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/05/04/证券投资概述/</id>
    <published>2020-05-04T05:14:24.000Z</published>
    <updated>2020-05-04T05:15:28.716Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-证券家族四兄弟"><a href="#1-证券家族四兄弟" class="headerlink" title="1    证券家族四兄弟"></a>1    证券家族四兄弟</h1><h2 id="1-1-债券"><a href="#1-1-债券" class="headerlink" title="1.1    债券"></a>1.1    债券</h2><p><strong>定义：政府和企业借债，向投资人开具的借条。规定了期限，还本付息。</strong></p><p>现代债券出现于十二世纪的意大利，国家<strong>将巨额债务拆分成标准化的小块出售，向百姓筹集资金，并定期还本付息。</strong></p><p>债券的种类有上百种，但可以进行以下几种归类。债券根据期限可以分为<strong>短期债券、中期债券、长期债券</strong>，划分界限为一年和十年。通常来说债券偿还期越长风险越大，收益也越高。</p><p>债券根据发行人的不同可以分为<strong>政府债券、金融债券、企业债券</strong>。政府债券是信用最好、收益最稳定、风险最小的债券，由政府的权力进行保障。政府债券可分为<strong>中央政府债券</strong>和<strong>地方政府债券</strong>，中央政府债券也被称为<strong>金边债券</strong>。金融债券的风险和收益通常高于政府债券，金融企业通常包括商业银行、证券公司、保险公司。这些企业资金雄厚、实力强大。在中国这类企业通常有国资背景。企业债券为在证券交易所上市的企业发行的债券，风险较大。</p><p>债券的风险小，收益较低。在股市熊市的时候可以作为一种不错的投资选项。<strong>债券基金</strong>相比于债券流动性更强，购买三天后即可赎回。</p><p>债券按照信用等级可以被分为<strong>垃圾债</strong>和<strong>投资债</strong>。</p><h2 id="1-2-股票"><a href="#1-2-股票" class="headerlink" title="1.2    股票"></a>1.2    股票</h2><p><strong>定义：企业向投资人发行的共担风险、共享收益的所有权凭证。</strong></p><p>普通股（A股）的三大性质：</p><ol><li><strong>有限责任</strong>：投资人所需承担的风险取决于投资的金额。</li><li><strong>分红</strong>：按照股份分红，企业挣的越多分红越多。</li><li><strong>投票权</strong>：股东享有企业决策的投票权，股份越多权重越大。</li></ol><p>优先股：优先比普通股的股东分红，但股东不能参与公司决策。</p><p>决策权股（B股）：企业授权给管理层的股票，其决策权重高于普通股。</p><p>股票收益来源于两个方面：<strong>分红</strong>和<strong>资本利得</strong>。分红为从企业收益中分享给投资者的红利；资本利得为股票买卖的差价。</p><h2 id="1-3-基金"><a href="#1-3-基金" class="headerlink" title="1.3    基金"></a>1.3    基金</h2><p><strong>定义：基金是代人理财的机构。代人投资证券的机构为证券投资基金。</strong></p><p>特征：</p><ol><li>共担风险，共享收益。</li><li>专家理财：将散户的钱统一交给专家投资。</li><li>分散风险：一个基金可以投资上百个股票。</li></ol><p>按照投资对象对基金分类：</p><ol><li><strong>债券基金</strong></li><li><strong>股票基金</strong></li><li><strong>混合基金</strong>：既投资债券又投资股票的基金。</li><li><strong>货币基金</strong>：投资货币性金融资产（即一年期以内的金融资产，流动性强，风险小）的基金。余额宝是一种货币基金。</li></ol><p>在股市熊市的时候可购买货币基金，在牛市的时候如果不知道该买哪只股票可全仓股票基金和混合基金。</p><p>按照基金风格分类：</p><ol><li><strong>主动型基金</strong>：只挑选少量股票且不断换股，以更高收益为目的。对基金经理要求高，交易手续费较高，为1.5%。</li><li><strong>被动型基金</strong>：挑选大量股票，分散风险，不换股。手续费低，为0.5%。其中以指数基金最为典型。指数基金是以某特定指数（比如沪深300）的成分股为投资对象的被动型基金。在证券市场可直接交易的指数基金称为<strong>ETF</strong>。</li></ol><p>大部分主动型基金跑不过指数基金。</p><p>基金还可以被分为<strong>私募基金（对冲基金）</strong>和<strong>公募基金（共同基金）</strong>。公募基金在公共场合公开募集资金，而私募基金在公开场合是买不到的。公募基金门槛低，通常散户就能投资；而私募基金则对投资金额、投资人财产情况、收入等有要求。公募基金通常收到政府和企业的严格监管，有严格的限制和流程。公募基金经理收入来源于手续费，私募基金经理来源于投资收入的一部分（投资负收益则没有收入）。私募基金收到的监管少，通常良莠不齐。</p><p>万金油投资——指数基金定投。门槛低，成本低，避免追涨杀跌。</p><p>指数基金可分为<strong>宽基指数基金</strong>和<strong>行业指数基金</strong>。宽基即面向各行各业（如沪深300），行业指数基金则对应单一行业（如白酒ETF）。指数基金还可以被分为<strong>场内基金</strong>和<strong>场外基金</strong>。场内基金在证券交易所和股票软件购买，可以像股票一样实时交易；场外基金可以在银行、支付宝、股票软件购买，不能实时交易。</p><p>比较知名的ETF：</p><ol><li>沪深300：从上海和深圳证券市场中选取最优的300只大型企业A股作为样本编制而成的成份股指数,原则上每半年调整一次样本股。</li><li>中证500：排在沪深300之后的500只中小企业A股。</li><li>上证50：上海证券市场前50，包括茅台、中国平安和招商银行等。</li><li>上证指数：上海证券综合指数，这个指数的样本是在上海证券交易所全部上市股票，这里包括A股和B股。（2600-2700低买，2900-3000高卖）</li><li>创业50：创业板前50。</li><li>创业指数：创业板前100。 </li></ol><h2 id="1-4-衍生工具"><a href="#1-4-衍生工具" class="headerlink" title="1.4    衍生工具"></a>1.4    衍生工具</h2><p><strong>定义：在股票、债券、 商品、货币的买卖合同上加上新的买卖合同，就是衍生工具。主要包括远期合约、期货、期权、互换。</strong></p><p><strong>远期合约</strong>：易双方约定在未来的某一确定时间，以确定的价格买卖一定数量的某种金融资产的合约。远期合约通常是大合同，由于未来的不确定性，远期合约有较高的被毁约的风险。</p><p><strong>期货</strong>：以某种大众产品如棉花、大豆、石油等及金融资产如股票、债券等为标的标准化可交易合约。具有多次交易（保证金+实际金额）、杠杆、高收益高风险等特性。</p><p><strong>期权</strong>：合约赋予持有人在某一特定日期或该日之前的任何时间以固定价格购进或售出一种资产的权利。需要支付权利金购买。</p><p><strong>互换</strong>：互换两个或两个以上的当事人按照共同商定的条件，在约定的时间内定期交换现金流的金融交易，可分为货币互换、利率互换、股权互换、信用互换等类别。如人民币换美元。</p><p>期货的交易为跨期交易，共分两次交易。第一次交易叫做<strong>开仓</strong>，只需交保证金；第二次交易叫<strong>平仓</strong>，需支付全额。由于期货第一次交易只需要交少量保证金，以小博大，所以期货是加杠杆的。认为货物涨价，先买后卖的订单叫<strong>多单</strong>；认为货物降价，先卖后买的订单叫<strong>空单</strong>。空单时可以向期货交易所借货卖出，并在一定期限后买回归还。</p><p>期货的意义：</p><ol><li>对企业来说可以锁住风险，安心经营。</li><li>期货能直观反映一个货物的涨跌。</li><li>具有国家战略意义（比如可以用人民币结算石油期货）。</li></ol><h2 id="加餐：证券玩家、投资者与投机者"><a href="#加餐：证券玩家、投资者与投机者" class="headerlink" title="加餐：证券玩家、投资者与投机者"></a>加餐：证券玩家、投资者与投机者</h2><p>证券市场上存在三种人：证券玩家、投资者和投机者。</p><p>证券玩家对市场上的风吹草动十分敏感，每天都在追涨杀跌做短线。这类人通常挣不到钱。</p><p>投资者通常研究优质企业，进行长线投资。</p><p>投机者介于投资者和证券玩家之间，关心大趋势。比如贸易战、加息、汇率变动等。</p><h1 id="2-证券市场"><a href="#2-证券市场" class="headerlink" title="2    证券市场"></a>2    证券市场</h1><p>证券市场根据职能可以分为<strong>证券发行市场</strong>和<strong>证券交易市场</strong>，根据交易点可分为<strong>场内市场</strong>和<strong>场外市场</strong>，场内即证券交易所内，场外通常是在商业银行柜台进行交易。证券市场根据服务对象可分为<strong>一板、二板、三板</strong>。一板通常给大型企业上市，二板是创业板，三板是场外市场，被称为<strong>股权代办交易系统</strong>，证券公司在三板代办不能在主板上市的公司股票。</p><h2 id="2-1-证券发行市场"><a href="#2-1-证券发行市场" class="headerlink" title="2.1    证券发行市场"></a>2.1    证券发行市场</h2><p>证券发行的方式包括<strong>私募</strong>和<strong>公募</strong>。私募为私下发行证券，向特定投资人发行新证券；公募在市场上公开募集，需要向市场和社会公开信息，比如发行人的经济状况、证券的特点等。股票发行需要公开<strong>招股说明书</strong>，内容包括企业的经营状况、主营业务、未来收入等。</p><p>证券发行还可以被分为<strong>直接发行</strong>和<strong>间接发行</strong>。直接发行是直接将证券发给投资人，间接发行是将证券通过<strong>证券公司</strong>（又称<strong>券商</strong>、<strong>投行</strong>）发行。间接发行分为<strong>代销</strong>和<strong>包销</strong>。代销时，证券公司仅作为销售平台，不负责任；包销时，证券公司收购所有证券并加手续费卖出，卖不出去的证券只能证券公司自己留着。</p><p>发行定价的方式：</p><ol><li><strong>市盈率(PER)定价</strong>：市盈率指股票价格除以每股盈利的比率。成熟的市场国家市盈率通常为15倍，我国通常按照20倍市盈率定价。</li><li><strong>净资产定价</strong>：净资产指指企业的资产总额减去负债以后的净额。股票的价格按照每股净资产的一定倍数定价。</li><li><strong>竞价</strong>。</li></ol><p>在我国主要是市盈率定价。</p><h3 id="注册制"><a href="#注册制" class="headerlink" title="注册制"></a>注册制</h3><p>注册制由<strong>核准制</strong>发展而来。核准制下监管部门对上市企业制定相应标准，并对符合标准的企业进行实质性审核；而注册制下监管部门制定严格的标准并审核相关材料，剩下的由证券交易所和证券公司进行安排。注册制启用后，上市企业将变得远多于核准制。注册制时成熟的市场经济国家的证券市场常规的办法。注册制目前在我国创业板试点。</p><p>注册制的影响：</p><ol><li>新股不会受到严格的审查监管，股民对炒新股会更加谨慎。</li><li>大浪淘沙，留住好企业。市场的入口和出口都会放得更开，仙股（股价低于1元）更容易退市。</li><li>注册制将留住好企业，引导市场变得成熟，牛市更长熊市更短。</li></ol><h3 id="加餐：上市公司割韭菜的套路"><a href="#加餐：上市公司割韭菜的套路" class="headerlink" title="加餐：上市公司割韭菜的套路"></a>加餐：上市公司割韭菜的套路</h3><ol><li>假装回购（回购：上市公司将股票的一部分买回来注销，减少股票量来抬高股价），实际上并没有回购或者回购数量没有预定的那么多，吸引散户抬高股价。</li><li>无意义的增发股票圈钱，但并没有用获得的钱做利于公司发展的事。</li><li>不分红。</li></ol><h2 id="2-2-证券交易市场"><a href="#2-2-证券交易市场" class="headerlink" title="2.2    证券交易市场"></a>2.2    证券交易市场</h2><p><strong>证券交易所</strong>是以拍卖形式交易旧证券的场所。证券交易所里有两类人：<strong>内部管理人员</strong>和<strong>交易所会员</strong>。交易所会员的注册要求非常高，上海证券交易所的注册资本为500万元。交易所会员有两类人：<strong>经纪人</strong>和<strong>交易商</strong>。经纪人即是证券公司，普通人通过证券公司进行投资；交易商是有大型证券订单的大公司。</p><p>证券市场交易时间为每周一到周五上午时段9:30-11:30，下午时段13:00-15:00。周六、周日及上海证券交易所、深圳证券交易所公告的休市日不交易。由于我国市场太不成熟，所以股票交易由原先的T+0制度改为T+1制度，并设定了涨跌停板。</p><p>在上市公司股票价格出现重大波动，上市公司有重大决策时，为了引起投资者注意，停止股票交易，称为<strong>停牌</strong>；而<strong>摘牌</strong>指将不符合条件的上市公司踢出证券市场。</p><p>我国为了一定程度上维护投资者利益，实施了<strong>ST(Special Treatment)制度</strong>。当一个企业变成仙股，就会被扣上ST的帽子。如果企业继续下行，ST会变成*ST，临近退市边缘。ST制度会让部分垃圾企业成为巨婴，如今我国正严格退市制度，将垃圾企业清理出市。</p><h1 id="3-证券基本分析"><a href="#3-证券基本分析" class="headerlink" title="3    证券基本分析"></a>3    证券基本分析</h1><h2 id="3-1-宏观环境"><a href="#3-1-宏观环境" class="headerlink" title="3.1    宏观环境"></a>3.1    宏观环境</h2><p>宏观环境主要包括三个方向：<strong>社会大环境、大政方针、经济周期与反周期</strong>。</p><p>社会大环境：人口老龄化（医疗）、人民收入提高（消费）等。</p><p>大政方针：中国制造2025、一带一路、雄安新区等。</p><p>经济周期：人们在预料到经济周期前通常会对股票进行买入或卖出，所以股市通常是经济周期的先行指标。部分行业不受经济周期影响，比如食品行业、烟草行业、医疗行业等；部分行业受经济周期影响较大，比如钢铁、冶金、建材行业。</p><p>反周期的政策：经济周期处于萧条期时不利于经济发展，此时政府会推出反周期的政策来刺激经济。</p><h2 id="3-2-行业对股市的影响"><a href="#3-2-行业对股市的影响" class="headerlink" title="3.2    行业对股市的影响"></a>3.2    行业对股市的影响</h2><p>行业：指一组提供同类相互密切替代商品或服务的公司。</p><p>判断一个行业的时候一定要知道其挣钱模式，即其面向的客户、出售的产品等。</p><p>行业和人一样有从幼年、青年、中年到老年的生命周期。幼年期的企业处于创业期，以亏损为主；青年期的企业竞争对手多，风险大；中年企业产品定型、市场成熟，可以作为主要投资方向。</p><p>周期股与防御股：珠宝、建材、钢铁、有色金属受经济影响大，建议在牛市时作为主要买入对象；食品、烟酒、医药、调料等受经济影响小，可以在熊市的时候投资。</p><h2 id="3-3-企业对股市的影响"><a href="#3-3-企业对股市的影响" class="headerlink" title="3.3    企业对股市的影响"></a>3.3    企业对股市的影响</h2><p><strong>得天独厚的自然条件、品牌技术专利、研发投入、顾客的转化成本</strong>是企业无法被替代的重要条件。举例：</p><ul><li>茅台在赤水河旁边，所处气候适合酿造高品质的白酒；</li><li>茅台有很多酱香酒的专利，无法被模仿；</li><li>华为麒麟980芯片研发投入3亿美元，令普通公司望而却步;</li><li>微软和苹果的PC端操作系统用户、腾讯QQ和微信用户转化成本非常高；</li><li>滴滴网约车、摩拜共享单车的模式门槛低，容易产生竞争对手。</li></ul><h2 id="加餐：粗看股票"><a href="#加餐：粗看股票" class="headerlink" title="加餐：粗看股票"></a>加餐：粗看股票</h2><ol><li>看F10中的<strong>每股收益、市盈率、企业主营业务、竞争对手、竞争对手的股价</strong>。其中以市盈率为主。如果是高成长的行业，30-40倍的市盈率是可以接收的；如果是成熟行业，市盈率通常在15倍以下为佳；如果是衰退行业，市盈率尽量选择在10倍以下。（股票中的F10的意思是指在分析软件中按键盘上的F10可以查看该公司基本面的文本资料，指向该股的基本公开信息，包括股本、股东、财务数据、公司概况和沿革、公司公告、媒体信息等等，都可快速查到。）</li><li>看<strong>毛利率</strong>（销售利润/销售收入）：毛利率＞20%为良，＞40%为优；</li><li>看<strong>净资产收益率</strong>（(资产-负债)/收益）：净资产＞15%为优。</li></ol><p>这三个指标不能只看某一年的数据，某一年的数据不能作为常态，应当看近几年的数据。</p><p>K线图可以看是否处于历史低位或者单日U型、V型底部。</p><h1 id="4-证券技术分析"><a href="#4-证券技术分析" class="headerlink" title="4    证券技术分析"></a>4    证券技术分析</h1><h2 id="4-1-图形识别"><a href="#4-1-图形识别" class="headerlink" title="4.1    图形识别"></a>4.1    图形识别</h2><p><strong>压力（阻力）与支撑</strong>：支撑位是指在股价下跌时遇到买方大量买入，从而止跌回稳的价位；其对应的是阻力位。</p><p>压力位与支撑位的转换：压力位意味着股价上方有一群套牢盘，许多人之前在该点位买入并随着股价下跌而亏损，想在该点位卖出。如果股价突破了压力位继续上涨，在压力位卖出的人又会后悔，想在该点买入，此时压力位就转换成了支撑位。支撑位同理。</p><p>当股价多次试图突破压力位时没成功时，就会在压力位套牢一大批人，留下多个压力位，股价转跌。支撑位同理。</p><p><img src="https://i.loli.net/2020/05/03/nV26gKahL9QOcUP.png" alt="ISBW11D8MS_@P29_PR_QST1.png"></p><p><strong>趋势线</strong>就是由压力点或支撑点组成的呈上涨/下跌/水平趋势的<strong>上下轨线</strong>，通过趋势线可以看出股价的趋势；当趋势不明显或者水平的时候，趋近上轨时卖出，趋近下轨时买入。</p><p><img src="https://i.loli.net/2020/05/03/b8owYM2zQTdnDSP.png" alt="DU5`FA_HCC80SGLI16_D__3.png"></p><p>根据压力和支撑理论，可以衍生出头肩底、W底、圆弧底等图形理论，在此不多赘述。</p><p><strong>缺口理论</strong>：</p><p><img src="https://i.loli.net/2020/05/03/cR98qnYl2w4iQaj.png" alt="V@N_B9_P_UL0MLZ_O2UB4_V.png"></p><h2 id="加餐：蜡烛图与K线形态组合"><a href="#加餐：蜡烛图与K线形态组合" class="headerlink" title="加餐：蜡烛图与K线形态组合"></a>加餐：蜡烛图与K线形态组合</h2><p>蜡烛图：</p><p><img src="https://i.loli.net/2020/05/04/75FuKJWsMceBUCf.png" alt="_`8Y2IE_MQA_8QNJE_FKGXF.png"></p><p>连续行情+同价线=行情结束</p><p>连续行情+同价线+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/gKlEu63dLDshIbW.png" alt="5X__V`G@_RN1@ZE3V_JC_C5.png"></p><p>连续行情+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/BAm2JskVGLyQh4e.png" alt="_KI4__0FV8A2M_S4_`PEQPS.png"></p><p><img src="https://i.loli.net/2020/05/04/t8AsOuDdEZ5aTpM.png" alt="DM0KUNY_9SN65H_V__~_`PL.png"></p><p>成交密集区的跳空缺口=横盘结束</p><p><img src="https://i.loli.net/2020/05/04/xiguQbB4oXSypm1.png" alt="7_MJWHJ___N_@1_1`R_XLMF.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-证券家族四兄弟&quot;&gt;&lt;a href=&quot;#1-证券家族四兄弟&quot; class=&quot;headerlink&quot; title=&quot;1    证券家族四兄弟&quot;&gt;&lt;/a&gt;1    证券家族四兄弟&lt;/h1&gt;&lt;h2 id=&quot;1-1-债券&quot;&gt;&lt;a href=&quot;#1-1-债券&quot; class
      
    
    </summary>
    
      <category term="经济与金融" scheme="http://a-kali.github.io/categories/%E7%BB%8F%E6%B5%8E%E4%B8%8E%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="证券投资" scheme="http://a-kali.github.io/tags/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：使用RNN进行情感识别</title>
    <link href="http://a-kali.github.io/2020/03/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/03/03/论文笔记：使用RNN进行情感识别/</id>
    <published>2020-03-03T08:31:11.000Z</published>
    <updated>2020-03-03T09:22:57.794Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1701.08071" target="_blank" rel="noopener">Emotion Recognition From Speech With Recurrent Neural Networks</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在<strong>自动语音识别(Automated Speech Recognition, ASR)</strong>任务中，语音被模型转化成文字。但是在人们的对话过程中除了文本以外还有其它重要的信息，比如<strong>语调、情感、响度</strong>。这些信息在语音理解中亦扮演者十分重要的角色。本文将围绕其中“情感”部分进行概述，即<strong>语音情感识别(Speech Emotion Recognition, SER)</strong>。</p><p>在语音情感识别方面存在一些难点：</p><ol><li><strong>情感是主观的</strong>，不同人对于同一段语音，理解出的情感可能不同。</li><li><strong>同一段语音可能包含多种情感</strong>。（可以通过<a href="https://zhuanlan.zhihu.com/p/86745594" target="_blank" rel="noopener">CTC损失函数</a>解决）</li><li><strong>数据来源</strong>：从电影中截取的语音可能和现实中存在偏差。通常会找专业演员来演绎各种情感来制造数据。</li></ol><h1 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2    Related works"></a>2    Related works</h1><p>大部分文献将语音情感识别视为一个分类问题，对每一个utterance分配一个label。utterance即为一小段语音，是语音的最小单元。</p><p>在深度学习之前，大多研究提取底层的手工特征，用传统分类器进行分类，比如HMM（隐马尔可夫模型）或GMM（高斯混合模型）。</p><p>深度学习出现后，有人把utterance分帧计算低层特征，用三层全连接层，对输出概率聚合成utterance水平的特征（用简单的统计量，比如最大值，最小值，平均值等），最后用ELM（Extreme Learning Machine）分类。</p><p>后面出现了纯深度学习和端到端的架构模型。有人使用Attention CNN，有人用DBN，还有人用迁移学习把语音识别的任务（数据集）迁移到语音情感识别中。</p><h1 id="3-Data-and-preprocessing"><a href="#3-Data-and-preprocessing" class="headerlink" title="3    Data and preprocessing"></a>3    Data and preprocessing</h1><p>IEMOCAP（Interactive Emotional Dyadic Motion Capture）被选作数据集，因为它有详尽的获取方法，免费的学术许可，较长的语音时长和良好的标注。</p><p>大约包括12个小时，含有视频，音频和人脸关键点的数据。由南加利福尼亚大学戏剧系的10位专业演员表演所得。评估者对每个utterance给出评价（10个情感选项），当一半以上的评估者对某个utterance的评估一致时，该utterance才分配到评估的感情。本文中选取其中4种情感用于分析（生气，兴奋，中立和伤心），只有这些样本才被考虑到本文工作中，下图是标签分布。</p><p><img src="https://img2018.cnblogs.com/blog/1160281/201811/1160281-20181113191458804-356625669.png" alt="img"></p><p>原始信号的采样率是16kHz，直接使用计算量很大，需要尽量保持信息的同时减小计算量。本文对utterance进行分帧，帧长为200ms，帧移为100ms，在帧上计算声学特征（用了哪些声学特征见下文介绍），然后把这些特征合在一起作为utterance的特征输入到模型。关于帧长的选取，论文从30ms到200ms都做过实验发现效果差别不大，而较长的帧可以导致比较少的帧，能减小计算量，所以使用了200ms。</p><p>对于语音信号的特征主要有三种，一是声学特征，也就是声波的一些属性；二是音律特征，指的是停用词，韵律（押韵，平仄），响度，这个特征依赖于说话人，所以没有用这类特征；三是语义学特征，就是语音对应的文字内容的信息。</p><p>本文只使用了声学特征，使用的是python库PyAudioAnalysis的API提供的34个特征，主要包括3个时域特征（过零率，能量，能量熵），5个谱特征，13个MFCC特征，13个音阶特征。也就是一帧的声音用34维的向量来表示。</p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4    Approach"></a>4    Approach</h1><p>因为一个utterance只对应一个标签，但是有很多帧，有些帧是不包含情感的，所以输入序列和输出序列难以一一对应，为了应对这个问题，可以使用CTC（Connectionist Temporal Classification）的方法。</p><p>CTC模型中的LSTM的输入时间步和输出时间步T为78，因为每个语音样本划分成了78帧。情感标签有4个，加上空白符，得到大小为5的字符集合。真实输出只有一个标签，所以在这些长度为78的输出序列中，经过B转换后能得到一个真实情感标签的那些序列才是我们要的序列，用CTC的方法来使得这些序列产生的概率最大。</p><p>注：文本大量参考这篇文章<a href="https://www.cnblogs.com/liaohuiqiang/p/9954088.html" target="_blank" rel="noopener">论文笔记：Emotion Recognition From Speech With Recurrent Neural Networks</a>，以自己的语言整理一遍笔记，旨在巩固记忆加深理解，并非完全原创。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1701.08071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Emotion Recognition From Speech With Recurrent Neural Netw
      
    
    </summary>
    
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="RNN" scheme="http://a-kali.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>DAIC-WOZ抑郁评估数据格式</title>
    <link href="http://a-kali.github.io/2020/02/09/DAIC-WOZ%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/"/>
    <id>http://a-kali.github.io/2020/02/09/DAIC-WOZ抑郁评估数据格式/</id>
    <published>2020-02-09T14:52:27.000Z</published>
    <updated>2020-02-11T13:02:26.459Z</updated>
    
    <content type="html"><![CDATA[<p>DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的数据。该代理能够自动对人们进行访谈，并在语言(verbal)和非语言(nonverbal)指标上识别精神疾病。收集的数据包括音频和视频记录以及大量的的问卷回答；这部分语料库包括一个名为Ellie的动画虚拟面试官主持的Oz访谈，由另一个房间里的真人面试官控制。数据已被转录和注释的各种语言的和非语言的特征。</p><h1 id="Data-description"><a href="#Data-description" class="headerlink" title="Data description"></a>Data description</h1><p>数据包中包含<strong>编号300-492、共189个数据样本</strong>（其中 342,394,398,460 因技术原因被移除）。数据包格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Pack\</span><br><span class="line">300_P</span><br><span class="line">301_P</span><br><span class="line">...</span><br><span class="line">492_P</span><br><span class="line">util</span><br><span class="line">documents</span><br><span class="line">train_split.csv</span><br><span class="line">dev_split.csv</span><br><span class="line">test_split.csv</span><br></pre></td></tr></table></figure><p>部分样本需要提醒：</p><ul><li>373 - 在5:52-7:00有一个中断，助手进入房间解决一个小的技术问题，会议继续进行并结束。</li><li>444 - 在4:46-6:27左右有一个中断，参与者的手机响了，助手进入房间帮助他们关机。</li><li>451,458,480 - 会话在技术上是完整的，但是缺少了Ellie(虚拟人类)部分的记录。参与者的成绩单仍然包括在内，但没有面试官的问题。</li><li>402 - 视频结尾被删减了约2分钟。</li></ul><p><strong>train_split_Depression_AVEC2017.csv</strong>：该文件包含参与者id、PHQ8二进制标签(PHQ8得分&gt;= 10)、PHQ8得分和参与者性别，以及PHQ8问卷的每个问题的唯一回答。详细信息参见documents文件夹下的scherer_etal2015_VowelSpace.pdf。</p><p><strong>dev_split_Depression_AVEC2017.csv</strong>：同上。</p><p><strong>test_split_Depression_AVEC2017.csv</strong>：仅包含id和性别。</p><p>每个样本文件夹下文件组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">XXX_P\ </span><br><span class="line">  XXX_CLNF_features.txt </span><br><span class="line">  XXX_CLNF_features3D.txt </span><br><span class="line">  XXX_CLNF_gaze.txt </span><br><span class="line">  XXX_CLNF_hog.bin </span><br><span class="line">  XXX_CLNF_pose.txt </span><br><span class="line">  XXX_CLNF_AUs.csv   </span><br><span class="line">  XXX_AUDIO.wav </span><br><span class="line">  XXX_COVAREP.csv </span><br><span class="line">  XXX_FORMANT.csv </span><br><span class="line">  XXX_TRANSCRIPT.csv</span><br></pre></td></tr></table></figure><p>util文件夹组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">util\ </span><br><span class="line">  runHOGread_example.m </span><br><span class="line">  Read_HOG_files.m</span><br></pre></td></tr></table></figure><h1 id="File-description-and-feature-documentation"><a href="#File-description-and-feature-documentation" class="headerlink" title="File  description  and  feature  documentation"></a>File  description  and  feature  documentation</h1><p>这部分表述的是每个样本文件夹下各个文件的作用。</p><h2 id="1-CLNF-framework-output"><a href="#1-CLNF-framework-output" class="headerlink" title="1    CLNF framework output"></a>1    CLNF framework output</h2><p>这部分是由CLNF人脸关键点检测算法输出的数据，包括以下文件：</p><ul><li>XXX.CLNF_features.txt<ul><li>包含68个2D人脸关键点；</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success, x0,  x1,…,  x67,  y0,  y1,…, y67。分别表示 帧、时间点、置信度、是否检查成功，各个关键点坐标；</li><li>值之间由逗号分隔，虽然后缀是txt但应该当作csv文件处理。</li></ul></li><li>XXX_CLNF_AUs.csv<ul><li>AU表示Action Unit，是<a href="http://www.360doc.com/content/15/0128/13/10690471_444446832.shtml" target="_blank" rel="noopener">面部表情编码系统</a>(Facial Action Coding System, FACS)的运动单元。每一个AU代表一个表情元素；</li><li>文件格式：frame,  timestamp,  confidence,  success,  AU01_r,  AU02_r, AU04_r,  AU05_r,  AU06_r, AU09_r,  AU10_r,  AU12_r,  AU14_r, AU15_r,  AU17_r,  AU20_r,  AU25_r,  AU26_r,  AU04_c, AU12_c,  AU15_c,  AU23_c,  AU28_c,  AU45_c。其中AUX_r表示该面部包含该AU的概率，而AUX_c则用二值表示是否包含该AU。</li></ul></li><li>XXX.CLNF_features3D.txt<ul><li>包含68个3D人脸关键点；</li><li>格式与2D的类似，只是多了个坐标轴。以摄像机为坐标(0,0,0)，单位为毫米。</li></ul></li><li>XXX.CLNF_gaze.txt<ul><li>文件包含4个视线向量。前两个表示以世界为坐标空间，双眼的视线向量；后两个表示以头为坐标空间，双眼的视线向量。</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success,  x_0,  y_0,  z_0,  x_1,  y_1,  z_1, x_h0,  y_h0,  z_h0,  x_h1,  y_h1,  z_h1</li></ul></li><li>XXX.CLNF_hog.bin<ul><li>Felzenswalb’s  HoG</li></ul></li><li>XXX.CLNF_pose.txt<ul><li>pose文件包含两个坐标，X,Y,Z是位置坐标，Rx,Ry,Rz是头部旋转坐标。位置是以毫米为单位的世界坐标，旋转是以弧度和欧拉角为单位的(为了得到一个合适的旋转矩阵，使用R= Rx * Ry * Rz)。</li><li>文件格式：frame_number,  timestamp(seconds),  confidence,  detection_success,  X,  Y,  Z,  Rx,  Ry,  Rz</li></ul></li></ul><h2 id="2-Audio-file"><a href="#2-Audio-file" class="headerlink" title="2    Audio file"></a>2    Audio file</h2><ul><li>XXX_AUDIO.wav<ul><li>耳机录音频率为16kHz。音频文件中可能包含少量虚拟面试官的信息，在处理时使用记录文件(transcript files)来缓解这个问题。</li></ul></li></ul><h2 id="3-Transcript-file"><a href="#3-Transcript-file" class="headerlink" title="3    Transcript file"></a>3    Transcript file</h2><ul><li>XXX_TRANSCRIPT.csv</li></ul><h2 id="4-Audio-features"><a href="#4-Audio-features" class="headerlink" title="4    Audio features"></a>4    Audio features</h2><ul><li>XXX_COVAREP.csv</li><li>XXX_FORMANT.csv</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的
      
    
    </summary>
    
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>3D点云深度学习综述之 Shape Classification</title>
    <link href="http://a-kali.github.io/2020/02/08/3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E4%B9%8B-Shape-Classification/"/>
    <id>http://a-kali.github.io/2020/02/08/3D点云深度学习综述之-Shape-Classification/</id>
    <published>2020-02-08T15:45:21.000Z</published>
    <updated>2020-02-09T14:57:27.169Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1    摘要"></a>1    摘要</h1><p>3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术，已经成功地应用于解决各种二维视觉问题。然而，由于使用深度神经网络处理点云所面临的独特挑战，对点云的深度学习仍处于起步阶段。最近，点云上的深度学习变得越来越流行，人们提出了许多方法来解决这一领域的不同问题。目前点云分类方法可以主要被分为两类：<strong>基于投影的网络（Projection-based Networks）</strong>和<strong>点基网络（Point-based Networks）</strong>。本文将从这两个方向对用于点云分类的神经网络方法进行了综述。</p><p><img src="https://i.loli.net/2020/02/09/KUAF1fT6tzu3nVl.png" alt="64NDV_C@`6_PE_XEMYT9OL6.png"></p><h1 id="2-基于投影的网络"><a href="#2-基于投影的网络" class="headerlink" title="2    基于投影的网络"></a>2    基于投影的网络</h1><p>基于投影的方法将三维点云投影到不同的表达模式中(例如：多视图、体积表示)，用于特征学习和形状分类。</p><h2 id="2-1-多视图表示"><a href="#2-1-多视图表示" class="headerlink" title="2.1    多视图表示"></a>2.1    多视图表示</h2><p>多视图（Multi-view）方法通常<strong>提取一个3D对象从多个角度投影的特征，之后再通过某种方法将多个特征融合起来</strong>。而如何将视图特征融合起来则是这个方法的关键。MVCNN开创了这种方法的先例，其简单地将多个视图的特征max-pools到一个全局描述符（global descriptor）中。而max-pooling仅仅保留了单一视图的最大值元素，损失了大量信息。MHBN在此之上进行改进，通过双线性池化（bilinear pooling）来得到一个全局的描述符。后续有许多网络进行了进一步的改进，在此不一一赘述。</p><h2 id="2-2-体积表示"><a href="#2-2-体积表示" class="headerlink" title="2.2    体积表示"></a>2.2    体积表示</h2><p>早期的研究者们通常<strong>使用3D卷积网络对基于3D点云的体积表示（Volumetric representation）进行特征提取</strong>。Daniel等人提出了一种体积占用网络VoxNet来实现3D对象识别。Wu等人提出了一种基于卷积深度信念的3D ShapeNets来学习不同3D形状的点分布。在体素网格上，三维形状通常由二进制变量的概率分布来表示。虽然取得了令人鼓舞的性能，但这些方法无法很好地扩展到稠密的三维数据，因为计算和内存占用是随分辨率的立方增长的。为此，引入了层次结构和紧凑的图结构(如八叉树)来降低这些方法的计算和内存开销。Wang等人提出了一种基于八叉树的OctNet来进行三维形状分类。将在最细叶八分区采样的三维模型的平均法向量输入网络，将3D- cnn应用于被三维形状表面占据的八分区。与基于密集输入网格的基线网络相比，OctNet对高分辨率点云的内存和运行时间要求要少得多。leet等人提出了一种称为点网格的混合网络，它集成了点和网格表示来实现高效的点云处理。在每个嵌入式网格单元中采样的点的数量是恒定的，这使得网络可以通过使用3D卷积来提取几何细节。</p><h1 id="3-点基网络"><a href="#3-点基网络" class="headerlink" title="3    点基网络"></a>3    点基网络</h1><p>点基网络使用最原始的点作为网络的输入。由于其能保留更多的原始特征，点基网络成为当前3D点云识别的主流方法。该方法可以被分为<strong>点级多层感知机（Pointwise MLP）、卷积神经网络、图神经网络、基于数据索引网络（Data indexing-based networks）和其它神经网络</strong>。</p><h2 id="3-1-点级多层感知机"><a href="#3-1-点级多层感知机" class="headerlink" title="3.1 点级多层感知机"></a>3.1 点级多层感知机</h2><p>这些方法<strong>使用MLPs对每个点独立建模，然后使用对称函数聚合全局特征</strong>，如图所示。由于对称函数的存在，这类网络可以保证三维点云的无序性。然而，三维点之间的几何关系并没有得到充分的考虑。</p><p><img src="https://i.loli.net/2020/02/09/8O26rgyBF3i5UvJ.png" alt="FK_UYCNROC_~JTZ_`B`C_KU.png"></p><p>这个方向的开山之作是PointNet，其使用max-pooling聚合多个MLP提取的特征。该网络独立地提取每个点的特征，没有考虑到点之间的结构信息。分层网络PointNet++于此被提出用来捕获邻近点之间的几何结构信息。</p><p>由于其简单性和强大的表示能力，许多网络都是基于PointNet开发的。Achlioptas等人提出了一种深度自编码网络学习点云表示。它的编码器遵循PointNet的设计，使用5个一维卷积层、ReLU非线性激活、批处理标准化和最大池化操作独立地学习点特征。在PATs(Point Attention Transformers)中，每个点都由其绝对位置和相对于其相邻点的相对位置来表示,然后利用GSA(Group Shuffle Attention)来捕获点之间的关系，并建立了一个排列不变、可微、可训练的端到端GSS(Gumbel Subset Sampling)层来学习层次特征。Mo-Net的架构类似于PointNet，但它采用有限的一组矩作为其网络的输入。PointWeb也是在PointNet++上构建的，它使用局部邻居的上下文来使用自适应特性调整(Adaptive Feature Adjustment, AFA)改进点特性。Duan等人提出了一种结构关系网络(SRN)，利用MLP来学习不同局部结构之间的结构关系特征。Lin等人通过为PointNet学习的输入空间和函数空间构造一个查找表来加速推理过程。在中等大小的机器上，ModelNet和ShapeNet数据集上的推理时间加快了1.5 ms，是PointNet的32倍。SRINet首先投影一个点云来获得旋转不变量表示，然后利用基于PointNet的backbone来提取全局特征，并使用基于图的融合来提取局部特征。</p><h2 id="3-2-卷积网络"><a href="#3-2-卷积网络" class="headerlink" title="3.2    卷积网络"></a>3.2    卷积网络</h2><p>与二维网格结构(如图像)定义的卷积核相比，三维点云的卷积核由于点云的不规则性而难以设计。根据卷积核的类型，目前的三维卷积网络可以分为<strong>连续卷积网络和离散卷积网络</strong>，如图所示。</p><p><img src="https://i.loli.net/2020/02/09/YNDcs95ty12ig6p.png" alt="_OW`QQ_M_`4TFGIAJRAWGHW.png"></p><h3 id="3-2-1-3D连续卷积网络"><a href="#3-2-1-3D连续卷积网络" class="headerlink" title="3.2.1    3D连续卷积网络"></a>3.2.1    3D连续卷积网络</h3><p><strong>3D连续卷积在连续空间上定义卷积核，其邻近点的权重与中心点的空间分布有关。</strong>3D连续卷积可以被解释为点子集的加权线性组合，通常使用MLP学习每个点的权重。</p><p>例如 RS-CNN 的关键层 RS-Conv 需要点的子集在某种程度上作为其输入，使用MLP学习局部点之间从低级关系（如Euclidear距离和相对位置）到高级关系之间的映射来实现卷积。</p><p>一些方法还使用现有的算法来执行卷积。在PointConv中，卷积被定义为相对于重要抽样的连续三维卷积的蒙特卡罗估计。卷积核由一个加权函数(通过MLP层学习)和一个密度函数(通过核化密度估计和MLP层学习)组成。为了提高记忆和计算效率，将三维卷积进一步简化为矩阵乘法和二维卷积两种运算。在相同的参数设置下，其内存消耗可减少约64倍。类似的网络还有MCCNN、SpiderCNN、PCNN、KPConv。</p><p>而一些方法则被提出用于解决三维卷积网络所面临的旋转等变问题。Esteves等人提出了以多值球函数为输入，学习旋转等变表示的三维球面卷积神经网络(Spherical CNN)。利用球面谐域内的锚点对谱进行参数化，得到局部卷积滤波器。张量场网络(Tensor field networks)被提出用于定义点积运算，它是一个可学习的径向函数和球面调和函数的乘积，这两个函数对于点的三维旋转、平移和排列是局部等价的。SPHNet以PCNN为基础，通过在体积度量函数的卷积过程中加入球谐核实现旋转不变性。</p><h3 id="3-2-2-3D离散卷积网络"><a href="#3-2-2-3D离散卷积网络" class="headerlink" title="3.2.2    3D离散卷积网络"></a>3.2.2    3D离散卷积网络</h3><p><strong>3D离散卷积在规则网格上定义卷积核，其中相邻点的权值与相对于中心点的偏移量相关。</strong></p><p>Hua等人将非均匀三维点云转化为均匀网格，并在每个网格上定义卷积核。与2D卷积(为每个像素分配权重)不同，3D卷积核为落入相同网格的所有点分配相同的权重。对于给定的点，位于同一网格上的所有相邻点的平均特征是从上一层计算出来的。然后对所有网格的平均特征进行加权求和，得到当前层的输出。Lei等人定义了一个球面卷积核，方法是将一个三维球面邻近区域划分为多个容量容器，并将每个容器与一个可学习的加权矩阵相关联。一个点的球面卷积核的输出由相邻点的加权激活值的平均值的非线性激活决定。类似的神经网络还有GeoConv、PointCNN、InterpConv、RIConv、A-CNN、ReLPV、SFCNN等。</p><h2 id="3-3-图神经网络"><a href="#3-3-图神经网络" class="headerlink" title="3.3    图神经网络"></a>3.3    图神经网络</h2><p><strong>基于图的网络将点云中的每个点视为一个图的顶点，并基于每个点的邻居为图生成有向边。然后在空间或光谱域中进行特征学习。</strong>典型的基于图的网络如图所示。</p><p><img src="https://i.loli.net/2020/02/09/JkrwP75SA2TaXCd.png" alt="MLVH~H__VJDM32NKGZ4_7LX.png"></p><h2 id="3-4-基于数据索引的网络"><a href="#3-4-基于数据索引的网络" class="headerlink" title="3.4    基于数据索引的网络"></a>3.4    基于数据索引的网络</h2><p>这类网络是<strong>基于不同的数据索引结构(如八叉树和kd-tree)构建的。在这些方法中，点特征由沿着树从叶节点到根节点的层次学习得到。</strong>其典型代表作有Kd-Net、3DContextNet、SO-Net等。</p><h1 id="4-性能对比"><a href="#4-性能对比" class="headerlink" title="4    性能对比"></a>4    性能对比</h1><p><img src="https://i.loli.net/2020/02/09/lzaiosyh2jkuqr9.png" alt="ST8DMA9EJ2DE__N48_9HN@F.png"></p><p>大概可以看出基于卷积的GeoCNN表现最佳。</p><p>参考论文：Deep Learning for 3D Point Clouds: A Survey</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-摘要&quot;&gt;&lt;a href=&quot;#1-摘要&quot; class=&quot;headerlink&quot; title=&quot;1    摘要&quot;&gt;&lt;/a&gt;1    摘要&lt;/h1&gt;&lt;p&gt;3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="3D点云" scheme="http://a-kali.github.io/tags/3D%E7%82%B9%E4%BA%91/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multi-level Attention network using text, audio and video for Depression Prediction</title>
    <link href="http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/"/>
    <id>http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/</id>
    <published>2020-02-05T02:18:16.000Z</published>
    <updated>2020-04-13T02:58:01.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一种常见的心理疾病，严重影响心理和身体健康，甚至可能导致失去生命。由于缺乏诊断抑郁症的有效手段，越来越多的人对通过行为线索来自动诊断以及阶段预测抑郁症感兴趣。摘要提出了一种<strong>基于多级注意的多模态抑郁症预测网络</strong>，该网络融合了音频、视频和文本模式的特征，同时学习模式内和模式间的相关性。多层次的注意力通过选择每个模式中最具影响力的特征来加强整体学习。我们进行了详尽的实验，为音频、视频和文本模式创建不同的回归模型。建立了几种不同构型的融合模型，分析了各特征和模态的影响。就均方根误差而言，我们比当前基准高出17.52%。</p><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2    Related Works"></a>2    Related Works</h1><p>在[28]中，作者对融合技术在抑郁症检测中的应用进行了综述。他们还提出了一种基于计算语言学的多模态融合检测方法。在[25]中，作者基于上下文感知特征生成技术和端到端可训练的深度神经网络对临床访谈语料库数据集的抑郁水平进行了分析。他们进一步在变压器网络中注入基于主题建模的数据增强技术。Zhang等人发布了一个用于人类行为分析的多模态自发情感语料库[44]。面部表情由3D动态测距、高分辨率视频采集和红外成像传感器捕捉。除了面部环境，还会监测血压、呼吸和脉搏率，以判断一个人的情绪状态。利用AVEC挑战中发布的数据，对音频、视频和生理参数进行调查，以观察受试者情绪状态的关键发现。在[30]中，作者融合了音频、视觉和文本线索来获取多媒体内容中的情感。他们利用特征和决策级融合技术来进行有效的决策。在[2]中，作者利用副语言、头部姿势和眼睛凝视进行多模式抑郁检测。在选定特征的统计检验的帮助下，推理机将受试者分为抑郁和健康两类。当结合多种模式时，了解每种模式在任务预测中的贡献是很重要的，注意力网络可以用来研究其相对重要性。在这篇论文中，我们在每个情态中使用注意来理解情态中低层或深层特征的相对重要性。在融合三种模式的同时，我们还使用了注意力层，并学习了注意力的权重，从而找到每种模式的重要性比例。Querishi等人的论文[32]是唯一一篇在使用数据集子集和在一层应用注意力方面最接近我们的论文。通过在多个层次上使用多层注意力，我们能够获得比它们更好的结果，并且由于注意力操作，网络的计算成本更低，从而最小化了框架的测试时间。</p><h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3    Methodology"></a>3    Methodology</h1><h2 id="3-1-Text-Modality"><a href="#3-1-Text-Modality" class="headerlink" title="3.1    Text Modality"></a>3.1    Text Modality</h2><p>由于有几个参与者使用的是英语口语词汇，所以我们对这些词汇进行了修改，用原始的完整词汇替换这些词汇，否则，在训练语言建模或其他预测的神经网络时，这些词汇就会变成不存在于词向量中的词汇。我们<strong>使用预训练的通用语句编码器（Universal Sentence Encoder）来得到Embedings</strong>。为了获得常数大小的张量，我们使用零补齐短句，并有一个常数数量的时间步为400。每个语句嵌入向量的长度为512，使得最后的数组维数为(400,512)。我们使用两层叠加的双向长短时记忆网络结构，以Embeding为输入，以PHQ分数为输出，训练语音转录的回归模型。每个BLSTM层有200个隐藏单元，其中第一层的前向层的每个隐藏单元的输出连接到第二层的前向隐藏单元的输入。同样的连接也建立在每一个隐藏单元的后向层，以创建堆叠。这两层BLSTM在每一步的输出为(batchsize,400)，并将其作为输入发送到前馈层进行回归。我们保持前馈层的节点数为(500,100,60,1)，并使用ReLU作为激活函数。</p><h2 id="3-2-Audio-Modality"><a href="#3-2-Audio-Modality" class="headerlink" title="3.2    Audio Modality"></a>3.2    Audio Modality</h2><p>对于音频模态，我们使用不同的音频特征(低级特征及其功能)创建模型。在功能(functional)方面，变分的算术平均值和协效率被应用于底层特征，并作为底层特征之上的知识抽象。语音音色是由诸如梅尔频率倒谱系数(MFCC)这样的LLD特征编码的，研究表明，低阶MFCC在情感预测和副语言语音分析任务中更重要。eGeMAPS特征集包含88个特征，包括GeMAPS特征集及其频谱特征和功能。GeMAPS特性包括频率相关特性(音高、抖动、共振峰)，能量相关功能(微光，响度，谐波噪声比)，光谱参数(α比，哈马伯格比)，谐波以及六个时序特征相关的语音比率。除了上述的这些低电平特征外，音频样本的高维深度表示是通过将音频通过深度频谱和VGG网络来提取的。这一特征在本文的其余部分被称为深度densenet特征。</p><p>对于音频特征，我们的实验只考虑了参与者所说的向量的跨度。每个特征都是挑战数据的一部分，它们有不同的采样率。功能性音频和深度densenet特征采样频率为1Hz，而BoAW采样频率为10Hz，LLD采样频率为100Hz。低级MFCC特征长度为39，低级eGEMAPS长度为23，总时间步长为140500。而功能的长度分别为78和88，时间步长分别为1300和1410。BoAW-MFCC和BoAW-eGEMAPS的特性长度为100，每个特性的时间步长为14050。深度densenet特征长度为1920维，时间步长为1415步。</p><p>在单独的音频通道，我们训练了另一个二层的BLSTM网络，每层有200个隐藏单元。我们将最后一层的输出传递给多层感知器，每一层(500,100,60,1)个节点，并以ReLU作为激活函数。</p><h2 id="3-3-Visual-Modality"><a href="#3-3-Visual-Modality" class="headerlink" title="3.3    Visual Modality"></a>3.3    Visual Modality</h2><p>对于挑战数据集中的视频特征，我们对LLD及其function进行了实验。由于深度LSTM网络也可以从数据(如function和更抽象的信息)中学习类似的特征，所以我们选择使用LLD，因为它包含的信息比其平均值和标准差更多。每个用于姿态、注视和面部动作单元(FAU)的LLD特征都以10Hz采样。这些特征的长度分别为6、8、35，都有15000个时间步长。在挑战数据中还提供了BoVW，它的长度为100，有15000个时间步长。我们使用这些特性来训练每个特征的模型，所有特征都使用一个包含200个隐藏单元的BLSTM单层，然后是一个maxpooling和回归器。我们尝试了各种不同的组合，比如所有输出的和、输出的平均值，还使用了maxpooling作为三个备选方案，但是maxpooling效果最好，所以我们在LSTM输出上使用了maxpooling。</p><h2 id="3-4-Fusion-of-Modalities"><a href="#3-4-Fusion-of-Modalities" class="headerlink" title="3.4    Fusion of Modalities"></a>3.4    Fusion of Modalities</h2><p>早期融合(early fusion)需要消耗大量的计算资源，当使用神经网络训练时可能导致过拟合，因此，<strong>后期融合(late fusion)和混合融合(hybrid fusion)</strong>模型变得更加普遍。我们提出了一种<strong>基于多级注意力机制的网络，该网络学习每个特征的重要性，并对它们进行相应的加权，从而实现更好的早期融合和预测</strong>。这样的注意力网络让我们了解到哪种模态的特征对学习更有影响。它还能表示出各个模态对预测结果准确度所做出的贡献比例。</p><p>在融合的过程中，我们在每个通道之间进行了多次实验。首先，我们融合了视频模态的LLD。我们获取注视、姿势和面部动作单元特征，将它们通过包含200个BLSTM神经元的单层网络传递进行前向传播，并对它们施加注意力。注意层的输出通过另一个具有200个单元的BLSTM层。我们对LSTM的输出进行全局最大池化，并通过128个隐藏单元的网络前向传播。<strong>(BLSTM-&gt;Attention-&gt;BLSTM-&gt;Maxpooling-&gt;FC)</strong></p><p>在第二个模型中，我们使用一个类似的网络将视频LLD与BoVWs相结合，该网络由包含200个隐藏单元的BLSTM层、注意力层和另一个BLSTM组成，然后通过一个前馈层进行回归。</p><p>第三个融合模型将视频模态的注意向量输出和文本模态的输出进行合并(combine)，并在视频回归器之前通过一个堆叠的BLSTM和一个注意力层。</p><p>第四个融合模型同时使用音频和文本模态。我们再次在每个模态中获取注意层的输出，并构建一个混合融合网络，但通过两条路径传递它们。在第一种路径中，每个模态的注意力层输出被拼接起来输入注意力层；另一条路径通过两个注意层的输出通过一个堆叠的BLSTM，该BLSTM有2层，每层有200个单元。在堆叠的BLSTM层上应用一个注意层，该输出被馈送到128个隐藏单元的前馈网络。由于使用了文本特征，因此比单独使用音频特征的模型具有更好的性能。</p><p>我们的第五个融合模型同时使用视频和文本模态，这里我们再次在视频的每个子模态上使用注意力层，然后使用另一个注意网络将它与文本模态合并。令人惊讶的是，这种融合的结果与音频和文本模态融合非常相似，学习曲线也非常相似。</p><p>我们的第六个也是最终融合模型使用了所有的模态。我们使用基于注意力的视觉模态来获得一个包含128个单元的向量，同时使用基于注意力的音频模态来获得一个128单元向量，随后从文本模态中提取信息并从中得到128位(bit)的向量。我们再次在这三种模态上使用另一个注意力层，将它们融合在一起，回归到PHQ8评分。收敛后，视频、音频和文本的注意率分别为[0.21262352, 0.21262285, 0.57475364]。</p><p><img src="https://s1.ax1x.com/2020/04/08/GWzDvq.png" alt="GWzDvq.png"></p><h1 id="4-Results"><a href="#4-Results" class="headerlink" title="4    Results"></a>4    Results</h1><p>本节详细介绍了所有回归模型的结果及其融合研究。由于测试数据的标签在挑战中不可用，所以我们在验证(dev)分区上展示了大部分结果。测试分区上的唯一结果来自基于文本的模型，我们使用该模型提交了一份报告，并从挑战中获得了所有的分数。</p><h2 id="4-1-Results-from-Text-Modality"><a href="#4-1-Results-from-Text-Modality" class="headerlink" title="4.1    Results from Text Modality"></a>4.1    Results from Text Modality</h2><p>在E-DAIC(挑战数据)和DAIC-WoZ数据的测试集上，基于注意力的BLSTM网络在文本方面的训练效果优于其他模态。这与临床医生的观察一致，即<strong>语言内容是一个重要的标志，具有明确的特征，可能直接影响患者处于哪个抑郁阶段</strong>。我们在验证集的根均方误差(RMSE)为4.37 。在测试集上，基于文本的模型能够实现平均绝对误差(MAE)为4.02，RMSE为4.73，对应的一致性相关系数(CCC)为0.67，CCC是该比赛中的主要评估系数。该模型的皮尔逊相关系数(PCC)为0.676，决定系数(r2)为0.457，根据对测试集的挑战结果，斯皮尔曼相关系数(SCC)为0.651。总体而言，该网络工作优于现有模型的8.95%。代码在15个epoch时收敛，验证损失为4.37，批处理大小为10，这些参数是根据经验选择的。预测单个文本抄写的平均测试时间为0.09秒。</p><h2 id="4-2-Results-from-Audio-Modality"><a href="#4-2-Results-from-Audio-Modality" class="headerlink" title="4.2    Results from Audio Modality"></a>4.2    Results from Audio Modality</h2><p>与基线模型相比，我们每个单独的网络在RMSE方面都表现出色。在基于音频MFCC特征的模型中，我们的表现比基线高出29.80%，而在eGEMAPS中，我们的表现则高出29.04%。对于BoAW-MFCC，我们的表现比基准高出10.44%，而对于BoAW-eGE，我们的表现比基准高出14.46%。每个单独的音频特性代码运行15个epoch，批处理大小为10。使用Functional MFCC对一个样本的平均时间要求为0.23秒，使用Functional eGEMAPS为0.14秒，使用BoAW-MFCC为0.45秒，使用BoAW-eGE为0.45秒，DS-DNet特征为0.13秒。对于音频模型，我们尝试了一个卷积神经网络架构来融合MFCC、eGEMAPS和DS-DNet特性，但是我们发现Bi-LSTMs的性能略好于卷积网络，其对序列特征有着更好的学习能力。</p><h2 id="4-3-Results-from-Visual-Modality"><a href="#4-3-Results-from-Visual-Modality" class="headerlink" title="4.3    Results from Visual Modality"></a>4.3    Results from Visual Modality</h2><p>视频特征的结果好于基线和技术水平，但仍比文本和语音模式的结果差。在视觉特征中，BoVW的表现最好，超出基线4.8%。与[32]相比，我们使用姿势特征的表现要比他们好9.3%，使用凝视的表现要比他们好6.6%，使用面部动作单元的表现要比他们好8.7%。</p><h2 id="4-4-Results-from-Fused-Modalities"><a href="#4-4-Results-from-Fused-Modalities" class="headerlink" title="4.4    Results from Fused Modalities"></a>4.4    Results from Fused Modalities</h2><p>该模型使用了所有的特征，融合了多层次的注意力，得到了超出baseline 17.52%的结果。与[32]相比较，音频-文本融合网络和视频-文本融合网络的性能分别提高了5.8%和9.19%，而全融合网络的性能略差。这不是结论性的，因为本文使用的数据集略有不同。注意机制自动权衡每种模式中的每个特征，并允许网络关注回归决策中最重要的特征。这样，网络就能了解特征与PHQ-8 scores之间的关系。</p><h1 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5    Discussion"></a>5    Discussion</h1><p>本文提出了一种基于多级注意力的早期融合网络，融合音频、视频和文本模式来预测抑郁症的严重程度。在这项任务中，我们观察到注意力网络给予文本情态的权重最高，给予音频和视频情态的权重几乎相等。给予特定词更高的权重是与临床医生一致的，因为言语内容对诊断抑郁水平至关重要。音频和视频是同样重要的信息来源，对预防严重程度至关重要。我们对视频数据重要性较低的直觉是，我们可以从视频模态(眼球注视、面部动作单元和头部姿势)中使用有限的特征。临床医生在面对面的访谈中可以观察一个人的身体姿势(自我接触颤抖等)或记录电生理信号，从而进行诊断。</p><p>多层次注意力的使用使我们在所有个体和融合模型中获得了比基线和技术水平更好的结果。把注意力放在每个特征和形态上总体上有两方面的优势。首先，这让我们更深入和更好地理解每个特征在抑郁症预测中的重要性。其次，简化了网络的整体计算复杂度，减少了训练和测试时间。实验结果表明，基于多水平注意力的全特征融合模型较基线有17.52%的提高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一
      
    
    </summary>
    
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁检测" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multimodal Fusion with Deep Neural Networks for Audio-Video Emotion Recognition</title>
    <link href="http://a-kali.github.io/2020/02/03/%E3%80%90%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%91Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/"/>
    <id>http://a-kali.github.io/2020/02/03/【论文解读】Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/</id>
    <published>2020-02-03T13:07:57.000Z</published>
    <updated>2020-04-07T13:53:09.586Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：本文提出了一种<strong>新的音频、视频和文本多模态融合用于情绪识别的深度神经网络</strong>。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平上进行早期模式融合的先进系统相比，我们提出的DNN可以实现更高水平的<a href="http://blog.sina.com.cn/s/blog_1859648c00102ylcy.html" target="_blank" rel="noopener"><strong>一致性相关系数(Concordance Correlation Coefficient, CCC)</strong></a>。该DNN在数据集开发分区上分别获得了0.606、0.534和0.170的CCCs，分别为唤起度(arousal)  、效价(valence)和喜欢度(liking)。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>最先进的传感器捕捉音频和视频信号，这为许多创新技术铺平了道路，这些技术可以实现非接触式的监测和诊断。例如，它们可以对个人进行持续的健康监测，这对于治疗和管理范围广泛的慢性疾病、神经障碍和精神健康问题，如糖尿病、高血压、哮喘、自闭症谱系障碍、疲劳、抑郁、药物成瘾等，变得越来越重要。</p><p>人们普遍认为，未来的人类环境(家庭、工作场所、公共交通等)将包含智能传感器阵列，可以支持和预测所需的行动，以一种普遍和不显眼的方式最佳地自我调节心理状态。识别个人情感状态的技术，特别是图像/视频和语音处理技术以及机器学习技术，有望在这一未来愿景中发挥关键作用。</p><p>然而，尽管有精密的传感器和技术，如何在真实场景精准识别仍然是一项挑战。首先，稳定地捕获时空信息，并为种群中的每个表达提取共同的可有效编码的时间特征同时抑制特定主题(类内)的变化是很难的。根据特定的个体行为和捕捉条件，面部表情和言语表达具有显著的时空变化。此外，创建具有代表性的大型音频视频数据集，并根据需要提供可靠的专家注释，以设计识别模型，并准确检测唤醒和效价水平是非常昂贵的。</p><p>在本文中，我们认为动态表情识别技术能够使用多种不同的模式准确地评估对象的时序情绪状态。我们提出了一种深度神经网络(DNN)结构，用于语音、人脸和文本信息的多模态融合，用于音视频情感识别。为了准确识别，本文提出的DNN提供了一个中间层次的融合，其中特征、分类器和融合函数以端到端的方式进行全局优化。</p><p>论文结构如下。第二部分介绍了文献中提出的针对不同模式的情绪识别技术。第三节介绍了多模态融合的DNN体系结构。第四部分给出了实验协议，第五部分给出了在RECOLA数据集上进行的实验，以及针对音频、视频和文本三种模式的实验结果，以及它们的融合。最后，在最后一节中对未来的工作进行了总结和展望。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>在过去的十年中，面部表情识别(ER)一直是人们非常感兴趣的话题。人们提出了许多ER技术，可以从一张静止的面部图像中自动识别出七种常见的情绪类型——喜悦、惊讶、愤怒、恐惧、厌恶、悲伤和中性。这些面部ER的静态技术往往是基于外观或基于几何图形的方法。最近，动态面部表情识别技术已经成为一种提高性能的方法，其中表情类型是通过在一个对象的物理面部表情过程中所捕获的一系列图像或视频帧来估计的。这不仅可以在空间域提取人脸的外观信息，还可以在时间域提取人脸的演化信息。该技术可以是<strong>基于形状、基于外观或基于动作</strong>的方法。</p><p>基于形状的方法，如约束局部模型(constrained local model, CLM)，基于突出锚点标记来表示面部特征形状，这些标记的运动为识别过程提供了判别信息；基于外观的方法，如LBP-TOP从面部图像中提取图像强度或其他纹理特征来表示面部表情；基于运动的自由变形模型等面部表情时空演化方法，需要可靠的人脸定位方法。例如，Guo等人使用atlas构造和稀疏表示从动态表达式中提取时空信息。虽然计算复杂度较高，包括时间信息和空间信息，但与静态图像相比，具有更高的识别精度。</p><p>在过去的几年里，自动检测说话人的情绪状态变得越来越流行。为了提高抑郁检测和情绪检测系统的准确性，人们探索了不同的方法。这两个方向有一些相似之处。从抑郁检测方面，France等人已经表明，格式频率的变化是抑郁和自杀倾向的良好指标。Cummings等人利用能量和光谱特征对抑郁症(抑郁症和非抑郁症)进行了二分类，其准确率约为70%。Moore等人通过统计分析(平均值、中位数和标准偏差)音高、能量或说话速度等韵律特征，获得了75%的准确率。就像几乎所有涉及到机器学习技术的领域一样，神经网络在情绪检测中的应用已经变得非常流行。研究人员通常使用DNNs、长短时记忆神经网络(LSTMs)和卷积神经网络(CNNs)。</p><p>包括以往AVEC比赛结果在内的多项情绪或减压检测研究的证据表明，通过整合来自多个不同信息源的证据(主要在特征、得分或决策层面)，可以提高识别系统的准确性和可靠性。因此，人们最近对通过多模态融合来检测情绪状态，特别是语音和面部模式的检测产生了一些兴趣。Kachele等人提出了一种分级多分类器框架，该框架通过引入不同程度的确定性来自适应地组合输入模式。声音韵律和面部动作单元也被用来检测抑郁。Menget等人提出了一种利用运动历史直方图动态特征从音频和视频中识别抑郁症的多层系统。Nasir等人提出了一种多模态特征，在多分辨率建模和融合框架中捕捉抑郁行为线索。特别地，他们利用Teager基于能量和i-vector的特征，连同音素率和持续时间来预测音频，并在视频中使用多项式参数化的时间变化和从面部地标获得的区域特征。最后，Williamson等人提出了一种很有前途的系统，该系统利用语音、韵律和面部动作单元特征的互补多模态信息来预测抑郁症的严重程度，尽管每种模态的贡献并没有被讨论。</p><p>尽管有精密的传感器和强大的技术，但在现实世界场景中开发精确的情绪识别模型仍面临一些挑战。在设计过程中，代表性数据的数量是有限的。假设识别模型是使用从特定条件下提取的有限数量的标记参考样本设计的。虽然许多音视频信号可以被捕获来设计识别模型，但它们需要昂贵的专家注释来创建大规模的数据集来检测唤醒和价态水平。在操作过程中，在操作域(即、受试者的办公室、家庭等)，在各种情况下。根据特定的捕捉条件和个体行为，面部和语音表情会随着时间发生显著的动态变化。因此，识别模型并不能代表操作域内模式的类内变化。实际上，任何分布上的变化(无论是域移位还是概念漂移)都会降低系统性能。识别模型需要对特定人员、传感器和计算设备以及操作环境进行校准和调整。</p><p>众所周知，随着时间的推移，结合来自不同模式的时空信息可以提高鲁棒性和识别精度。模式也可以根据上下文或语义信息动态组合，例如记录环境中的噪声。例如，可以根据捕获条件在不同的层(分辨率)组合深度学习架构的输出响应。</p><p>本文主要研究利用深度学习体系结构来生成精确的混合情感识别系统。例如，Kim等人提出了一种层次化的3级CNN结构来组合多模态源。DNNs被认为是学习具有特定目标的转换序列，以获得将在一个系统中组合的特征。由于特征级和分数级融合不一定能获得较高的精度，因此提出了一种混合方法，通过学习特征和分类器来优化多模态融合。</p><h1 id="3-Multimodal-Fusion-with-Deep-NNs"><a href="#3-Multimodal-Fusion-with-Deep-NNs" class="headerlink" title="3    Multimodal Fusion with Deep NNs"></a>3    Multimodal Fusion with Deep NNs</h1><p>本文提出了一种高效的的DNN结构，该结构可以从多个信息源中学习受试者行为与情绪状态之间的映射。对于给定的时序面部和语音模态（包括文本信息）的AVEC SEWA 数据库，本系统旨在从中学习特征表示、分类和融合以对影本进行准确地预测。</p><p>该方法的主要内容在于联合学习每一个判别表示，以及它们的分类和融合函数。每个特征子集首先由一个或多个隐藏层独立处理。网络的这一部分学习给定模态的最佳特征表示。然后，每个块的最后隐藏层相互连接到一个或多个完全连接的层，这些层将进行特征的分类和融合。从全局的角度来看，这个网络应该学习如何转换输入特征，从而进行分类和融合，并产生一个全局决策。训练一个混合分类器来组合这些特征可以提高识别系统的整体精度。该架构使用了三种不同的信息来源——音频（语音）、视频（脸部）和文本。有关特性集的详细信息，请参阅[20]。</p><p>1)音频特征：声学特征由23个声学LLD组成，如能量、频谱和倒频谱、音高、音质和微韵律特征，每10ms的短帧提取一次。使用一个包含1000个音频单词的编码书，在6秒的时间段里计算分段级声学特征，并创建音频单词的直方图。因此，得到的特征向量有1000维。</p><p>2)视频特征：每帧（帧步20ms）提取视频特征，包括三种特征类型：按度数归一化的人脸方向；10个视点的像素坐标；49个面部标志的像素坐标。每个带有独立码本和直方图的BoVW被创建为三个面部特征类型，每个码本的大小为1000，产生一个3000维的节段级特征向量。</p><p>3)文本特征：文本特征是基于语音转录的词袋特征表示。该词典包含521个单词，其中只考虑了一元语法（unigram）。在一段6秒的时间内创建的直方图。总的来说，文本包(BoTW)包含521个特征。</p><h2 id="3-1-DNN-Architecture"><a href="#3-1-DNN-Architecture" class="headerlink" title="3.1    DNN Architecture"></a>3.1    DNN Architecture</h2><p>所提出的多模态融合的DNN体系结构如图所示。DNN对每个模态使用一对全连接层分别处理音频、视频和文本，以生成和探索同一类型特征之间的相关性。然后，第二阶段(b)合并这些独立层的输出。该层在一个块中接收前一步的输出，并提供一个与模式的本质相关联的全连接层(c)。DNN输出由单个线性神经元(d)产生，作为整个网络的回归值。最后，利用标度模(e)来缩小预测量与标签量之间的差距。不同的标度函数被认为是最好的结果-小数标度，最小最大归一化和标准差标度。最终的预测(f)通过线性函数(前一层的加权和)产生。</p><p><img src="https://i.loli.net/2020/02/04/AMvSXGaWkoDQr1d.png" alt="E@CC6K8WWH9YLDBW7UAALCT.png"></p><h1 id="4-Experimental-Methodology"><a href="#4-Experimental-Methodology" class="headerlink" title="4    Experimental Methodology"></a>4    Experimental Methodology</h1><p>RECOLA数据集包括一个训练集和一个开发集(development set)。在本研究的实验中，开发集被分为两个子集。第一个包含从开发集的14份数据中随机选择的5个数据。剩下的数据作为测试集来评估方法的性能。</p><p>在SVR实验中，原始提案被用于建立单峰和早期融合系统。后融合方法使用第一个开发子集来优化每个单模系统。使用第二个开发子集对融合函数进行了优化。</p><p>在DNN的情况下，更小的开发集被用来确定每个模式的最佳层数和神经元数，以及融合层的神经元数。第二个开发子集用于测试模型。注意，每个情感维度都有自己的体系结构，如表所示。</p><p>本次挑战采用的评价指标为CCC，其定义为:</p><p><img src="https://i.loli.net/2020/02/04/VKebGpFvu2qTNLd.png" alt="_1VUA87A@T_2AO9_Y9_4_OI.png"></p><h2 id="4-1-Preprocessing"><a href="#4-1-Preprocessing" class="headerlink" title="4.1    Preprocessing"></a>4.1    Preprocessing</h2><p>当应用延迟补偿函数时，CCC评分的结果有显著的提高。图2显示了CCC随延迟的变化。</p><p><img src="https://i.loli.net/2020/02/04/p9Gat8BS4V3ud1s.png" alt="Y__AZX_RMMEY_8~AES`42H3.png"></p><h1 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5    Conclusion and Future Work"></a>5    Conclusion and Future Work</h1><p>本文提出了一种新的DNN结构来预测情绪状态。它融合了三种不同的模态：音频信号、面部特征和音频信号的对话文本。每个模态首先由两个全连接层独立编码，然后合并成一个单一的表示，然后用于估计主体的情绪状态。该网络以端到端方式训练，提供比其他提出的架构更高的CCC。通过对输入特征进行适当的归一化，并对回归的输出进行时间平滑处理，可以期望得到进一步的改进。对于未来的工作，我们计划扩展我们的工作，包括一个基于递归神经网络的最后阶段，它可以学习情绪的时间模式，从而提高整个系统的准确性。此外，我们还将评估从明确训练的卷积神经网络中提取的视觉特征的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要：本文提出了一种&lt;strong&gt;新的音频、视频和文本多模态融合用于情绪识别的深度神经网络&lt;/strong&gt;。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="多模态" scheme="http://a-kali.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（14）—— 如何选刊</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8814%EF%BC%89%E2%80%94%E2%80%94-%E5%A6%82%E4%BD%95%E9%80%89%E5%88%8A/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（14）——-如何选刊/</id>
    <published>2020-02-03T05:10:55.000Z</published>
    <updated>2020-02-03T05:49:36.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何确定你的目标期刊？"><a href="#如何确定你的目标期刊？" class="headerlink" title="如何确定你的目标期刊？"></a>如何确定你的目标期刊？</h1><ul><li>期刊的类型、领域范围</li><li>期刊的声望和影响力（影响因子、引用率）</li><li>审稿和发表程序</li></ul><h2 id="1-期刊的目标和范围"><a href="#1-期刊的目标和范围" class="headerlink" title="1    期刊的目标和范围"></a>1    期刊的目标和范围</h2><p>回答三个问题：</p><ol><li>研究领域是否相关？</li><li>你的论文类型是否在该期刊发表过？</li><li>期刊的读者群体是什么？（专业/综合、亚洲/欧洲 等）</li></ol><h2 id="2-期刊的声望和影响力"><a href="#2-期刊的声望和影响力" class="headerlink" title="2    期刊的声望和影响力"></a>2    期刊的声望和影响力</h2><p>四个问题：</p><ol><li>同行评价/阅读人数如何？</li><li>期刊的影响因子是否满足你的要求？</li><li>期刊收录的数据库有哪些？</li><li>期刊的出版形式？（在线/印刷）</li><li>期刊的委员会成员、赞助者是否知名？</li></ol><h2 id="3-审稿和发表程序"><a href="#3-审稿和发表程序" class="headerlink" title="3    审稿和发表程序"></a>3    审稿和发表程序</h2><ol><li>期刊的出版频率（月刊/半月刊/季刊）</li><li>发表周期（一审/二审/发表）</li><li>接收后见刊时间（即发表到刊物上）</li><li>发表费用（版面费、彩图费是否合理？能否报销？）</li><li>是否可以公开获取（开放权限），多久能公开获取（数据库收录）</li></ol><h1 id="查找合适的投稿期刊"><a href="#查找合适的投稿期刊" class="headerlink" title="查找合适的投稿期刊"></a>查找合适的投稿期刊</h1><ol><li><strong>从每年的期刊影响因子中查找</strong>：每年的6月份左右Thomson都会发布上一年所有SCI期刊的影响因子，通过这些可以了解本专业领域的顶级期刊，选择合适自己的期刊。</li><li><strong>从参考文献中获得合适的期刊</strong>：统计在撰写论文过程中阅读的大量文献来自于哪些期刊，再根据自己论文的质量选出合适的期刊。</li><li><strong>询问同行或者导师</strong>。</li><li><strong>向期刊编辑或者主编咨询</strong>。 </li><li><strong>借助选刊工具</strong>：<ol><li>Edanz Journal Selector</li><li>Elsevier Journal Finder</li><li>Journal Article Name Estimator</li><li>Springer Journal Selector</li><li>MedSci期刊选择智能支持系统</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;如何确定你的目标期刊？&quot;&gt;&lt;a href=&quot;#如何确定你的目标期刊？&quot; class=&quot;headerlink&quot; title=&quot;如何确定你的目标期刊？&quot;&gt;&lt;/a&gt;如何确定你的目标期刊？&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;期刊的类型、领域范围&lt;/li&gt;
&lt;li&gt;期刊的声望和影响
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（13）—— 写作语句链接</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8813%EF%BC%89%E2%80%94%E2%80%94-%E5%86%99%E4%BD%9C%E8%AF%AD%E5%8F%A5%E9%93%BE%E6%8E%A5/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（13）——-写作语句链接/</id>
    <published>2020-02-03T03:59:33.000Z</published>
    <updated>2020-02-03T04:49:42.992Z</updated>
    
    <content type="html"><![CDATA[<p>在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括<strong>连接词和副词</strong>。</p><p>根据语句的前后关系，连接词可以分为7类：</p><ul><li><p><strong>因果关系</strong>；常用连接词：therefore, consequently, thus, hence, as a result, indeed等，用来对前句的内容进行总结。</p></li><li><p><strong>并列关系</strong>；常用连接词：also, likewise, besides in addition, moreover, furthermore等，用来连接多个同等事物或特征。</p></li><li><p><strong>相反关系</strong>；常用连接词：in contrast, but, however, yet, on the other hand, surprisingly, nevertheless, instead of等，用来描述某些设想与文献描述不符的现象或者结果。</p></li><li><p><strong>相似关系</strong>（类似于并列关系）；常用连接词：similarly, likewise，用于表达两个功能或者结果具有相似性。</p></li><li><p><strong>举例关系</strong>；常用连接词：for example, for instance, specifically, such as including等。</p></li><li><p><strong>时间关系</strong>；如：later等。</p></li><li><p><strong>顺序关系</strong>（类似于时间关系）；常用连接词：then, next, finally, first, second等。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括&lt;strong&gt;连接词和副词&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据语句的前后关系，连接词可以分为7类
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（11）—— 实在憋不出来咋办</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8811%EF%BC%89%E2%80%94%E2%80%94-%E5%AE%9E%E5%9C%A8%E6%86%8B%E4%B8%8D%E5%87%BA%E6%9D%A5%E5%92%8B%E5%8A%9E/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（11）——-实在憋不出来咋办/</id>
    <published>2020-02-03T03:19:11.000Z</published>
    <updated>2020-02-03T03:56:54.403Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><p>本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。</p><p><strong>去哪里“抄”？</strong></p><ul><li>Google 图书、图书馆（书籍）</li><li>维基百科（定义、性质）</li><li>论文文献（Introduction、Methods、Discussion、Figure legend）：抄语法、词组，修改主语等。</li></ul><p><strong>查重</strong>：论文写完后，一定要去做查重。如果被审稿人查出重复率较高会退回重改，影响发表时间。查重服务可以来自于万方等数据库，或者淘宝、闲鱼商家提供的服务。</p><p><strong>语法、词汇差错</strong>：</p><ul><li>易改软件</li><li>英语润色服务</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;p&gt;本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;去哪里“抄”？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google 图书、图书馆（书籍）&lt;/li&gt;
&lt;li&gt;维基百科（定义、性质
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（3）—— Figure Legend</title>
    <link href="http://a-kali.github.io/2020/02/02/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94-Figure-Legend/"/>
    <id>http://a-kali.github.io/2020/02/02/SCI写作入门（3）——-Figure-Legend/</id>
    <published>2020-02-02T13:59:24.000Z</published>
    <updated>2020-02-02T14:07:04.688Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><h1 id="1-图片说明"><a href="#1-图片说明" class="headerlink" title="1    图片说明"></a>1    图片说明</h1><h1 id="2-表格注释"><a href="#2-表格注释" class="headerlink" title="2    表格注释"></a>2    表格注释</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;h1 id=&quot;1-图片说明&quot;&gt;&lt;a href=&quot;#1-图片说明&quot; class=&quot;headerlink&quot; title=&quot;1    图片说明&quot;&gt;&lt;/a&gt;1    图片说明&lt;/h1&gt;&lt;h1 id=&quot;2-表格注释&quot;&gt;&lt;a href=&quot;#2-表格注释&quot;
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
</feed>
