<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-01-06T11:51:46.104Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020年上半年论文阅读计划</title>
    <link href="http://a-kali.github.io/2020/01/06/2020%E5%B9%B4%E4%B8%8A%E5%8D%8A%E5%B9%B4%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/"/>
    <id>http://a-kali.github.io/2020/01/06/2020年上半年论文阅读计划/</id>
    <published>2020-01-06T10:58:49.000Z</published>
    <updated>2020-01-06T11:51:46.104Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="计划" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Bengali.AI Handwritten Grapheme Classification 比赛记录</title>
    <link href="http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/"/>
    <id>http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/</id>
    <published>2020-01-03T14:33:32.000Z</published>
    <updated>2020-01-09T09:45:53.283Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Please enter the password to read the blog." />    <label for="pass">Please enter the password to read the blog.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      The article has been encrypted, please enter your password to view.&lt;br&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="Kaggle" scheme="http://a-kali.github.io/tags/Kaggle/"/>
    
      <category term="比赛记录" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab全家桶（From v1 to v3+）</title>
    <link href="http://a-kali.github.io/2019/12/13/DeepLab%E5%85%A8%E5%AE%B6%E6%A1%B6/"/>
    <id>http://a-kali.github.io/2019/12/13/DeepLab全家桶/</id>
    <published>2019-12-13T15:19:07.000Z</published>
    <updated>2019-12-17T08:59:29.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DeepLabv1"><a href="#DeepLabv1" class="headerlink" title="DeepLabv1"></a>DeepLabv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1412.7062v3" target="_blank" rel="noopener">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>其实挺烦看这种远古论文的，引用的算法现在都不太常见，使用的措辞也和现在不太一样。该论文主要引入了<strong>空洞卷积(Astrous/Dilated Convolution)</strong>和<strong>条件随机场(Conditional Random Field, CRF)</strong>。</p><p>空洞卷积，顾名思义，即是在卷积核权重之间注入空洞，<strong>使用小卷积核的计算量获得大卷积核的感受野</strong>。（如理解有误请邮件指正）</p><p>空洞卷积比传统卷积多一个参数为<strong>采样率(dilation rate)</strong>，表示一个卷积核中采样的间隔。</p><p><img src="https://s2.ax1x.com/2019/12/14/Q2DWex.gif" alt="Q2DWex.gif"></p><p>条件随机场涉及到很多机器学习的知识，学起来比较耗时间，而且在后来的DeepLab版本中被取代，所以此处暂略，有机会再补上。</p><h1 id="DeepLabv2"><a href="#DeepLabv2" class="headerlink" title="DeepLabv2"></a>DeepLabv2</h1><p>论文地址：<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>比起v1，v2的主要改动是增加了<strong>带孔空间金字塔池化(ASPP)</strong>模块，其思想来源于SPPnet。但是文中对ASPP的阐述非常少，完全没有讲清楚ASPP的机制，只能通过论文中的图片和网上的博客来猜。</p><p><img src="https://i.loli.net/2019/12/15/MgRpErQ4utse9ND.png" alt="YM_ISX2ECM53ZW3_4T7HNYJ.png"></p><p><img src="https://i.loli.net/2019/12/15/6S7hpAo3ZiefQBN.png" alt="_H7RQ3P@__TYL_4_87Z0H05.png"></p><p>可以看出，ASPP使用了几种不同采样率的空洞卷积，对一张特征图得出多个分支后，最终concat到一起。我到现在也没搞明白为啥叫“空间金字塔池化”而不是“空间金字塔卷积”。</p><h1 id="DeepLabv3"><a href="#DeepLabv3" class="headerlink" title="DeepLabv3"></a>DeepLabv3</h1><p>论文地址：<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>提出了串联(cascade)和并联(parallel)两种格式，并指出并联效果更好。</p><p><img src="https://i.loli.net/2019/12/16/3PZxML24biRdpGj.png" alt="YF@__L_0M@_0RUKBG_N_O6C.png"></p><p><img src="https://i.loli.net/2019/12/15/tqhbdGpKZIwgf8A.png" alt="_C1PITC8~_S3@U_48_2_L5M.png"></p><p>网络去除了CRF，修改了一些参数，应用了一些新技术（比如批归一化）使模型更加精简。</p><p>虽然从文中看不出做了多少修改，但作者说性能得到了很大的提升。科科。</p><h1 id="DeepLabv3-1"><a href="#DeepLabv3-1" class="headerlink" title="DeepLabv3+"></a>DeepLabv3+</h1><p>论文地址：<a href="https://arxiv.org/abs/1802.02611v1" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>8102年，deeplab终于将Encoder-Decoder结构加进网络里了，之前一直用的双线性插值做上采样。</p><p><img src="https://i.loli.net/2019/12/16/bHvnI59LjUo3JcQ.png" alt="V7U32G_QEI_GOMGI97N6LAG.png"></p><p><img src="https://i.loli.net/2019/12/16/1UiclraR5NuOVvz.png" alt="__ZDD_GOD~3NX9_P_L@HSWA.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://blog.csdn.net/qq_31622015/article/details/90551107" target="_blank" rel="noopener">【语义分割系列：一】DeepLab v1 / v2 论文阅读翻译笔记</a></li><li><a href="https://blog.csdn.net/qq_21997625/article/details/87080576" target="_blank" rel="noopener">语义分割(semantic segmentation)—DeepLabV3之ASPP(Atrous Spatial Pyramid Pooling)代码详解</a></li><li><a href="https://blog.csdn.net/guo_rongxin/article/details/79842895" target="_blank" rel="noopener">deeplab v3论文翻译 Rethinking Atrous Convolution for Semantic Image Segmentation</a></li><li><a href="https://blog.csdn.net/fish_like_apple/article/details/82787705" target="_blank" rel="noopener">Deeplab相关改进的阅读记录（Deeplab V3和Deeplab V3+）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DeepLabv1&quot;&gt;&lt;a href=&quot;#DeepLabv1&quot; class=&quot;headerlink&quot; title=&quot;DeepLabv1&quot;&gt;&lt;/a&gt;DeepLabv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1412.7
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DeepLab" scheme="http://a-kali.github.io/tags/DeepLab/"/>
    
  </entry>
  
  <entry>
    <title>SENet: Squeeze-and-Excitation Networks</title>
    <link href="http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/"/>
    <id>http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/</id>
    <published>2019-12-08T11:24:08.000Z</published>
    <updated>2019-12-17T09:00:21.295Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SENet 是北京 Momenta 公司研发团队提出的网络结构，该团队凭借SENet以极大的优势获得了 ImageNet 2017 竞赛的图像分类任务冠军。该网络至今(2019.12.12)仍然是最强力的分类网络之一。</p><p>我们从卷积网络开始说起。近些年来，卷积神经网络在很多领域上都取得了巨大的突破。而卷积核作为卷积神经网络的核心，通常被看做是在局部感受野上，将<strong>空间上（spatial）的信息和特征维度上（channel-wise）的信息</strong>进行聚合的信息聚合体。卷积神经网络由一系列卷积层、非线性层和下采样层构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述。</p><p>Inception 结构即是从空间上提取特征的典型案例，其使用不同大小的卷积核，聚合多种不同感受野的特征来提升性能。而 SENet 则是从通道特征之间的关系来考虑，采用了一种<strong>特征重标定(feature recalibration)</strong>的策略，即<strong>通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><h2 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h2><p><img src="https://i.loli.net/2019/12/12/SmgX8AJWzBUvR9s.png" alt="I8_M4J____T1XB_FTW_P_H5.png"></p><p>上图是一个 SE 模块的示意图。</p><ul><li><p><strong>Squeeze</strong>：$F_{sq}(·)$ 将特征图进行<strong>全局平均池化(global average pooling)</strong>，使每个二维的特征通道对应生成一个实数，这个实数某种程度上具有全局的感受野。这意味着当 SE 模块用于靠近输入的层时，也能获得全局的感受野，这一点在很多任务中都是非常有用的。</p></li><li><p><strong>Excitation</strong>：$F_{ex}(·,W)$ 是一些<strong>全连接层和激活函数</strong>，其输入为squeeze的结果，<strong>输出为每个特征通道的重要性</strong>。具体分为4层网络：</p><ul><li>FC1：输入长度为 C 的向量，输出长度为 C/16 的向量。</li><li>ReLU：增加非线性</li><li>FC2：输入长度为 C/16 的向量，输出长度为 C 的向量。</li><li>Sigmoid：将输出限制到(0,1)之间，作为每个通道的权重，表示通道的重要性。</li></ul><p>使用两个而不是一个全连接层的好处在于：</p><ol><li>多加了一个ReLU层，具有更多的非线性，可以更好地拟合通道间复杂的相关性</li><li>极大地减少了参数量和计算量（约为原计算量的1/8）</li></ol></li><li><p><strong>Reweight</strong>：通过乘法将 Excitation 得到的通道权重加权到先前的特征上，完成在通道维度上对原始特征的重标定。</p></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者将 SE 模块拟合到了 Inception 和 ResNet 中，对网络性能产生了较大的增益。</p><p><img src="https://i.loli.net/2019/12/12/myFb9gUAhtNZSxV.png" alt="_LW`_DHP74CK_LED0COG_1P.png"></p><p>在理论上 SE 模块仅增加了网络 1% 的计算量；在实验中，由于 GPU 的架构原因，在 GPU 上的运算时间增加了 10%，而在 CPU 上仅增加了 2%。</p><p><img src="https://i.loli.net/2019/12/12/lzBcfFuq27vDGjw.png" alt="28R__T_T_RPEF6@_EIL4_8F.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/wfei101/article/details/79672944" target="_blank" rel="noopener">Face Paper：SeNet论文详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SENet" scheme="http://a-kali.github.io/tags/SENet/"/>
    
  </entry>
  
  <entry>
    <title>轻量级卷积神经网络综述：从SqueezeNet到MixNet</title>
    <link href="http://a-kali.github.io/2019/12/05/%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%EF%BC%9A%E4%BB%8ESqueezeNet%E5%88%B0EfficientNet/"/>
    <id>http://a-kali.github.io/2019/12/05/轻量级卷积神经网络综述：从SqueezeNet到EfficientNet/</id>
    <published>2019-12-05T10:40:16.000Z</published>
    <updated>2020-01-06T11:02:30.316Z</updated>
    
    <content type="html"><![CDATA[<p>在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗电量高则会导致汽车、手机等移动端续航能力变差，而只有轻量级的神经网络能解决这个问题。下面我将介绍近年来轻量级卷积神经网络的发展。</p><h1 id="1-SqueezeNet"><a href="#1-SqueezeNet" class="headerlink" title="1    SqueezeNet"></a>1    SqueezeNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1602.07360v2" target="_blank" rel="noopener">SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</a></p><p>SqueezeNet网络的主要亮点在于提出了<strong>Fire Module</strong>来减少参数量。Fire Module 分为两部分：<strong>Squeeze 和 Expand</strong>。Squeeze层通过 1×1 卷积对特征图进行降维，减少参数量，Expand层分别使用 1×1 和 3×3 卷积对降维后的特征图进行处理后concat到一起。比起直接用3×3卷积，这种方法减少了一定的运算量。</p><p><img src="https://i.loli.net/2019/12/05/8TYQwPWMnCU3ok4.png" alt="(R7V7PD.png"></p><p>整个网络由多个Fire Module堆叠而成，很像GoogLeNet。右边两个网络结构参考了ResNet。</p><p><img src="https://i.loli.net/2019/12/05/rW4ugsYPjLUxXVh.png" alt="`YX7Opng"></p><h1 id="2-MobileNet-v1"><a href="#2-MobileNet-v1" class="headerlink" title="2    MobileNet v1"></a>2    MobileNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p><p>MobileNet由谷歌公司提出，主要用于移动和嵌入式视觉应用，其亮点在于采用<strong>深度可分离卷积(Depth-wise Separable Convolution)</strong> 代替传统卷积。</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>深度可分离卷积分为两步：</p><ol><li><strong>Depthwise convolution</strong>：对特征图各个通道进行卷积，每个卷积核只有一个通道且只负责特征图的一个通道。</li><li><strong>Pointwise convolution</strong>：使用1×1卷积将特征图串起来，得到和普通卷积一样的输出。</li></ol><p><img src="https://i.loli.net/2019/12/05/9bdIvHnKpY3XOG8.png" alt="W8I.png"></p><h2 id="运算量对比"><a href="#运算量对比" class="headerlink" title="运算量对比"></a>运算量对比</h2><p>假设输入图像为12×12×3，输出图像为8×8×256。</p><ul><li>Convolution：<ul><li>卷积核大小 5×5×3，卷积核数量 256</li><li>数据量：5×5×3×256 = 19200</li><li>计算量：仅考虑乘法运算，每产生一个输出值就要进行5×5×3次运算，一共要产生8×8×256个输出值，故 5×5×3×256×8×8 = 1228800。</li></ul></li><li>Depthwise Separable Convolution：<ul><li>Depthwise convolution：卷积核大小 5×5×1，卷积核数量 3</li><li>Pointwise convolution：卷积核大小 1×1×3，卷积核数量 256</li><li>数据量：5×5×1×3+1×1×3×256 = 843</li><li>计算量：5×5×1×3×8×8+1×1×3×8×8×256 = 53952</li></ul></li></ul><h2 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h2><p>左边是传统卷积，右边是深度可分离卷积。</p><p><img src="https://i.loli.net/2019/12/05/K2pm9axhdokJOiR.png" alt="FT.png"></p><h2 id="实验结果对比"><a href="#实验结果对比" class="headerlink" title="实验结果对比"></a>实验结果对比</h2><p>可以看到MobileNet在只牺牲了少量精确度的情况下节约了大量的运算量和网络参数。</p><p><img src="https://i.loli.net/2019/12/05/YwsgB2EQ7cxT1XC.png" alt="O.png"></p><h1 id="3-Xception"><a href="#3-Xception" class="headerlink" title="3    Xception"></a>3    Xception</h1><p>论文地址：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolutions</a></p><p>Xception 借鉴了深度可分离卷积的思想并以此改进了Inception V3。</p><p><img src="https://i.loli.net/2019/12/05/Ht6YxQXpTvRbVA2.png" alt="Fpng"></p><p>图中是一个Xception模块，先用 1×1 卷积改变特征图的通道数，再对输出的每个通道分别进行 3×3 卷积，最后将 3×3 卷积的输出concat到一起。</p><h1 id="4-ShuffleNet-v1"><a href="#4-ShuffleNet-v1" class="headerlink" title="4    ShuffleNet v1"></a>4    ShuffleNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p><p>ShuffleNet是由旷视公司提出的轻量级网络，该网络结构主要使用了<strong>分组卷积(group convolution)</strong>和<strong>通道洗牌(channel shuffle)</strong>。</p><p><img src="https://i.loli.net/2019/12/05/tsFGvMo17V6QuIg.png" alt="MBng"></p><p>图a展示了分组卷积，即将通道均等分为多组，分别进行卷积操作（类似于深度可分离卷积）。但这样会导致组之间的信息不流通，对精度造成影响。于是使用通道洗牌的方式，对各组的通道进行交换。</p><p>下图是两种ShuffleNet单元：</p><p><img src="https://i.loli.net/2019/12/05/iNdkjWleXIDqsFK.png" alt="7EYQpng"></p><h1 id="5-MobileNet-v2"><a href="#5-MobileNet-v2" class="headerlink" title="5    MobileNet v2"></a>5    MobileNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p><h2 id="MobileNet-v1-存在问题"><a href="#MobileNet-v1-存在问题" class="headerlink" title="MobileNet v1 存在问题"></a>MobileNet v1 存在问题</h2><p>MobileNet v1 虽然很好地降低了模型运算量，但依然存在如下两个问题：</p><ol><li>MobileNet v1 的结构是类似于 VGG 的堆叠结构，而这种结构比起后来的 ResNet、GoogLeNet 来说性能不高。</li><li>Depthwise Convolution 的潜在问题：论文作者发现，由于<strong>深度残差卷积产生的特征图通道数较少，在 ReLU 的影响下很容易产生较大的信息损耗</strong>（这个故事告诉我们不要在压缩通道后用ReLU）。</li></ol><h2 id="MobileNet-v2-的创新点"><a href="#MobileNet-v2-的创新点" class="headerlink" title="MobileNet v2 的创新点"></a>MobileNet v2 的创新点</h2><p>为了解决 v1 存在的问题，v2 提出了以下改进方法：</p><ol><li><p><strong>Inverted Residual Block</strong>：首先从名字可以看出，这是从传统残差块演化而来的<strong>逆残差</strong>，两者主要的不同在于对 1×1 卷积的运用方式不同。传统的残差块使用 1×1 卷积降低特征图的通道数，减少 3×3 卷积的运算量；而逆残差则是用 1×1 卷积来提升维度，以便提升网络的准确度。可能作者觉得反正 Depthwise Convolution 运算量也不大，不如就牺牲一丢丢速度来提高一下精度吧。</p><p><img src="https://i.loli.net/2019/12/08/cBeWJVdTSU9t5Eb.png" alt="Q_P`KE8N_V3JY_L``4O6CXS.png"></p></li><li><p><strong>Linear Bottlenecks</strong>：对比 v1 和 v2 的结构可以看出，v2 使用线性函数替换了 v1 模块最后的ReLU6：</p></li></ol><p><img src="https://i.loli.net/2019/12/11/aSftLEykhVcC19I.png" alt="H_T83__0YR5Q6K_Y~V8_M_S.png"></p><h1 id="6-ShuffleNet-v2"><a href="#6-ShuffleNet-v2" class="headerlink" title="6    ShuffleNet v2"></a>6    ShuffleNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1807.11164" target="_blank" rel="noopener">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常用于神经网络的设计指导指标使用的是计算复杂度衡量指标：<strong>FLOPs</strong>，而不是更直接的评价指标：<strong>运行速度</strong>(speed)。而作者发现相同FLOPs的网络速度可能差别很大，认为FLOPs并不能作为网络性能的唯一衡量指标。</p><p>造成FLOPs和速度不成比例的原因：</p><ol><li>部分影响速度的原因没有被FLOPs包含在内：<ul><li><strong>内存访问成本</strong>(memory access cost, <strong>MAC</strong>)：这会使得强大的GPU算力受到限制。</li><li><strong>并行度</strong>(degree of parallelism)：在相同FLOPs的情况下，并行度高的网络模型速度远高于低并行度模型。</li></ul></li><li>不同的运行平台会影响FLOPs。比如说新版的CUDNN专门对 3 × 3 卷积运算进行了优化。</li></ol><p>出于这点考虑，作者提出了两点高效结构设计的指导性原则：</p><ol><li>应当使用直接的评价指标（e.g., 速度）而不是间接的（e.g., FLOPs）。</li><li>应当在规定的平台上进行评估。</li></ol><h2 id="高效卷积网络设计准测"><a href="#高效卷积网络设计准测" class="headerlink" title="高效卷积网络设计准测"></a>高效卷积网络设计准测</h2><ul><li><strong>G1: 当输入、输出channels数目相同时，conv计算所需的MAC最低。</strong>以深度可分离卷积(Depth-wise Separable Convolution)为例，其 pointwise convlution (i.e., 1×1 conv) 部分占用了其大部分复杂度。设$c_1$，$c_2$为 1 × 1 卷积的输入、输出通道数，$h$和$w$为特征图的高和宽，则 FLOPs 计算为 $B=hwc_1c_2$。内存访问操作次数为 $MAC=hw(c_1+c_2)+c_1c_2$。得出下面的不等式，仅当输入输出通道数相同时，MAC最小：<script type="math/tex; mode=display">MAC\geq 2\sqrt{hwB}+\frac{B}{hw}</script></li><li><strong>G2: 过多的分组卷积(Group Convolution)会增大 MAC 开销。</strong>设分组数量为 $g$，从下面公式可以看出随着 $g$ 增加，MAC增加。  ：</li></ul><script type="math/tex; mode=display">MAC=hwc_1+\frac{Bg}{c_1}+\frac{B}{hx}</script><ul><li><strong>G3: 网络碎片化(fragmentation)会减少并发度。</strong>这里的碎片化大概指的是模型的分支数量。比如说 NASNET-A 的分支数就高达13，而 ResNet 的分支数为2或3。作者通过实验证明，分支数量的提升会提高网络的准确率，但也会因降低GPU并行计算能力而影响效率。</li><li><strong>G4: Element-wise 操作的计算量不容忽视。</strong>element-wise包括激活、张量相加、添加偏置等，它们的共同特征就是FLOPS较小但是MAC相对较大。同时作者将 depthwise convolution 操作也算入了element-wise，因为其有着同样高的 MAC/FLOP 比率。</li></ul><p>目前的轻量级网络结构主要是是以FLOPS作为度量标准设计的，而没有考虑以上的几点属性。比如说，ShuffleNet v1使用了过多的分组卷积(与G2违背)、bottleneck-like块(与G1违背)；MobileNet v2使用倒置的bottleneck结构(与G1违背)，同时使用了深度卷积和ReLU在”thick”特征图上(与G4违背)；自动生成结构过多的使用了碎片化结构(与G3违背)</p><h2 id="ShuffleNet-V2-网络结构"><a href="#ShuffleNet-V2-网络结构" class="headerlink" title="ShuffleNet V2 网络结构"></a>ShuffleNet V2 网络结构</h2><p>为了使ShuffleNet更加高效，关键在于保持等宽的出入输出通道，以及使用密集卷积操作而不是过多的分组卷积。</p><p><img src="https://i.loli.net/2019/12/11/3YuOe1yB8zalwjL.png" alt="S9YD9ZB_OHNUS`GHFVS@9D0.png"></p><p>如图，左边两个是 ShuffleNet v1 的模块，右边两个是 ShuffleNet v2 的模块。</p><p>图c是 ShuffleNet v2 的基本模块，其首先将输入的通道随机split成两部分（这是一种变相的分组卷积，不过只分了两个组，遵守了G2和G3），一部分恒等映射到模块尾部，另一部分通过三个输入输出通道数相同的卷积前向传播（遵守了G1），之后使用concat操作（而不是add操作，遵守了G4）将两个分支结合在一起，最后进行通道洗牌(channel shuffle)。</p><p>图d为下采样模块，原理类似，stride=2缩小特征图，没有使用channel split操作，最后两个分支concat到一起使通道数翻倍。</p><p>恒等映射后concat到模块尾部，能使特征得到复用，提高准确度。这种思想来源于DenseNet。</p><h1 id="7-MnasNet（待更新）"><a href="#7-MnasNet（待更新）" class="headerlink" title="7    MnasNet（待更新）"></a>7    MnasNet（待更新）</h1><p>太复杂了，回头再看</p><h1 id="8-MobileNet-v3（待更新）"><a href="#8-MobileNet-v3（待更新）" class="headerlink" title="8    MobileNet v3（待更新）"></a>8    MobileNet v3（待更新）</h1><p>基于MnasNet</p><h1 id="9-MixNet（待更新）"><a href="#9-MixNet（待更新）" class="headerlink" title="9    MixNet（待更新）"></a>9    MixNet（待更新）</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://baijiahao.baidu.com/s?id=1589005428414488177&amp;wfr=spider" target="_blank" rel="noopener">纵览轻量化卷积神经网络：SqueezeNet、MobileNet、ShuffleNet、Xception</a></li><li><a href="https://www.jianshu.com/p/fdd7d7353c55" target="_blank" rel="noopener">SqueezeNet | 轻量级深层神经网络</a></li><li><a href="https://www.greedyai.com/" target="_blank" rel="noopener">贪心学院</a></li><li><a href="https://blog.csdn.net/lk3030/article/details/84847879" target="_blank" rel="noopener">Xception</a></li><li><a href="https://blog.csdn.net/kangdi7547/article/details/81431572" target="_blank" rel="noopener">轻量级模型：MobileNet V2</a></li><li><a href="https://www.jianshu.com/p/71e32918ea0a?utm_source=oschina-app" target="_blank" rel="noopener">精简CNN模型系列之六：ShuffleNet v2</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/81322175" target="_blank" rel="noopener">ShuffleNet v2算法笔记</a></li><li><a href="https://blog.csdn.net/h__ang/article/details/88618089" target="_blank" rel="noopener">ShuffleNet_v2模型解读</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="轻量级网络" scheme="http://a-kali.github.io/tags/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SqueezeNet" scheme="http://a-kali.github.io/tags/SqueezeNet/"/>
    
      <category term="MobileNet" scheme="http://a-kali.github.io/tags/MobileNet/"/>
    
      <category term="Xception" scheme="http://a-kali.github.io/tags/Xception/"/>
    
      <category term="ShuffleNet" scheme="http://a-kali.github.io/tags/ShuffleNet/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
      <category term="深度可分离卷积" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>SSD: Single-shot detectors</title>
    <link href="http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/"/>
    <id>http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/</id>
    <published>2019-12-04T08:49:35.000Z</published>
    <updated>2019-12-17T09:01:49.867Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文提出了一个基于深度神经网络的<strong>单步(single shot)目标检测器SSD</strong>，其在继承了YOLO单步预测高检测速度的同时，拥有不弱于Faster R-CNN的准确度。</p><h1 id="SSD-网络结构"><a href="#SSD-网络结构" class="headerlink" title="SSD 网络结构"></a>SSD 网络结构</h1><p><img src="https://i.loli.net/2019/12/04/qIhrwjU6uRMV4Nn.png" alt="png"></p><p>从图中可以看出：</p><ul><li>不同于YOLOv1和Faster R-CNN，SSD是一个全卷积网络。</li><li>SSD的预测结果并不完全由最后一层输出，而是由其5个<strong>额外特征层(Extra Feature Layers)</strong>和 VGG16中的一层的输出综合而来。</li><li>由于SSD是个全卷积网络，所以其分类操作也由卷积层进行。上图中横向的直线即是<strong>卷积分类器</strong>，卷积核大小为3×3，channel数量为anchors×(Classes+4)。此处anchors指anchor的数量；classes为类别数，预测值为每个类置信度，这点应该会给后面的NMS作为评判标准；+4就是(x,y,w,h)。</li><li>SSD的输出特征图平均每个像素都有一组anchor，整个网络共生成8732个anchor，远多于YOLO和Faster R-CNN。（这里有个问题，根据上面一条，使用3×3卷积核作为滑动窗口是没法做到每个像素都有anchor的，所以此处应该有padding）</li></ul><p><img src="https://i.loli.net/2019/12/04/To4hNmlKez1CrqV.png" alt="2Bng"></p><h1 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h1><p>检测结果中，未被选为最终结果的样本都是负样本。这导致负样本数量远大于正样本，样本不均衡。作者采用<strong>Hard negative mining</strong>的方式，仅选用被误认为是正样本可能性更大的负样本。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>论文中还提到了损失函数和anchor的选择，但跟其它的目标检测网络差不多，就不再赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="SSD" scheme="http://a-kali.github.io/tags/SSD/"/>
    
  </entry>
  
  <entry>
    <title>盘点那些在github上找到的宝藏</title>
    <link href="http://a-kali.github.io/2019/12/04/%E7%9B%98%E7%82%B9%E9%82%A3%E4%BA%9B%E5%9C%A8github%E4%B8%8A%E6%89%BE%E5%88%B0%E7%9A%84%E5%AE%9D%E8%97%8F/"/>
    <id>http://a-kali.github.io/2019/12/04/盘点那些在github上找到的宝藏/</id>
    <published>2019-12-03T16:14:40.000Z</published>
    <updated>2020-01-06T10:33:28.524Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>计算机视觉&amp;深度学习超级Road Map！</strong>：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没有SLAM、自动驾驶、行人检测、对抗样本）的学习路线和各个模型的概述。CVer必备！项目地址：<a href="https://github.com/weslynn/AlphaTree-graphic-deep-neural-network" target="_blank" rel="noopener">https://github.com/weslynn/AlphaTree-graphic-deep-neural-network</a></p></li><li><p><strong>使用Python实现各种算法</strong>：想学算法又不想学C++？重度Python患者的福音，项目地址：<a href="https://github.com/TheAlgorithms/Python" target="_blank" rel="noopener">https://github.com/TheAlgorithms/Python</a></p></li><li><p><strong>深度学习500问</strong>：以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，面试党必备。项目地址：<a href="https://github.com/scutan90/DeepLearning-500-questions" target="_blank" rel="noopener">https://github.com/scutan90/DeepLearning-500-questions</a></p></li><li><p><strong>3DCNN-PyTorch</strong>：PyTorch 3D卷积预训练模型。项目地址：<a href="https://github.com/kenshohara/3D-ResNets-PyTorch" target="_blank" rel="noopener">https://github.com/kenshohara/3D-ResNets-PyTorch</a> 。附腾讯优图的3D医疗影像预训练模型MedicalNet（未来会出2D），亲测效果不错，项目地址：<a href="https://github.com/Tencent/MedicalNet" target="_blank" rel="noopener">https://github.com/Tencent/MedicalNet</a></p></li><li><p><strong>南瓜书</strong>：西瓜书公式推导解析，节约2w根头发。项目地址：<a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">https://github.com/datawhalechina/pumpkin-book</a></p></li><li><p><strong>深度学习面试宝典</strong>：涵盖各大公司ML、CV、NLP、数学、算法、强化学习、SLAM等方向的面试题&amp;解答集合！外加面试技巧和经验！面试党吐血墙裂推荐！项目地址：<a href="https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs" target="_blank" rel="noopener">https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs</a></p></li><li><p><strong>PyTorch预训练模型</strong>：</p><ul><li>主要backbone汇总：<a href="https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet" target="_blank" rel="noopener">https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet</a> 。</li><li>EfficientNet：<a href="https://github.com/lukemelas/EfficientNet-PyTorch" target="_blank" rel="noopener">https://github.com/lukemelas/EfficientNet-PyTorch</a> 。这个预训练模型有个缺点，只能单GPU运行，但这不妨碍EfficientNet牛逼。</li><li>语义分割模型：<a href="https://github.com/qubvel/segmentation_models.pytorch" target="_blank" rel="noopener">https://github.com/qubvel/segmentation_models.pytorch</a> 。主流语义分割模型，可惜没有DeepLab系列。</li><li>DeepLab：<a href="https://github.com/jfzhang95/pytorch-deeplab-xception" target="_blank" rel="noopener">https://github.com/jfzhang95/pytorch-deeplab-xception</a></li></ul></li><li><p><strong>数据增广大全</strong>：涵盖了CV、自然语言、音频方向的各种数据增广图例、调用库和论文。项目地址：<a href="https://github.com/AgaMiko/data-augmentation-review" target="_blank" rel="noopener">https://github.com/AgaMiko/data-augmentation-review</a></p></li><li><p><strong>Chrome实用插件大全</strong>：Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类。项目地址：<a href="https://github.com/zhaoolee/ChromeAppHeroes" target="_blank" rel="noopener">https://github.com/zhaoolee/ChromeAppHeroes</a></p></li><li><p><strong>绘制炫酷的神经网络图</strong>：</p><ul><li><p>使用图形界面绘制：<a href="https://github.com/zfrenchee" target="_blank" rel="noopener">https://github.com/zfrenchee</a></p><p>画图工具体验地址：<a href="http://alexlenail.me/NN-SVG/" target="_blank" rel="noopener">http://alexlenail.me/NN-SVG/</a></p></li><li><p>使用LaTeX绘制：<a href="https://github.com/HarisIqbal88/PlotNeuralNet" target="_blank" rel="noopener">https://github.com/HarisIqbal88/PlotNeuralNet</a></p></li><li><p>使用参数绘制：<a href="https://cbovar.github.io/ConvNetDraw/" target="_blank" rel="noopener">https://cbovar.github.io/ConvNetDraw/</a></p><p><img src="https://i.loli.net/2019/12/15/lY4JZtPcwC6fvUu.png" alt="6KJ9MJ75ZKFMJCB9_Z8_DPQ.png"></p></li><li><p>使用Python绘制：<a href="https://github.com/gwding/draw_convnet" target="_blank" rel="noopener">https://github.com/gwding/draw_convnet</a></p></li><li><p>方便&amp;好看&amp;著名的NetScope：<a href="https://github.com/ethereon/netscope" target="_blank" rel="noopener">https://github.com/ethereon/netscope</a></p></li></ul></li></ul><p>找到其它宝藏的小伙伴可以通过邮箱发给我鸭！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;计算机视觉&amp;amp;深度学习超级Road Map！&lt;/strong&gt;：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="PyTorch" scheme="http://a-kali.github.io/tags/PyTorch/"/>
    
      <category term="github" scheme="http://a-kali.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>常见的聚类算法(K-Means\GMM\DBSCAN)</title>
    <link href="http://a-kali.github.io/2019/12/02/k-means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://a-kali.github.io/2019/12/02/k-means-聚类算法/</id>
    <published>2019-12-02T15:59:48.000Z</published>
    <updated>2019-12-17T09:00:44.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>在<strong>无监督学习(unsupervised learning)</strong>中，训练样本的标记信息是未知的，需要·通过一些算法来揭示这些样本数据中的内在性质和规律。无监督学习通常解决的是<strong>聚类(clustering)</strong>问题。</p><p>聚类算法通常将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个”簇(cluster)”。而每个簇都包含一定的内在关系或者性质。</p><p>如下图是一个为做标记的样本集，通过它们的分布，我们很容易对上图中的样本做出以下几种划分：</p><p>当需要将其划分为两个簇时，即 k=2 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKF1eO.png" alt="QKF1eO.png"></p><p>当需要将其划分为四个簇时，即 k=4 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKFalt.png" alt="QKFalt.png"></p><p>而对这些样本进行划分的就是聚类算法。下面我们将介绍几种常见的聚类算法中的算法。</p><h1 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means 算法"></a>K-means 算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>k-means算法又名 k 均值算法。其算法思想大致为：<strong>先从样本集中随机选取 k 个样本作为簇中心，并计算所有样本与这 k 个簇中心的距离，对于每一个样本，将其划分到与其距离最近的簇中心所在的簇中，对于新的簇计算各个簇的新的簇中心，根据新的簇中心来重新划分簇。重复上述过程，直到所有样本到其簇中心距离之和达到最小。</strong>在普通K-means算法中，k 值通常凭经验和需求、通过多次尝试来选择；度量距离的方法通常采用<strong>欧氏距离</strong>，其它距离测量方法有曼哈顿距离、切比雪夫距离等，在这里不一一赘述。</p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>（待更新）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;headerlink&quot; title=&quot;聚类&quot;&gt;&lt;/a&gt;聚类&lt;/h1&gt;&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt;中，训练样本的标记信息是未知的，需要·通过一些算
      
    
    </summary>
    
      <category term="机器学习" scheme="http://a-kali.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="k-means" scheme="http://a-kali.github.io/tags/k-means/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://a-kali.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类" scheme="http://a-kali.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>部分计算机视觉算法面试题解答</title>
    <link href="http://a-kali.github.io/2019/12/02/%E9%83%A8%E5%88%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98%E8%A7%A3%E7%AD%94/"/>
    <id>http://a-kali.github.io/2019/12/02/部分计算机视觉算法面试题解答/</id>
    <published>2019-12-01T16:59:03.000Z</published>
    <updated>2019-12-01T17:15:59.062Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout是失活神经元还是失活连接</strong></p><p>A：失活神经元并清除失活神经元周围的连接</p><p><a href="https://imgse.com/i/Qm4Aht" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qm4Aht.md.png" alt="Qm4Aht.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout是失活神经元还是失活连接&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A：失活神经元并清除失活神经元周围的连接&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://imgse.com/i/Qm4Aht&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>常见激活函数汇总</title>
    <link href="http://a-kali.github.io/2019/12/01/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/"/>
    <id>http://a-kali.github.io/2019/12/01/激活函数汇总/</id>
    <published>2019-12-01T15:05:09.000Z</published>
    <updated>2019-12-01T16:38:44.814Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h1><p>Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。</p><p>函数式：</p><script type="math/tex; mode=display">σ(x) = \frac{1}{1+e^{-x}}</script><p>导数式：</p><script type="math/tex; mode=display">σ'(x) = σ(x)×(1-σ(x))</script><p>函数图像：</p><p><a href="https://imgse.com/i/QmcKiT" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcKiT.md.png" alt="QmcKiT.md.png"></a></p><h1 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h1><p>ReLU是神经网络激活层最常用的一种函数，因为其运算简单、易于求导，能用最简单的方式实现非线性运算的性质。</p><p>函数式：</p><script type="math/tex; mode=display">f(x)=max(0,x)</script><p>函数图像：</p><p><a href="https://imgse.com/i/QmcLkV" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcLkV.md.png" alt="QmcLkV.md.png"></a></p><h1 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h1><p>常见于递归神经网络。</p><p>函数式：</p><script type="math/tex; mode=display">f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}</script><p>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/QmgaBn.png" alt="QmgaBn.png"></p><h1 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h1><p>函数式：</p><script type="math/tex; mode=display">f(x)=\left\{\begin{aligned}x, \quad x\geq0\\ax, \quad x<0\end{aligned}\right.</script><p>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/Qm21bR.png" alt="Qm21bR.png"></p><h1 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h1><p>PReLU(Parametric Rectified Linear Unit) 参数化修正线性单元。其参数随着网络训练而改变。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmRH0A.png" alt="QmRH0A.png"></p><p>参数更新：</p><p><img src="https://s2.ax1x.com/2019/12/02/QmROtP.png" alt="QmROtP.png"></p><h1 id="RReLU"><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h1><p>Random Leaky ReLU，其参数是随机生成的在[0, 1)之间的值。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmWjER.png" alt="QmWjER.png"></p><h1 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h1><p>指数线性单元。右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。ELU的输出均值接近于零，所以收敛速度更快。α为常数。</p><p><a href="https://imgse.com/i/Qmf1bj" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qmf1bj.md.png" alt="Qmf1bj.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sigmoid&quot;&gt;&lt;a href=&quot;#Sigmoid&quot; class=&quot;headerlink&quot; title=&quot;Sigmoid&quot;&gt;&lt;/a&gt;Sigmoid&lt;/h1&gt;&lt;p&gt;Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。&lt;/p&gt;
&lt;p&gt;函数
      
    
    </summary>
    
      <category term="神经网络" scheme="http://a-kali.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="激活函数" scheme="http://a-kali.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>递归神经网络（RNN）</title>
    <link href="http://a-kali.github.io/2019/12/01/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88RNN%EF%BC%89/"/>
    <id>http://a-kali.github.io/2019/12/01/递归神经网络（RNN）/</id>
    <published>2019-12-01T14:08:19.000Z</published>
    <updated>2019-12-01T16:57:04.347Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么需要递归神经网络？"><a href="#为什么需要递归神经网络？" class="headerlink" title="为什么需要递归神经网络？"></a>为什么需要递归神经网络？</h1><p>在现实生活中，有很多数据是随着时间的变化而变化的，如：股票行情、语音、天气等，而且文本数据也是有顺序的。这类数据统称为序列数据。序列数据通常长短不一，处理这种数据需要一个能够捕获时间的模型，RNN(Recurrent Neural Network)便应运而生。</p><p>（待更新）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;为什么需要递归神经网络？&quot;&gt;&lt;a href=&quot;#为什么需要递归神经网络？&quot; class=&quot;headerlink&quot; title=&quot;为什么需要递归神经网络？&quot;&gt;&lt;/a&gt;为什么需要递归神经网络？&lt;/h1&gt;&lt;p&gt;在现实生活中，有很多数据是随着时间的变化而变化的，如：股票行
      
    
    </summary>
    
    
      <category term="RNN" scheme="http://a-kali.github.io/tags/RNN/"/>
    
      <category term="LSTM" scheme="http://a-kali.github.io/tags/LSTM/"/>
    
      <category term="GRU" scheme="http://a-kali.github.io/tags/GRU/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv1 - YOLOv3</title>
    <link href="http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/"/>
    <id>http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/</id>
    <published>2019-11-27T09:00:31.000Z</published>
    <updated>2020-01-06T10:57:54.994Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>众所周知，在深度学习目标检测领域有着两个流派，分别是<strong>基于候选区域的R-CNN流派</strong>和<strong>直接回归输出边框的YOLO流派</strong>。R-CNN系列的准确率较高，但即便发展到Faster R-CNN，运算速度也才只有7fps。为了使检测工作更接近实时，作者提出了YOLO结构。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCYCPs.png" alt="QCYCPs.png"></p><h2 id="YOLO-的实现"><a href="#YOLO-的实现" class="headerlink" title="YOLO 的实现"></a>YOLO 的实现</h2><p>（第一次看可能有点复杂，建议拿笔出来边梳理边看）</p><p>YOLO 将输入图像划分为 <strong>S × S 个网格</strong>。如果一个物体的中心点在这个网格中，则该网格负责检测这个物体。每个网格预测 <strong>B 个边框(bounding box)</strong>及其<strong>置信度(confidence)</strong>。其中置信度为<strong>该网格包含目标物体的概率</strong>乘以预测边界框与真实边界框(ground truth)的<strong>交并比(IOU)</strong>，即：</p><script type="math/tex; mode=display">Confidence=Pr(Object)×IOU^{truth}_{pred}</script><p>也就是说，<strong>当置信度为0时，边框内不含有任何目标物，除此之外置信度都等于交并比</strong>。该置信度只是个预测值，受真实的置信度监督。这点可以从后面的损失函数看出来。</p><p>于是我们得知，每个边框由5个预测值组成，分别为$x,y,w,h,confidence$。</p><p>同时每个网格预测一组 <strong>C 个类别的概率</strong> $Pr(Class_i|Object)$，即输出一组长度为 C 的概率向量。这个概率表示网格含有物体的情况下，各个类别属于该网格的概率。</p><p>经过上述步骤，最终在神经网络末端输出一个$S×S×(B×5+C)$的张量。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCIFr6.png" alt="QCIFr6.png"></p><p>测试时，将置信度和每一类概率相乘</p><script type="math/tex; mode=display">Score=Pr(Class_i|Object)×Pr(Object)×IOU^{truth}_{pred}=Pr(Class_i)×IOU^{truth}_{pred}</script><p>得到的Score表示<strong>每一类在每个边框中的置信度(class-specific confidence for each box)</strong>。通过设置阈值筛选出得分高的box，再以的分最高的box为基准进行NMS选出最优结果。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>如图，在 PASCAL VOC 数据集中，图像输入为 448×448，取 S=7，B=2，一共有20 个类别（C=20），则输出就是 7x7x30 的一个 tensor。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCHg76.png" alt="QCHg76.png"></p><p>可以看出这是一个<strong>彻头彻尾的端到端网络</strong>。看到这里可能会有点震惊，上面讲了那么多复杂的设定到头来居然只是个这么朴素的端到端网络？事实上，上面那么多设定大多都是来源于其巧妙设计的损失函数。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://s2.ax1x.com/2019/11/28/QCq2WD.png" alt="QCq2WD.png"></p><p>由于坐标、长宽、置信度的重要性不同，作者给予了他们不同的损失函数和权重。</p><ul><li>重视坐标预测，给这些损失前面赋予更大的权重，取 5。</li><li>对没有 object 的 box 的 confidence loss，赋予较小的损失权重，取 0.5。</li><li>有 object 的 box 的 confidence loss 和类别的 loss 的损失权重取 1。</li><li>对不同大小的边框预测中，相比于大边框，小边框预测偏一点造成的影响更大。而均方误差中对同样的偏移 loss 是一样。为了缓和这个问题，作者用了一个比较取巧的办法，就是将 box 的 width 和 height 取平方根代替原本的 height 和 width。</li></ul><h2 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a>YOLO 的缺点</h2><ul><li>YOLO对比较密集的、小型的物体（如鸟群）检测效果不佳。因为会有两个同类物体出现在同一个网格中的情况。</li><li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱。</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li><li>召回率远低于RCNN系列。</li></ul><h1 id="YOLOv2-amp-YOLO9000"><a href="#YOLOv2-amp-YOLO9000" class="headerlink" title="YOLOv2 &amp; YOLO9000"></a>YOLOv2 &amp; YOLO9000</h1><p>论文地址：<a href="http://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>YOLOv2：在多方面进行改进，在mAP上超过了使用resnet作为backbone的Faster R-CNN 和 SSD，而且速度更快。</li><li>YOLO9000：使用大量分类数据集和检测数据集进行联合训练，能够对9000+类别进行检测。</li></ul><h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><p>YOLOv2 和 YOLOv1 对比：</p><p><img src="https://s2.ax1x.com/2019/11/29/QA2ryq.png" alt="QA2ryq.png"></p><ul><li><strong>增加了 batch norm</strong>。</li><li><strong>使用高分辨率微调模型</strong>：YOLOv1 采用224×224大小的图片进行预训练，但训练检测模型时使用的是448×448，这一变动对模型性能会产生一定影响。而YOLOv2在常规预训练和进行正式训练之间使用了448×448的分类图像样本进行了微调，缓解了分辨率突然切换造成的影响。</li><li><strong>采用了 Anchor Boxes</strong>：借鉴Faster R-CNN的做法使用了锚框，大幅提高了召回率但mAP轻微下降。</li><li>将图片输入尺寸改为416×416，grid改为13×13，<strong>使grid长宽为奇数</strong>，这样能更有效地预测图片中央的目标物（根据经验，目标物在图片中央的可能性较大）。</li><li><strong>使用 k-means 聚类算法来选择锚框</strong>：手工选择的锚框可能对性能产生影响性能。作者使用k-means对训练集目标框进行聚类，以IOU为距离计算指标，即 $d = 1 - IOU$。在对性能和准确率进行衡量之后，选择了 $k = 5$，得出聚类结果的5个聚类中心作为锚框的最终选择（只取锚框的大小和形状，不取锚框的位置）。</li><li><strong>Direct location prediction</strong>：在RPN中的锚框非常不稳定，其公式如下：</li></ul><script type="math/tex; mode=display">x=(t_x*w_a)−x_a\\y=(t_y*h_a)−y_a</script><p>$t_x$和$t_y$为预测值，当$t_x=1$时，预测框相比于原本的锚框将右移一整个锚框的宽度！YOLOv2对这种方法进行了改进：</p><script type="math/tex; mode=display">b_x=σ(t_x)+c_x\\b_y=σ(t_y)+c_y\\b_w=p_we^{t_w}\\b_h=p_he^{t_h}\\Pr(object)*IOU(b,object)=σ(t_o)</script><p>$t_x,t_y,t_w,t_h,t_o$为预测值，被Sigmoid函数限制在(0,1)之间。之后再通过下图的一些运算得到最终box。在<strong>限制了预测值大小</strong>的情况下，模型参数会更容易学习。</p><p><img src="https://i.loli.net/2019/12/03/Pb7ZzNEjQhDtS4c.png" alt="RH.png"></p><ul><li><strong>Fine-Grained Features</strong>：为了能在小型目标上获得更好的效果，作者把浅层高分辨率的特征图叠加到深层低分辨率的图上了，但没详细说明，估计跟U-Net差不多吧。</li><li><strong>多尺度训练</strong>：因为YOLOv2是全卷积，所以能用任意大小的图像作为输入。作者使用了{320,352，…，608}大小的图像进行训练以提高模型的泛化性能。</li></ul><h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><p>大部分网络使用VGG16作为backbone，但是VGG有点臃肿。作者自定义了<strong>Darknet-19</strong>作为网络的backbone。</p><p><img src="https://i.loli.net/2019/12/03/MAWNrVztIbf3CjJ.png" alt="BPV.png"></p><h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><p>这部分讲了下YOLO9000和WordTree，但没有看懂而且好像不是很重要的亚子，跳了。</p><h1 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h1><p>待更新</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/woduoxiangfeiya/article/details/80866155" target="_blank" rel="noopener">YOLOv1论文翻译</a></p><p>[2]<a href="https://blog.csdn.net/guleileo/article/details/80581858" target="_blank" rel="noopener">从YOLOv1到YOLOv3，目标检测的进化之路</a></p><p>[3]<a href="https://www.jianshu.com/p/517a1b344a88" target="_blank" rel="noopener">YOLOv2 / YOLO9000 深入理解</a></p><p>[4]<a href="https://www.jianshu.com/p/b02f64e0d44b" target="_blank" rel="noopener">Yolo系列其二：Yolo_v2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;YOLOv1&quot;&gt;&lt;a href=&quot;#YOLOv1&quot; class=&quot;headerlink&quot; title=&quot;YOLOv1&quot;&gt;&lt;/a&gt;YOLOv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot; target
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>深度学习自动驾驶概述</title>
    <link href="http://a-kali.github.io/2019/11/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2019/11/05/深度学习自动驾驶概述/</id>
    <published>2019-11-05T09:45:08.000Z</published>
    <updated>2019-11-05T09:46:18.720Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目标：</strong>使用端到端的深度学习方法，根据车载摄像头的画面来判断如何<strong>打方向盘和踩油门</strong>。</p><p><img src="https://s2.ax1x.com/2019/11/05/MprAb9.png" alt="MprAb9.png"></p><p>参考论文：End to End Learning for Self-Driving Cars</p><p><strong>收集数据：</strong></p><p><img src="https://s2.ax1x.com/2019/11/05/MpBqT1.png" alt="MpBqT1.png"></p><p>汽车人为行驶时，其左中右三个摄像头、方向盘转向、油门、转向灯等数据都会通过其 CAN bus 传入处理器。而如今的汽车中基本都带有上述传感器帮忙训练神经网络；当汽车自动驾驶时，汽车根据中间摄像头传入的数据来操控方向盘等设备。</p><p><img src="https://s2.ax1x.com/2019/11/05/Mpye1K.png" alt="Mpye1K.png"></p><p><strong>自动驾驶模拟器：</strong></p><p><a href="https://imgchr.com/i/Mp6LR0" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/05/Mp6LR0.md.png" alt="Mp6LR0.md.png"></a></p><p>看起来很好玩的样子，有空看看源码了解下神经网络输出如何操控这些游戏。</p><p><strong>图像处理：</strong></p><ul><li>亮度调整（适应白天、晚上、阴天、晴天等情景）</li><li>归一化</li><li>图像切割（去除地平线以上和车头部分的无关紧要的数据）</li><li>水平翻转（左转右转）</li><li>数据平衡（欠采样、过采样、给样本少的数据更大权重、合成新数据）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;目标：&lt;/strong&gt;使用端到端的深度学习方法，根据车载摄像头的画面来判断如何&lt;strong&gt;打方向盘和踩油门&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/11/05/MprAb9.png&quot; al
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="概述" scheme="http://a-kali.github.io/tags/%E6%A6%82%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle days China, Oct 2019</title>
    <link href="http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/"/>
    <id>http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/</id>
    <published>2019-10-30T12:17:30.000Z</published>
    <updated>2019-11-09T15:52:47.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Winning-competitions-with-deep-learning-skills-—-SeuTao"><a href="#Winning-competitions-with-deep-learning-skills-—-SeuTao" class="headerlink" title="Winning competitions with deep learning skills — SeuTao"></a>Winning competitions with deep learning skills — SeuTao</h1><p><img src="https://s2.ax1x.com/2019/11/09/Mnu7rQ.png" alt="Mnu7rQ.png"></p><h2 id="Prepare-for-a-DL-competition"><a href="#Prepare-for-a-DL-competition" class="headerlink" title="Prepare for a DL competition"></a>Prepare for a DL competition</h2><ul><li>GPUs 是基础&amp;必要条件，但不是获得金牌的决定性条件。有着9块金牌的涛神在2019年也才只有2块1080ti而已。</li><li>多读 paper 是获得 idea 的关键，在很多 paper 中能找到相似问题的解决方案。</li><li>多读别人的代码。</li></ul><h2 id="Five-steps-to-Win-a-DL-competition"><a href="#Five-steps-to-Win-a-DL-competition" class="headerlink" title="Five steps to Win a DL competition"></a>Five steps to Win a DL competition</h2><ul><li>Understand the data</li><li>Build a strong baseline</li><li>Find the tricks</li><li>Ensemble</li><li>Pseudo-labels</li></ul><h3 id="Build-a-strong-baseline"><a href="#Build-a-strong-baseline" class="headerlink" title="Build a strong baseline"></a>Build a strong baseline</h3><ul><li>据涛神的看法，建立一个 <strong>strong baseline</strong> 是整个比赛中最重要的一环。一个高质量的 baseline 可以直接让你拿到<strong>银牌</strong>甚至 top15。可以建立一个高质量的 pipeline 并重复利用。</li><li>不要使用花里胡哨的神经网络架构和损失函数。这里大概可以理解为，baseline应使用简单轻量的神经网络，便于快速训练、调参、尝试 tricks。</li><li><strong>优化器</strong>：动量梯度下降或者 lr(3e-4) Adam优化器。优化器的改变对网络性能提升不大。</li><li><strong>学习率</strong>：可以尝试 warm up 和 余弦退火/cyclic lr</li><li>找到对数据合适的<strong>数据增强</strong>。</li><li>可靠的<strong>本地验证</strong>。在kaggle上提交验证相对麻烦而且有次数限制，而有一个可靠的本地验证就能快速地尝试验证各种 tricks。</li><li><strong>BatchNorm</strong>问题，基线很难高分的一个原因，涉及到神经网络细节。这里没看懂先挂张图：<img src="https://s2.ax1x.com/2019/11/09/Mn3W5j.png" alt="Mn3W5j.png"></li></ul><h3 id="Find-the-tricks"><a href="#Find-the-tricks" class="headerlink" title="Find the tricks"></a>Find the tricks</h3><ul><li>任务型 trick：图片分类trick、目标检测trick等。这些trick需要大量相关论文的积累。</li><li>数据型 trick：这需要你对数据敏锐的分析。数据相关的trick往往是制胜的关键。</li></ul><h3 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h3><p>融合技巧很重要，比如stacking、blending等</p><p><img src="https://s2.ax1x.com/2019/11/09/Mn82y6.png" alt="Mn82y6.png"></p><h3 id="Pseudo-labels"><a href="#Pseudo-labels" class="headerlink" title="Pseudo labels"></a>Pseudo labels</h3><ul><li>易于使用而且几乎在所有的深度学习竞赛中都奏效。</li><li>可以通过测试集或者外部数据来生成伪标签。</li><li>在比赛的最后stage使用——Overfit the LB then create pseudo labels（这个有点难理解）</li><li>注意不要 overfit 伪标签</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li>实验效率很重要，总结每一次实验经验，不管是成功还是失败。</li><li>在 kernel only 这种限制测试时间的比赛上，可以使用模型蒸馏、加速。</li><li>找到任务实质相关的论文。</li><li>熟读计算机视觉各个分支的论文，很可能会在之前读过的相关的论文上找到thick。</li></ul><h1 id="Tricks-of-image-classification-—-Jun-Lan"><a href="#Tricks-of-image-classification-—-Jun-Lan" class="headerlink" title="Tricks of image classification — Jun Lan"></a>Tricks of image classification — Jun Lan</h1><ul><li><p>图像分类大致可以分为两种：多类别分类（一个样本属于一个类别） vs 多标签分类（一个样本属于多个类别）</p></li><li><p>找到之前相似的比赛，观察高分solution</p></li><li><p>将数据增强后的图片可视化查看效果，根据任务选择增强方法</p></li><li><p>医疗影像预训练数据：MedicalNet。目前没开源2d数据</p></li><li><p>cycle learning rate：减少调参，更快收敛</p></li><li><p>多类别：交叉熵损失；多标签：二值交叉熵损失</p></li><li><p>mixup：一种数据增强的方法。将两张图片及其标签按一定比例进行融合</p></li><li><p>apex：基于pytorch的低精度运算（32位或16位）。减少显存占用，增加训练速度。pure float可能会导致精度损失和溢出。解决方案：混合精度训练。（设成O1就行了）</p></li><li><p>梯度累加（batch accumulation）：增大batch的方法，（多累积几步再更新梯度？）</p></li><li><p>伪标签：数据少或有大量额外数据且没有标签的情况下</p><ol><li>训练集训练模型</li><li>测试数据</li><li>将置信度较高的数据放入训练集（0.95、0，98）</li><li>再训练</li></ol></li><li><p>数据蒸馏（knowledge distillation）：使用小模型（student）来获取大模型（teacher）中的核心知识</p><ol><li>将数据集分为k折</li><li>k折交叉验证训练teacher model</li><li>预测out-of-fold的标签</li><li>在out-of-fold训练student model</li></ol></li></ul><hr><h1 id="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"><a href="#半年5战5金：Kaggle史上最快GrandMaster是如何炼成的" class="headerlink" title="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"></a>半年5战5金：Kaggle史上最快GrandMaster是如何炼成的</h1><p>下面内容跟 kaggle days 没什么关系，是一些很有用的 tricks。整理自网络，有删改，原文地址：<a href="https://zhuanlan.zhihu.com/p/89476481" target="_blank" rel="noopener">Kaggle你问我答【1】——SeuTao</a></p><p>这是 Kaggle 你问我答 (AMA) 的第一期活动，本期请到的嘉宾是 SueTao，他研究生毕业于东南大学，目前是腾讯的一名算法工程师。SueTao 擅长计算机视觉（Computer Vision），半年 5 战 5 金，也许是史上最快的 GrandMaster。截至目前共斩获 9 金 3 银，kaggle 最高排名全球第 10。</p><p>以下是本期活动的问答集锦：</p><p><strong>Q1：如何搭建kaggle data pipeline?</strong></p><p>A1：我目前的比赛还是集中在cv，也做过语音，还有前段时候的PMP，都是DL相关的竞赛。 数据的pipeline其实是可以积累并且优化的。我觉得可以参考一些前人的代码，尤其是蛙神的code。 可以在蛙神的code基础上，慢慢优化跟积累出自己的数据pipeline。 DL数据pipeline中还有个很重要的部分就是数据增强，这块针对不同比赛可能有不同的做法。</p><p><strong>Q2：自己曾经努力拿过银牌，但是觉得金牌好难，特别是solo的情况，请问金牌和银牌的差距在哪里，如何突破？</strong></p><p>A2：我还是从我参与比较多的cv竞赛角度出发哈。首先，如果你是cv新人，在kaggle竞赛上觉得拿金牌很困难，其实是很正常的。目前cv赛基本被cv高手霸榜了。 如果你是已经比较熟悉cv各个方向的模型，那你可能需要一个竞赛好手来给你带路。毕竟竞赛还是有很多套路的。 如果是新人，我的建议是坚持，通过几个cv竞赛来积累对这个方向的认识。了解不同模型不同任务。 我觉得可以参考padue，大家如果看他竞赛的成绩的话，开始他也只是银牌水平，但是从前段时间的protein开始，他现在在cv赛的水平基本就是solo gold了。 deep learning实践的积累还是很重要，一口吃不成胖子。</p><p><strong>Q4：新出的3d object比赛是不是一种趋势，请问涛神对computer vision的发展有什么观察和展望？</strong></p><p>A4：cv的话3d绝对是一个趋势，包括学术界和工业界； sensor的成本越来越低，性能也越来越好；就人脸识别来说，用3d来说安全性和可靠性就更高了。 其实我目前也算是退坑computer vision了，也谈不上对cv有深入的认识。大家从kaggle上cv赛的数量上可以发现，cv对企业的价值还是非常高的。前景是非常好，例如工业检测之类的。</p><p><strong>Q5：怎么判断该改进网络结构还是调学习率？</strong></p><p>A5：学习率和学习策略可能是搭建baseline里面最重要的部分。这块需要在比赛的前期优化到最好，建议使用简单的网络作为baseline，然后仔细优化学习策略。没有提升空间之后再考虑别的方向的优化。</p><p><strong>Q6：是否应该从分类错误的sample中提取灵感继续改进？如果是该怎么做？</strong></p><p>A6：cv最好的一点是可以看图，非常直观。举个例子：比如之前的鲸鱼竞赛，baseline模型的bad case大多是一些姿态较大，分辨率较差的图像。那么我们就可以考虑增加对应的数据增强。效果也很显著。 再举个反面例子：刚刚结束的nips的cellsignal竞赛，是细胞的荧光成像。整个比赛我完全没有看bad case。 因为没有domain知识，图像非自然，很难观察。 但是也不妨碍比赛能拿名次，只看log来调参。</p><p><strong>Q7：请评价cv 各项任务中 state of the art 模型的实用性，有何推荐？</strong></p><p>A7：“试过才有发言权”，这是我做kaggle之后的一个经验。没做kaggle之前，我工作集中在轻量级的模型，对于sota的大模型几乎没有尝试。所以我在竞赛中会尽量去尝试各种sota，最终会有很多有意思的结论。 会发现kaiming的resnet为什么强，unet为什么就是好用。 有些很fancy的模型真的只是过拟合特定的数据集。 我也没有尝试过所有的sota，但是我觉得paper里的内容看看就好，去伪存真，实践出真知。</p><p><strong>Q8：作为一个新人从头开始拿到金牌的最佳策略？比如选择比赛的类型？</strong></p><p>A8：哈哈 因为我cv一把梭，只能给到cv的经验。如果新人想拿金牌的话，最好就是找一个蛙神all in的比赛，step by step follow蛙神！只要比所有人都肝，有足够计算资源，对齐discussion report出来的模型精度，solo gold就有希望！ 其实我第一个比赛TGS就是这么做的。</p><p><strong>Q9：在kaggle学到的东西是否有应用到别的地方？能否举例说明？</strong></p><p>A9：非常多。举个例子：模型集成（ensemble）。可能有些人说模型集成在实际工作中用不了；工作中的场景有效率的要求；在计算资源受限的情况下，3个小模型集成的效果可能远好于1个大模型的效果。 我之前的参与的人脸项目，其实就用了这样的策略，很好用。但是如何去集成，怎么增大模型间diversity，这些技巧大家可以从kaggle上学习。</p><p><strong>Q10：回头看自己的经历，对刚入坑的新人，有什么想提醒的经验和教训？</strong></p><p>A10：教训到没有，做比赛一年感触还是蛮多的，投入越多收获越大吧。希望大家坚持。 真的只有投入去做了，才会有收获。</p><p><strong>Q11：CV比赛假如遇到瓶颈会往哪些方向尝试？</strong></p><p>A11：数据层面绝对是提分收益最大的方向；还是要多看数据，多分析bad case；不看数据就调网络结构是不可取的。 数据层面有些线索之后，可以指导你对模型结构本身做一些改进。另外最重要的：多看paper，paper是idea的来源。</p><p><strong>Q12：一般会用哪种方式平时积累知识？</strong></p><p>A12：过去很长一段时间内，我积累的方式还是来自比赛 通过一个比赛，我可以验证很多paper的方法，实践在工作中无法使用的模型；帮助我深入理解一些数据上和模型上的问题 感觉从我个人而言，比赛和工作相辅相成，给我工作提供了非常好的积累和储备。</p><p><strong>Q13：想知道打比赛的节奏是什么， 比如比赛结束前一个月， 一周， 几天主要干什么？</strong></p><p>A13：基本上最后一周前，最终方案就要定了。考虑最终的集成。</p><p><strong>Q14：有复现比赛top solution的习惯吗？ 有的话是一种怎样的方式呢？</strong></p><p>A14：会看，但是很少会跑。因为一直忙着做新的比赛。其实应该仔细去研究下的。</p><p><strong>Q15：分类比赛中的最后的sub的阈值应该根据什么来选取呢，有什么选取技巧呢？</strong></p><p>A15：我只能说可靠的local validation是最重要的，所有涉及模型选择，调参；其实都需要一个依据，local validation就是这个依据。这样问题就变成如何建立可靠的local validation了。</p><p><strong>Q16：分类比赛中最后的两个sub一般会怎么样选择呢，不同的方案的模型，还是其他？</strong></p><p>A16：这个问题比较好。前期几个比赛的sub一般都是我选的，有幸抽中过金牌。我个人的建议是，差异一定要大，一个激进一个保守。 就dl比赛来说，集成最稳的是weight ave，简单有效，一般来说我会选一个这个； 然后一些存在过拟合风险的方法，但是lb和cv都很可观的方案，我也会选择一个。</p><p><strong>Q17：请问经常看到各位大佬同时参加好几个比赛，还能拿到很好的名次，这是怎么做到的？</strong></p><p>A17：其实kaggle上的top CVer都会有自己积累下来的pipeline。竞赛任务无非是这几种，迅速搭建一个可靠的baseline，对top选手很容易； 看似在做多个竞赛，可能跑的是一套代码。真的要最终比赛冲刺了，会有针对性地去理解数据和优化。</p><p><strong>Q18：图像比赛有什么通用的技巧吗？厉害的选手一次提交就可以进到绿圈，细节处理上有什么独到之处？</strong></p><p>A18：DL调参的细节太多了，需要很长时间的积累。同样的数据+网络，不同人的训练结果可能相差巨大。这是top CVer的核心竞争力 通用技巧的话，paper上带着“bag of tricks”的都需要仔细阅读 bag of tricks for image classification， bag of tricks for object detection。</p><p><strong>Q19：想问下之前说没法做bad case的时候通过log调参是怎么调的， 另外一般bad case怎么样比较好的分析？</strong></p><p>A19：其实很简单: bias-variance trade off，只看log的话，拿捏好这个。 比如nips cellsignal比赛，baseline效果是，training拟合的非常好，test却非常差。其实是一种train test consistency。从1）数据层面；2）网络层面，去分析可能的情况。1）数据层面:数据分布的问题，2）网络层面：batchnorm。针对性地去做实验，确定问题所在，继续观察bias-variance，要得出可靠结论，再进行下一步。</p><p><strong>Q20：我这边自己写了个基于 pytorch 的轮子, 每次基本上能跟上 public kernel 的步伐, 但是就是很难超越. 我估计是训练资源和调参问题. 那么: 调参大部分用已经训练好的模型来调, 还是每次改变参数都重新训练个几天, 哪种方法对 top CVer 比较实际?</strong></p><p>A20：建议解决计算资源问题，保证快速学习，训练资源很重要，其实最优的实验周期我个人感觉在半天。 半天能出一个实验结果最好，中间可以干别的。 结果出得太快也不好，要及时总结和记录实验。</p><p><strong>Q21：之前看到有新闻说模型会用贴纸识别面包机，用肤色识别罪犯的这种过拟合的情况，还有aptos存在模型通过图片尺寸leak发现lable，有没有什么好办法避免这种情况？</strong></p><p>A21：我感觉过拟合问题其实比大家想象的更严重，之前做活体检测基本就是这么个情况，难以范化。 目前的DL还比较‘蠢’，要说办法的话，加数据算不算？</p><p><strong>Q22：问一个技术性问题，碰到一些受阈值影响的metrics时，训练的时候取最好的模型应该依据val-metrics还是val-loss呢？valid的时候如果遍历阈值，可能会极大的影响效率。不同模型/不同epoch，用不同阈值取得的metrics比较，会不会‘不公平’？</strong></p><p>A22：其实我也没有很好的答案。是我的话，最优的val-metrics和val-loss模型我都会存。其实最担心的是优化的loss和metrics不一致。</p><p><strong>Q23：还想问下对warmRestart这类的循环式的scheduler有什么看法？和传统的ReduceLROnPlateau相比有什么优劣？</strong></p><p>A23：最近发现这个真的很好用。如果用step LR的话，很可能下降的位置就不够好。循环的学习策略，我的感受是既不会有太多过拟合，也不需要很仔细调参，基本会有个不错的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Winning-competitions-with-deep-learning-skills-—-SeuTao&quot;&gt;&lt;a href=&quot;#Winning-competitions-with-deep-learning-skills-—-SeuTao&quot; class=&quot;h
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="kaggle days" scheme="http://a-kali.github.io/tags/kaggle-days/"/>
    
      <category term="kaggle" scheme="http://a-kali.github.io/tags/kaggle/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
      <category term="比赛技巧" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E6%8A%80%E5%B7%A7/"/>
    
      <category term="优化器" scheme="http://a-kali.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
      <category term="学习率" scheme="http://a-kali.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
      <category term="数据蒸馏" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E8%92%B8%E9%A6%8F/"/>
    
      <category term="伪标签" scheme="http://a-kali.github.io/tags/%E4%BC%AA%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>深度学习语义分割初期（FCN、UNet、SegNet）</title>
    <link href="http://a-kali.github.io/2019/10/26/FCN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://a-kali.github.io/2019/10/26/FCN论文解读/</id>
    <published>2019-10-26T02:06:38.000Z</published>
    <updated>2019-12-17T09:02:42.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p>论文地址：<a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>原始的 CNN 在图像的分类和定位任务中都获得了不错的成绩，但在分割任务中表现不佳。本文提出了一种<strong>全卷积网络(Fully Convolution Network, FCN)</strong>，通过进行像素级的预测(pixelwise prediction)来实现<strong>语义分割(semantic segmentaion)</strong>。</p><p><a href="https://imgchr.com/i/MUdneI" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/15/MUdneI.md.png" alt="MUdneI.md.png"></a></p><p>实现全卷积网络主要基于三种技术：</p><ul><li>全卷积化（Fully Convolutional）</li><li>反卷积（Deconvolution）</li><li>跃层结构（Skip Layer）</li></ul><h2 id="全卷积化"><a href="#全卷积化" class="headerlink" title="全卷积化"></a>全卷积化</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUB4hV.png" alt="MUB4hV.png"></p><p>简单来说就是把传统CNN最后的全连接层换成了卷积层。全卷积在多篇目标检测的论文中都有提到，其能提取出样本的特征图，样本目标区域对应特征图的感兴趣区域所在位置（如上图中的猫对应heatmap中的彩色像素）。</p><h2 id="上采样（Upsampling）"><a href="#上采样（Upsampling）" class="headerlink" title="上采样（Upsampling）"></a>上采样（Upsampling）</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUBJ6e.png" alt="MUBJ6e.png"></p><p>图像(图a)在经过卷积、池化等一系列处理后，得到的特征图(图b)分辨率远小于原图像。这样一来特征图中的像素无法与原图中一一对应，无法对每个像素进行预测。于是需要对特征图进行<strong>上采样</strong>以提高特征图的分辨率。文中对比了三种上采样的方法，最终选择了<strong>反卷积</strong>。</p><h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>反卷积是文章作者最终采用的方法，下面是两种反卷积的示例，图解起来十分直观：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBwkt.gif" alt="MUBwkt.gif"></p><p><img src="https://s2.ax1x.com/2019/11/15/MUBBff.gif" alt="MUBBff.gif"></p><p>下面是另一种解释，这样一看好像确实是把卷积的操作反过来了：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUB20s.png" alt="MUB20s.png"></p><h2 id="跃层结构-Skip-Layer"><a href="#跃层结构-Skip-Layer" class="headerlink" title="跃层结构(Skip Layer)"></a>跃层结构(Skip Layer)</h2><p>FCN 通过卷积和反卷积我们基本能定位到目标区域，但是，我们会发现模型前期是通过卷积、池化、非线性激活函数等作用输出了特征权重图像，我们经过反卷积等操作输出的图像实际是很粗糙的，毕竟丢了很多细节。因此我们需要找到一种方式填补丢失的细节数据，所以就有了<strong>跃层结构</strong>。</p><p>跃层结构将浅层的位置信息和深层的语义信息结合起来，得到更佳鲁棒的结果，其过程如图：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBz9K.png" alt="MUBz9K.png"></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUDAAI.png" alt="MUDAAI.png"></p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>训练过程分为四个阶段，也体现了作者的设计思路，值得研究。</p><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdWsPg.png" alt="MdWsPg.png"></p><p>使用数据集对模型的分类backbone进行预训练，使卷积层获得提取相应特征的能力。最后两层红色的是全连接层。</p><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdW5IU.png" alt="MdW5IU.png"></p><p> <strong>从特征小图（16×16×4096）预测分割小图（16×16×21），之后直接升采样为大图（300×300×21）。</strong>这里输出通道数为21的原因是：采用的PASCAL数据集中有20类，算上背景类一共21类。每个通道预测一类的像素。反卷积（橙色）的步长为32，故该网络被称为<strong>FCN-32s</strong>。</p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdosiV.png" alt="MdosiV.png"></p><p>这个阶段上采样分为两次完成（橙色×2）。 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）通过跃层结构融合进来，提升精确性。 第二次反卷积步长为16，这个网络称为<strong>FCN-16s</strong>。 </p><h3 id="第四阶段"><a href="#第四阶段" class="headerlink" title="第四阶段"></a>第四阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdTPSS.png" alt="MdTPSS.png"></p><p>这个阶段和第三阶段差不多，相较多了一次上采样。这大概是最终得出的FCN模型，因为同样的原因被称为<strong>FCN-8s</strong>。</p><p>比较这几个阶段的输出可以看出，跃层结构利用浅层信息辅助逐步升采样，有更精细的结果。 </p><p><img src="https://s2.ax1x.com/2019/11/15/MdTHkq.png" alt="MdTHkq.png"></p><h2 id="FCN-的缺点"><a href="#FCN-的缺点" class="headerlink" title="FCN 的缺点"></a>FCN 的缺点</h2><ol><li>分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节。</li><li>因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系。</li></ol><h1 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h1><p>论文地址：<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p><p><strong>U-Net</strong>是医学图像领域十分常用的一种分割网络，因为跟FCN十分相似，就放这里顺便讲了。</p><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/18/MyR5wj.png" alt="MyR5wj.png"></p><p>由于整个结构图呈”U”字型，故名”U-Net”。在知道FCN的原理后，从图中可以很明显地看出U-Net的结构和FCN没太大区别。其主要区别于以下几点：</p><ul><li><p>由于Unet的主要目标数据集为医学影像（最开始是细胞图像），只需要对每个像素点进行二值分割（有病/没病），故输出的特征图只有2个channel。(output segmentation: 388×388×2)</p></li><li><p>在上采样部分依然有大量的特征通道，使得网络可以将环境信息向更高的分辨率层传播。下采样和上采样部分几乎是对称的。</p></li><li><p>输入图像尺寸(572×572)和输出图像尺寸(388×388)不一样。这点似乎是为了配合一种名为<strong>overlap-tile</strong>的方法。如下图，使用左图蓝色区域预测右图黄色区域，滑动蓝色区域重复此操作直到预测完整张图片（这种细胞图尺寸通常都很大）。最终会导致最边上的蓝色区域没法预测，对于这部分使用<strong>镜像法(mirroring)</strong>外推。</p><p>注：关于这部分我也不太确定，想要了解详细原理可以去官网看原版的实现代码。</p><p><img src="https://s2.ax1x.com/2019/11/18/MyqBmq.png" alt="MyqBmq.png"></p></li><li><p>浅层特征和深层特征合并时，Unet使用的是拼接方法（图中白色模块，估计是为了保留更多的channel），而FCN使用的是求和。</p></li><li><p>用少量图像训练便能取得不错的效果，这点对医学领域图像数据集较少的特性十分友好。</p></li></ul><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p><p>说实话这篇文章没啥意思，就概括地科普一下吧。</p><p>首先架构还是和FCN一样，没啥变化，但文中将网络前面提取特征的部分称为<strong>编码器(Encoder)</strong>，后面上采样的部分称为<strong>解码器(Decoder)</strong>。这组词被沿用至今，可能就是在这里提出来的。</p><p><img src="https://i.loli.net/2019/12/04/OTcs6yDtWQJRoup.png" alt="U61.png"></p><p>然后整篇文章的亮点在于：解码器通过使用从相应的编码器接受的<strong>max-pooling索引</strong>来进行非线性上采样。这种方法<strong>减少了所需要训练的参数量，并且改善了边界划分效果</strong>。</p><p><img src="https://i.loli.net/2019/12/04/mPTJNcjrvlVC6qM.png" alt="E5QZ.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/fate_fjh/article/details/52882134" target="_blank" rel="noopener">卷积神经网络CNN（1）——图像卷积与反卷积（后卷积，转置卷积）</a></p><p>[2]<a href="https://blog.csdn.net/qq_31347869/article/details/89429211" target="_blank" rel="noopener">【论文笔记】FCN</a></p><p>[3]<a href="http://www.sohu.com/a/270896638_633698" target="_blank" rel="noopener">10分钟看懂全卷积神经网络（ FCN ）：语义分割深度模型先驱 </a></p><p>[4]<a href="https://blog.csdn.net/qq_36269513/article/details/80420363" target="_blank" rel="noopener">FCN的学习及理解（Fully Convolutional Networks for Semantic Segmentation）</a></p><p>[5]<a href="https://blog.csdn.net/qq_37274615/article/details/73251503" target="_blank" rel="noopener">FCN的理解</a></p><p>[6]<a href="https://blog.csdn.net/justpsss/article/details/77170004" target="_blank" rel="noopener">FCN和U-Net</a></p><p>[7]<a href="https://blog.csdn.net/natsuka/article/details/78565229" target="_blank" rel="noopener">U-net翻译</a></p><p>[8]<a href="https://blog.csdn.net/mieleizhi0522/article/details/82025509" target="_blank" rel="noopener">U-net论文解析</a></p><p>[9]<a href="http://tech.ifeng.com/c/7kx5uizAx5u" target="_blank" rel="noopener">一文带你读懂 SegNet（语义分割）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FCN&quot;&gt;&lt;a href=&quot;#FCN&quot; class=&quot;headerlink&quot; title=&quot;FCN&quot;&gt;&lt;/a&gt;FCN&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot; target=&quot;_blank&quot; rel
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="FCN" scheme="http://a-kali.github.io/tags/FCN/"/>
    
      <category term="CNN" scheme="http://a-kali.github.io/tags/CNN/"/>
    
      <category term="U-Net" scheme="http://a-kali.github.io/tags/U-Net/"/>
    
      <category term="反卷积" scheme="http://a-kali.github.io/tags/%E5%8F%8D%E5%8D%B7%E7%A7%AF/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN 系列论文解读</title>
    <link href="http://a-kali.github.io/2019/10/10/R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://a-kali.github.io/2019/10/10/R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-论文解读/</id>
    <published>2019-10-10T09:21:36.000Z</published>
    <updated>2019-12-04T08:46:09.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p>论文地址：<a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p>发布时间：2014.10.22</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>伴随着AlexNet的横空出世，卷积神经网络开始进入人们的视线，R-CNN便是将卷积神经网络运用于目标检测和语义分割的一个成功典范，其在 VOC 2012 将最佳mAP提高了30%。其成绩对卷积神经网络在目标检测的运用产生了深远的影响。</p><p>但在这之前，需要解决两个主要的问题：</p><ol><li>与图片分类不同，目标检测需要在图片上定位目标的位置。那么如何利用深度的神经网络去做目标的定位？</li><li>如何在一个小规模的数据集上训练能力强劲的网络模型？</li></ol><p>R-CNN全称为Regions with CNN features，其名字来源于其主要使用的两项技术：卷积神经网络（CNN）和<strong>区域推荐</strong>（Region Proposals），而区域推荐正是第一个问题的解决方法。当时已有许多现成的区域推荐算法，本文作者使用的是<strong>选择性搜索(selective search)算法</strong>。</p><h2 id="选择性搜索"><a href="#选择性搜索" class="headerlink" title="选择性搜索"></a>选择性搜索</h2><p><img src="https://s2.ax1x.com/2019/10/25/KwZVHS.png" alt="KwZVHS.png"></p><p>大概就是根据临近颜色的相似度将左边的原图变成像右边由色块组成的图片，然后根据色块选出候选框。这样可以减少对一些不必要的区域进行卷积运算，比如左图左上角那个框。该算法被后续几代网络沿用，直到 Faster R-CNN 使用神经网络进行区域推荐。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>R-CNN整体过程如下：</p><ol><li>给定一张输入图片，使用selective search从图片中提取 2000 个类别独立的候选区域。</li><li>将每个候选区域缩放到227×227，输入到 CNN中抽取一个固定长度的特征向量。</li><li>使用<strong>各个类别对应的SVM对特征向量进行二分类</strong>，判断该候选区域是否包含该类别，之后对每个类别的窗口进行极大值抑制。</li></ol><p><img src="https://img-blog.csdnimg.cn/20181210155342586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JyaWJsdWU=,size_16,color_FFFFFF,t_70" alt></p><p>对于第二个问题，作者给出的解决方法是：在大型图片分类数据集ILSVRC上预训练卷积神经网络，并微调（fine-tuning）到小型目标检测数据集PASCAL上，这使得mAP上升了8个百分点。</p><p>R-CNN高效的原因：</p><ol><li>所有类别共享CNN参数</li><li>特征维度相对较小</li></ol><h1 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h1><p>论文地址：<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p><p>发布时间：2015.4.23</p><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><p>由于 CNN 需要固定大小的输入，在将图片输入到神经网络之前需要对图片进行缩放(warp)或裁剪(crop)。缩放会造成图片几何失真，而裁剪则可能损失部分目标物像素，这将会对图片识别精确度有所影响。</p><p><img src="https://s2.ax1x.com/2019/10/22/K8nKrd.png" alt="K8nKrd.png"></p><p>CNN 只能接收固定尺寸图片的原因是其全连接层节点数目固定，而其卷积层是可以接收不同尺寸的图片的。于是作者设计了用于神经网络中的 <strong>SPP</strong> (spatial pytamid pooling, 空间金字塔池化) 模块，位于卷积层和全连接层之间，用于<strong>接收任意尺寸的图片、提取其特征并产生固定大小的输出</strong>。而且实验表明，训练时使用不同尺寸的输入，可以提高测试精度。</p><h2 id="空间金字塔池化层"><a href="#空间金字塔池化层" class="headerlink" title="空间金字塔池化层"></a>空间金字塔池化层</h2><p><img src="https://s2.ax1x.com/2019/10/22/KGQ98P.png" alt="KGQ98P.png"></p><p>作者将 CNN 中的最后一个池化层用 SPP 替代。如图所示，<strong>SPP 将最后一层卷积层输出的特征图分割成不同尺寸的网格，分别为4×4、2×2、1×1，然后对每个小格进行max pooling，再将池化后的结果连接起来，就能得到（16+4+1）× 256 的固定长度的输出</strong>（这里的256为256个channel）。</p><h2 id="SPP-在目标检测中的应用"><a href="#SPP-在目标检测中的应用" class="headerlink" title="SPP 在目标检测中的应用"></a>SPP 在目标检测中的应用</h2><p>前面提到，R-CNN 在图像中选出2000个候选窗口，并将每个窗口缩放后输入到神经网络中，这样对一张图片反复使用深度卷积网络十分耗时。测试时，特征提取是其主要的时间瓶颈。</p><p>论文中提到，特征图的ROI与原图中的目标物的位置存在一定的映射关系，如下图：</p><p><img src="https://s2.ax1x.com/2019/10/22/KG6IIA.png" alt="KG6IIA.png"></p><p>于是<strong>对于一张图片，只需要提取一次特征，然后将特征图的2000个候选区域输入 SPP 模块就能得到固定长度的表示。由于只需要进行一次卷积操作，节省了大量候选区域通过神经网络的时间。</strong></p><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><p>论文地址：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></p><p>发布时间：2015.9.27</p><h2 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h2><p>SPPnet 虽然对R-CNN进行了一些改进，但仍然存在许多问题：</p><ul><li>需要大量产生候选框</li><li>对目标的定位只能靠候选框来粗略定位</li><li>多阶段pipeline，特征提取、模型训练、SVM分类器训练、边框回归要分别进行</li><li>特征图要存在本地磁盘，影响速度</li></ul><p>于是 Fast R-CNN 改进了在目标检测任务中的性能，其优势如下：</p><ul><li>相比 R-CNN、SPPnet 有着更高的 mAP</li><li>单阶段(single-stage)训练，使用多任务损失(multi-task loss)</li><li>训练可以更新网络每一层的参数</li><li>无需使用磁盘缓存特征</li></ul><h2 id="架构细节和模型训练"><a href="#架构细节和模型训练" class="headerlink" title="架构细节和模型训练"></a>架构细节和模型训练</h2><p><img src="https://s2.ax1x.com/2019/10/23/KtoCi6.png" alt="KtoCi6.png"></p><p>从上图直观上来看，Fast R-CNN 与 SPPnet 的结构有两个区别：</p><ol><li>SPP模块被换成了RoI池化层</li><li>网络末端有两个输出，分别用于图像分类和边框回归。分类器被换成了softmax。使用softmax的好处在于不用单独训练一个SVM分类器；缺点在于对于一个候选框最多只能分出一类物体，即使一个候选框包含了多个类别的目标（大概）。</li></ol><p>另外值得一提的是，Fast R-CNN 采用的是固定大小的输入，而不像SPPnet使用任意大小的输入。</p><h3 id="RoI-池化层"><a href="#RoI-池化层" class="headerlink" title="RoI 池化层"></a>RoI 池化层</h3><p>RoI 池化层实质上就是单层的 SPP 模块。其将一个候选窗口划分为 H×W 的网格，对每个网格内进行最大池化，最后输出一个长度为 H×W 的特征。超参数 H 和 W 视具体网络结构而定。</p><h3 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h3><p>多任务损失由分类任务损失和边框回归任务损失线性组合而成：</p><script type="math/tex; mode=display">L=L_{cls}(p,u)+\lambda [u\geq 1]L_{loc}(t^u,v)\\</script><p>其中：</p><script type="math/tex; mode=display">L_{cls}(p,u)=-\log p_u\\L_{loc}(t^u,v)=\sum smooth_{L_1}(t^u_i-v_i)</script><h3 id="Mini-batch-sampling"><a href="#Mini-batch-sampling" class="headerlink" title="Mini-batch sampling"></a>Mini-batch sampling</h3><p>（其实这一段我没看太懂，以下仅作参考）</p><p>在调优(fine tuning)训练时，每个mini-batch中首先加入 N 张完整图片，从 N 张图片中选出一共 R 个 IoU&gt;0.5 的候选区域，然后将这 R 个候选区域作为训练样本放入网络训练。</p><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>论文地址：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><p>发布时间：2016.1.6</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>性能优越的目标检测网络都依赖区域推荐(region proposal)算法来假定目标位置，比如R-CNN中的选择搜索(search selective)算法，而这些区域推荐的计算消耗正是整个网络性能的瓶颈。本文作者引入了<strong>区域推荐网络(Region Proposal Network, RPN)</strong>，尝试使用神经网络来进行区域提取。并将 RPN 和 Fast R-CNN 融合在一起，共享卷积特征，成为一个端到端的神经网络。</p><h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><p><img src="https://s2.ax1x.com/2019/10/25/KdqldS.png" alt="KdqldS.png"></p><p>Fast R-CNN 大致结构如图。可以看出，网络由四步组成：</p><ol><li>输入的图片经过卷积层输出一张特征图</li><li>将特征图输入 RPN，得到候选区域</li><li>将特征图上候选区域的对应位置输入到 RoI 池化层</li><li>输入到分类器得出分类结果</li></ol><p>那么 RPN 具体是怎样的呢？</p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p><img src="https://s2.ax1x.com/2019/10/25/KdIwZQ.png" alt="KdIwZQ.png"></p><p>从上图中可以看到Faster R-CNN更具体的结构，包括左下方的RPN模块。RPN具体流程如下：</p><ol><li><p>使用<strong>滑动窗口(slide window)</strong>遍历整个特征图(feature map)，遍历过程中以每个window中心产生9个预设<strong>锚框(anchor)</strong>，9个锚框分别对应3种尺寸和3种长宽比。</p><p><img src="https://s2.ax1x.com/2019/10/25/KwJZQS.png" alt="KwJZQS.png"></p></li><li><p>将锚框分别输入到<strong>线性分类层(cls layer)</strong>和<strong>边框回归层(reg layer)</strong>中。分类层通过softmax对锚框进行二分类，初步判断该锚框是前景还是背景（锚框里是否包含目标物）；回归层通过边框回归进一步修正锚框，使锚框定位更精确。</p><p><img src="https://s2.ax1x.com/2019/10/26/K0RaZ9.png" alt="K0RaZ9.png"></p></li><li><p>将筛选、修正后的锚框映射到特征图上，输入到ROI池化层。后续操作和Fast R-CNN一样。</p></li></ol><h2 id="Faster-R-CNN-的训练步骤"><a href="#Faster-R-CNN-的训练步骤" class="headerlink" title="Faster R-CNN 的训练步骤"></a>Faster R-CNN 的训练步骤</h2><ol><li>训练一个用于分类的 CNN（用于特征提取）</li><li>使用 CNN 的特征图作为输出，端到端的fine-tune RPN + CNN。IoU&gt;0.7的作为正样本，IoU&lt;0.3的为负样本。</li><li>固定RPN的权值，训练整个网络。</li><li>固定其余部分的权值，训练RPN</li><li>固定CNN、RPN，训练其余部分</li><li>重复步骤4、5直到满意为止</li></ol><p>由于Faster R-CNN的训练步骤过于繁杂，促使了后续的SSD网络对其进行改进。</p><h2 id="R-CNN-家族的总结"><a href="#R-CNN-家族的总结" class="headerlink" title="R-CNN 家族的总结"></a>R-CNN 家族的总结</h2><p><img src="https://i.loli.net/2019/12/04/4kIh9YJQwTuZvEX.png" alt="57KD5.png"></p><p>图源：贪心学院</p><h1 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h1><p>（待更新）</p><h1 id="相关面试题"><a href="#相关面试题" class="headerlink" title="相关面试题"></a>相关面试题</h1><p><strong>Q：讲下faster-rcnn？Faster-rcnn里面的NMS的算法原理是什么？</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;R-CNN&quot;&gt;&lt;a href=&quot;#R-CNN&quot; class=&quot;headerlink&quot; title=&quot;R-CNN&quot;&gt;&lt;/a&gt;R-CNN&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot; target=
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="R-CNN" scheme="http://a-kali.github.io/tags/R-CNN/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉算法岗面试归纳（持续解答ing）</title>
    <link href="http://a-kali.github.io/2019/10/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B2%97%E9%9D%A2%E8%AF%95%E5%BD%92%E7%BA%B3/"/>
    <id>http://a-kali.github.io/2019/10/05/计算机视觉算法岗面试归纳/</id>
    <published>2019-10-05T01:32:59.000Z</published>
    <updated>2019-12-12T13:57:25.353Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h1><ul><li>介绍一下调参的经验</li><li>Softmax的公式和伪代码</li><li>分类常见的指标有什么，如何理解AUC？</li><li>介绍决策树、RF、XGBoost、GBDT和 LightGBM</li><li>XGboost的loss函数的推导（mse以及非mse形式），以及求解推导。</li><li>使用O(N)复杂度完成GBDT分裂</li><li>介绍 F1-score，AUC，交叉熵，ROC</li><li>介绍 Adboost，GBDT，XGBoost</li><li>介绍不同的聚类算法：K-Means、GMM、DBSCAN等</li><li>CCA和PCA的区别</li><li>牛顿法能用于非凸函数吗？</li><li>XGBoost里处理缺失值的方法</li><li>样本不平衡对 SVM 的影响</li><li>KNN和Kmeans的算法中K的含义，K对算法的影响，怎么选择K</li><li>LR的全过程，从train到inference，损失函数</li><li>介绍常见的集成方法</li><li>LR + softmax做多分类和LR + multiLoss 做多分类区别在哪里</li><li>LR为什么用交叉熵作为loss函数</li><li>Kmeans的缺点？如何改善？</li><li><a href="https://a-kali.github.io/2019/12/02/k-means-聚类算法/">讲一下K-means算法的过程以及原理</a></li><li>为什么Bagging降方差，Boosting降偏差？</li><li>介绍XGBoost对GBDT的提升，LightGBM对XGBoost的提升</li><li>为什么要对连续型数值进行离散化，这样做有什么优势</li><li>LR 为什么用sigmoid函数？</li><li>怎么解决样本不均衡（重点考核损失函数优化）</li><li>HMM 和 CRF的区别</li><li>XGBoost 如何处理缺失数据？</li><li>写一下 LR 和 SVM 的损失函数</li><li>正负样本不均衡时的解决方案</li><li>知道哪些降维的方法，具体讲讲</li><li>线性模型和非线性模型都有哪些？</li><li>手写AUC的计算（小矩形积分得到总面积即可）</li><li>决策树分支的原理</li><li>offerpolicy 和 onpolicy 的区别</li><li>为什么随机森林的树比 GBDT 的深一点？</li><li>逻辑回归的目标函数(损失函数)是凸函数吗？</li><li>完全二叉树的概念</li><li>朴素贝叶斯与贝叶斯有什么区别？</li><li>SVM 为什么变成对偶问题来求解？</li><li>缺失值如何处理，什么情况下均值、众数，什么情况下丢弃特征。</li><li>诸如ID类的特征如何处理，编码方式one-hot还是其他的，高维时？什么样才算高维，有没有界定？</li><li>聚类的算法有哪些？评价方法？优化算法？</li><li>解释几何间隔和函数间隔</li><li>描述决策树，如何选特征，怎么划分，怎么剪枝，介绍信息增益</li><li>K-Means 聚类这种方法一定会收敛嘛？如果不收敛，怎么办？</li><li>SVM 的目标函数，为什么能用拉格朗日乘子法讲原始最优化问题转化为极大极小问题，数学原理是什么</li><li>介绍SVM，其中的软间隔是什么意思？</li><li>使用线性回归的时候什么时候会需要用L2？</li><li>如果F1已经趋于平稳，如何在保持F1稳定的前提下提高precision，降低recall；</li><li>LR 为什么不用 MSE，SVM 为什么用hinge不用logloss</li><li>XGBoost 怎么解决过拟合？怎么剪枝？怎么选择特征？怎么处理缺失值？</li><li>XGBoost 的默认深度</li><li>各种决策树模型的优劣（从最简单的ID3到最后的LGB）</li><li>SVM 核函数哪些是高维空间维度已知，哪些是未知的？</li><li>LR介绍、LR对特征需要做什么特殊处理吗？类别特征、连续特征</li><li>损失函数正则项的本质是什么? </li><li>SVM 有哪些核函数？</li><li>L1 正则化为什么能使特征稀疏？</li><li>Stacking原理，还有怎么调优？</li><li>XGBoost怎么调参？用了多少棵树？</li><li>各种决策树模型的优劣（从最简单的ID3到最后的LGB）</li><li>ID3 C4.5 CART的区别</li><li>手推 SVM, GBDT, XGBoost</li><li>CRF 怎么训练的（传统+深度学习）</li><li>得到AUC的两种计算方法</li><li>树的分裂方式（id3,gini,gdbt,xgboost）</li><li>监督学习的概念？什么是随机森林，随机森林的优点？</li><li>LR和SVM区别（计算复杂度）</li><li>Adam优化器的迭代公式</li><li>SGD每步做什么，为什么能online learning</li><li>L1 L2正则化区别</li><li>PCA原理和执行步骤</li><li>特征工程知道吗？举几个特征归一化的例子</li><li>SVM为什么可以处理非线性问题</li><li>L1正则化的先验分布？</li><li>L1的不知道，L2的先验分布知道吧？</li><li>多标签分类问题怎么解决，从损失函数角度考虑</li></ul><h1 id="NN"><a href="#NN" class="headerlink" title="NN"></a>NN</h1><ul><li><a href="https://a-kali.github.io/2019/12/01/激活函数汇总/">激活函数除了Sigmoid tanh ReLU 还有什么介绍一下</a></li><li>BFE 和 Dropout的关系</li><li>Dropout是失活神经元还是失活连接</li><li>手推梯度反向传播</li><li>分类网络样本不均衡怎么办？</li><li>dropout层作用，如何实现有什么作用？</li><li>Dropout 前向和反向的处理</li><li>神经网络如果没有激活函数还能解决线性不可分问题吗？</li><li>Tensorflow的动态图和静态图有什么区别</li><li>GN，BN，LN，IN 它们的共性和特性</li><li>为什么BN有泛化能力的改善. 什么场景用什么normalization方法，效果如何.</li><li>Dropout为什么能防止过拟合？具体实现</li><li>dropout在训练和测试时不同，怎么保证测试结果稳定</li><li>如何计算神经网络的 FLOPS？</li><li>梯度下降陷入局部最优有什么解决办法</li></ul><h1 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h1><ul><li>手写灰度直方图代码</li><li>介绍一下开运算和闭运算</li><li>介绍双目相机识别目标深度的原理</li><li>单目视觉如何测量深度？</li><li>介绍常见的边缘检测算法</li><li>SIFT 特征是如何保持旋转不变性的？</li><li>如何快速判断图中有环？</li><li>介绍常见的边缘检测算子</li><li>Hough 变换原理（直线和圆检测）</li><li>为什么 Sobel 算子中间是2，两边是1</li><li>算法题：实现 OpenCV中的图像缩放，包括实现双线性插值</li><li>输入图像灰度值对模型的影响，为什么要把0-255转化成0-1？</li><li>介绍 RANSAC</li><li>介绍一阶二阶边缘检测算子一阶二阶边缘检测算子</li><li>OpenCV里面findcontour函数的原理是什么？</li><li>相机里面的标定参数有哪些？是怎么计算这些参数的？</li><li>如何求边缘，45°边缘，高斯滤波和双边滤波</li><li>代码题：手撕实现图像的resize和rotate90度</li><li>手写中值滤波</li><li>介绍一下高斯滤波，均值滤波，中值滤波</li><li>SIFT特征提取怎么做的，具备什么性质，为什么</li><li>讲一下CTC的原理</li><li>夜间拍照的多图对齐和融合</li></ul><h1 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h1><ul><li>介绍你读到的19年 Anchor-free 目标检测论文</li><li>简单介绍Fast RCNN -&gt; Faster RCNN -&gt; mask RCNN (这个真的好高频)</li><li>256×256×3 -&gt; 128×128×64的卷积，stride，padding和待优化的参数有多少</li><li>手撕 SoftNMS代码</li><li>CNN反向传播公式推导；参数共享指的是？</li><li>介绍熟悉的NAS网络</li><li>介绍目标检测中的多尺度训练/测试？</li><li><a href="https://a-kali.github.io/2019/09/01/ResNet-CVPR-2016/">为什么 DenseNet 比 ResNet 更耗显存？</a></li><li>为什么深度学习中的图像分割要先编码再解码？</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">1×1 卷积有什么作用？</a></li><li>如何计算语义分割的 mIoU（写伪代码）</li><li>原始图片中的 RoI 如何映射到 feature map ?</li><li>PyTorch的高效convolution实现</li><li>PyTorch 不用库函数如何实现多机多卡</li><li>哪些情况用 MaxPool比AveragePool效果好？原因</li><li>介绍Anchor based 和Anchor free目标检测网络的优缺点</li><li>YOLOv3在小缺陷检测上也很好，RPN上和two-stage的有什么区别</li><li>MobileNetV2 module的参数量和FLOPs计算</li><li>CNN 的感受野受什么影响</li><li>CNN 如何保持平移方向不变性</li><li>如果分类的数据图像每一类只有几张，你会用什么方法？</li><li>RPN怎么计算 box 的实际坐标</li><li>介绍常见的 Anchor free 目标检测算法</li><li>算法题：编程实现目标检测中的 IoU 计算</li><li>公式及讲解soft attention，hard attention，multi head attention</li><li>卷积操作是线性的吗？CNN是线性的吗？为什么？（激活函数）常用的激活函数？</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">3×3 卷积核 与 5×5 卷积核相比的优点</a></li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">InceptionV1~V4系列介绍，以及每一版的改进，优缺点介绍</a></li><li>CNN Maxpooling 怎么反向传播？</li><li>写出 YOLOv3 的损失函数</li><li>YOLOV1~V3系列介绍，以及每一版的改进，优缺点介绍</li><li>介绍金字塔池化，ASPP，深度可分，带孔卷积</li><li>VGG网络什么特点，用到了哪几种卷积核？</li><li>介绍 anchor-based和anchor-free两者的优缺点</li><li>PyTorch 多gpu训练机制的原理，优化器以及网络参数保存机制</li><li>讲下faster-rcnn？Faster-rcnn里面的NMS的算法原理是什么？</li><li>Mask R-CNN 如何提高mask的分辨率？</li><li>普通卷积、DW+PW卷积计算量推导</li><li>MobileNet V2中的Residual结构最先是哪个网络提出来的</li><li>CornerNet介绍，CornerPooling是怎么做的，怎么解决cornernet检测物体合并为一个框的问题</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">GoogLeNet中为什么采用小的卷积核？</a></li><li><a href="https://a-kali.github.io/2019/10/26/FCN论文解读/">说一下UNet的结构</a></li><li>熟悉deeplab吗，aspp是怎样的，与其他的state-of-art的模型对比，deeplab还可以做哪些改进？</li><li>retinanet的focal loss是解决的什么问题</li><li>CRF后处理的目的</li><li>介绍deeplabv3，画出backbone（串联和并联），论文中认为这两种哪种方式更好？如何避免friding efect、deeplabv3的损失函数</li></ul><h1 id="SLAM"><a href="#SLAM" class="headerlink" title="SLAM"></a>SLAM</h1><ul><li>PnP求解最少需要几个点？</li><li>ORBSLAM的哪个部分最耗时？</li><li>ORBSLAM怎么克服尺度漂移问题？</li><li>回环原理讲一下，要估计哪些量？</li><li>后端BA中，如何存在outlier一般怎么解决？</li><li>BA中，海塞矩阵的求逆有哪些可以加速的方法？</li><li>单应矩阵(homography)为什么只有8个自由度？</li><li>如何设计一个视觉+IMU+RTK+Lidar的定位系统？</li><li>对于光照明暗变化、动态场景，视觉SLAM如何去解决？</li><li>ROS中，node属于多进程，如何把两个node放在一个进程中？</li><li>ORBSLAM 后端H矩阵求解的算法复杂度是多少？如何去加速后端求解？</li><li>ORB-SLAM的初始化步骤</li><li>介绍 Bundle Adjustment</li><li>机器人学中表示旋转的方式有哪些？区别是什么？</li><li>检测圆的方法有哪些？</li><li>霍夫圆变换的原理是什么？</li><li>你知道哪些点云匹配的算法？原理是什么？</li><li>ROS里面的一些基本操作怎么实现？</li><li>怎么估计3D姿态？用什么表示姿态？</li><li>相机标定方法与流程，内外参矩阵求解</li><li>什么是闭环检测？常用的方法有哪些？你用的哪种方法？有没有创新？</li><li>解释一下Gauss-Netwon和LM算法。</li><li>熟悉Ceres优化库吗？说一下。</li><li>描述（扩展）卡尔曼滤波与粒子滤波，你自己在用卡尔曼滤波时遇到什么问题没有？</li><li>除了视觉传感，还用过其他传感吗？比如GPS，激光雷达。。。</li></ul><h1 id="反向面试"><a href="#反向面试" class="headerlink" title="反向面试"></a>反向面试</h1><p>再也不用担心面试官灵魂拷问：你有什么要问我的么？</p><p>下面列表里的问题对于参加技术面试的人来说很有用：</p><ul><li>我的日常工作是什么？</li><li>入职培训会是什么样的？</li><li>你们怎么使用源码控制系统？</li><li>团队内/团队间的交流通常是怎样的？</li><li>有标准的开发环境吗？是强制的吗？</li><li>我可以为开源项目做贡献吗？是否需要审批？</li><li>团队里面初级和高级工程师的比例是多少？</li><li>晋升流程是怎样的？要求/预期是怎样沟通的？</li><li>我入职的岗位是新增还是接替之前离职的同事？</li><li>入职之后在哪个项目组，项目是新成立还是已有的？b公司是否有技术分享交流活动？有的话，多久一次呢？</li><li>更多提问可以在 <a href="https://github.com/yifeikong/reverse-interview-zh" target="_blank" rel="noopener">https://github.com/yifeikong/reverse-interview-zh</a> 找到</li></ul><p>出了以上几种类型的题目，还常见编程算法题、C++语言细节、Python语言细节、英语题、数学题、项目、计算机网络和操作系统</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ML&quot;&gt;&lt;a href=&quot;#ML&quot; class=&quot;headerlink&quot; title=&quot;ML&quot;&gt;&lt;/a&gt;ML&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;介绍一下调参的经验&lt;/li&gt;
&lt;li&gt;Softmax的公式和伪代码&lt;/li&gt;
&lt;li&gt;分类常见的指标有什么，如何理解AUC？&lt;/
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="SLAM" scheme="http://a-kali.github.io/tags/SLAM/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>周志华《机器学习》</title>
    <link href="http://a-kali.github.io/2019/09/16/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8A%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B/"/>
    <id>http://a-kali.github.io/2019/09/16/周志华《机器学习》/</id>
    <published>2019-09-16T11:28:39.000Z</published>
    <updated>2019-09-18T12:36:28.450Z</updated>
    
    <content type="html"><![CDATA[<h1 id="第-1-章-绪论"><a href="#第-1-章-绪论" class="headerlink" title="第 1 章    绪论"></a>第 1 章    绪论</h1><h2 id="1-2-基本术语"><a href="#1-2-基本术语" class="headerlink" title="1.2 基本术语"></a>1.2 基本术语</h2><p>假设（hypothesis）：根据数据的潜在规律学习而得的模型。亦称为学习器。</p><p>簇（cluster）：聚类学习中的一个组。</p><p>泛化（generalization）：学得模型适用于新样本的能力。</p><h2 id="1-3-假设空间"><a href="#1-3-假设空间" class="headerlink" title="1.3 假设空间"></a>1.3 假设空间</h2><p>假设空间：机器学习中可能的函数构成的空间。学习的过程即是在假设空间中进行搜索的过程。</p><h1 id="第-2-章-模型评估与选择"><a href="#第-2-章-模型评估与选择" class="headerlink" title="第 2 章    模型评估与选择"></a>第 2 章    模型评估与选择</h1><h2 id="2-2-评估方法"><a href="#2-2-评估方法" class="headerlink" title="2.2 评估方法"></a>2.2 评估方法</h2><h3 id="2-2-1-留出法"><a href="#2-2-1-留出法" class="headerlink" title="2.2.1 留出法"></a>2.2.1 留出法</h3><p><strong>留出法</strong>（hold-out）将数据集划分为两个互斥的集合，分别作为训练集和测试集。</p><h3 id="2-2-2-交叉验证法"><a href="#2-2-2-交叉验证法" class="headerlink" title="2.2.2 交叉验证法"></a>2.2.2 交叉验证法</h3><h3 id="2-2-3-自助法"><a href="#2-2-3-自助法" class="headerlink" title="2.2.3 自助法"></a>2.2.3 自助法</h3><p><strong>自助采样法</strong>（bootstrap sampling）对大小为 m 的数据集进行 m 次放回采样，采样得到的数据作为训练集，初始数据集中大约有 36.8% 的数据未被采样过，这部分数据作为测试集。</p><p>自助法在数据集较小、难以划分测试集和训练集时比较有用。但会改变原有数据集的分布，引入估计偏差。</p><h3 id="2-2-4-调参与最佳模型"><a href="#2-2-4-调参与最佳模型" class="headerlink" title="2.2.4 调参与最佳模型"></a>2.2.4 调参与最佳模型</h3><p>模型评估与选择中，用于评估模型的数据集常称为<strong>验证集</strong>。</p><h2 id="2-3-性能度量"><a href="#2-3-性能度量" class="headerlink" title="2.3 性能度量"></a>2.3 性能度量</h2><p>性能度量：对模型泛化能力的评价标准。</p><p>均方误差（mean squared error）：$E(f;D)=\frac{1}{m} \sum^m_{i=1}(f(x_i)-y_i)^2.$ 常用于回归任务中。</p><h3 id="2-3-1-错误率与精度"><a href="#2-3-1-错误率与精度" class="headerlink" title="2.3.1 错误率与精度"></a>2.3.1 错误率与精度</h3><ul><li><strong>错误率</strong>（error rate）：分类错误的样本数占样本总数的比例</li><li><strong>精度</strong>（accuracy）：分类正确的样本数占样本总数的比例</li></ul><p>此处的评估标准仅仅是根据样本分类的正误个数进行评估，没有表现出单个样本的错误程度。</p><h3 id="2-3-2-查准率、查全率与-F1"><a href="#2-3-2-查准率、查全率与-F1" class="headerlink" title="2.3.2 查准率、查全率与 F1"></a>2.3.2 查准率、查全率与 F1</h3><p>在信息检索等应用场景中经常出现如下的需求，比如想知道“检索出的信息中有多少比例是用户感兴趣的”“用户感兴趣的信息中有多少被检索出来了”。此时用<strong>查准率</strong>（precision）和<strong>查全率</strong>（recall，也被称为召回率）更为适合此类需求。</p><p>混淆矩阵：</p><p><a href="https://imgchr.com/i/nfoRB9" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/09/16/nfoRB9.png" alt="nfoRB9.png"></a></p><p>查准率 P 和查全率 R 分别被定义为</p><script type="math/tex; mode=display">P=\frac{TP}{TP+FP}\\R=\frac{TP}{TP+FN}</script><p>查全率和查准率是一对矛盾的度量。一般来说，查全率高时查准率低，查准率高时查全率低。</p><p>P-R曲线、ROC和AUC可参考<a href="https://a-kali.github.io/2019/09/03/机器学习中的评价指标/">机器学习中的评价指标</a>。</p><h3 id="2-3-4-代价敏感错误率与代价曲线"><a href="#2-3-4-代价敏感错误率与代价曲线" class="headerlink" title="2.3.4 代价敏感错误率与代价曲线"></a>2.3.4 代价敏感错误率与代价曲线</h3><p>不同类型的错误所造成的后果不同，为权衡不同类型错误所造成的不同损失，可以为错误赋予<strong>非均等代价</strong>。</p><p>在非均等代价下，我们所希望的不再是简单地最小化错误次数，而是希望最小化<strong>总体代价</strong>。以二分类为例，其代价敏感错误率为：</p><script type="math/tex; mode=display">E = \frac{1}{m}(\sum_{x_i\in D^+}I(f(x_i)\not=y_i)\times cost_{01}+\sum_{x_i\in D^-}I(f(x_i)\not=y_i)\times cost_{10})</script><p>其中$I(·)$为指示函数，$cost$为错误的权重（即代价）。</p><p><strong>代价曲线</strong>可以直接反映非均等代价下学习器的期望总体代价。代价曲线的绘制很简单：ROC曲线上的每一点对应了代价平面上的一条线段，根据ROC曲线上的每一点的状态绘制一条从(0,FPR) 到 (1, FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价。</p><p><img src="https://s2.ax1x.com/2019/09/18/n7Ri7j.png" alt="n7Ri7j.png"></p><h1 id="第-3-章-线性模型"><a href="#第-3-章-线性模型" class="headerlink" title="第 3 章    线性模型"></a>第 3 章    线性模型</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;第-1-章-绪论&quot;&gt;&lt;a href=&quot;#第-1-章-绪论&quot; class=&quot;headerlink&quot; title=&quot;第 1 章    绪论&quot;&gt;&lt;/a&gt;第 1 章    绪论&lt;/h1&gt;&lt;h2 id=&quot;1-2-基本术语&quot;&gt;&lt;a href=&quot;#1-2-基本术语&quot; class
      
    
    </summary>
    
      <category term="机器学习" scheme="http://a-kali.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Inception v1-v4 论文解读</title>
    <link href="http://a-kali.github.io/2019/09/04/Inception-v1-v4/"/>
    <id>http://a-kali.github.io/2019/09/04/Inception-v1-v4/</id>
    <published>2019-09-04T09:57:36.000Z</published>
    <updated>2019-12-02T09:38:59.617Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Inception-V1"><a href="#Inception-V1" class="headerlink" title="Inception V1"></a>Inception V1</h1><p>论文地址：<a href="https://arxiv.org/pdf/1409.4842v1.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a></p><h2 id="动机与深层思考"><a href="#动机与深层思考" class="headerlink" title="动机与深层思考"></a>动机与深层思考</h2><p>直接提升神经网络性能的方法是提升网络的深度和宽度。然而，更深的网络意味着其参数的大幅增加，从而导致计算量爆炸。因此，作者希望能在计算资源消耗恒定不变的条件下，提升网络性能。</p><p>降低计算资源消耗的一个方法是使用<a href="https://baike.baidu.com/item/稀疏连接/22764619?fr=aladdin" target="_blank" rel="noopener">稀疏连接</a>结构，但不均匀的稀疏数值运算在当前适合密集运算的硬件条件下运行十分低效。作者希望将稀疏连接结构运用于卷积层，并以此解决稀疏连接在密集运算条件下效率低下的问题。于是Inception便应运而生。</p><h2 id="架构细节"><a href="#架构细节" class="headerlink" title="架构细节"></a>架构细节</h2><p> <img src="https://s2.ax1x.com/2019/10/04/uDtGDI.png" alt="uDtGDI.png"></p><p>作者希望“找到最优的局部结构，并在空间上重复它”，如上的Inception模块便是作者找到的最优局部结构。该结构有<strong>四个通道，同时使用了1×1、3×3、5×5的卷积核</strong>。作者表示“卷积核的大小并没有什么特殊含义，其便利性大于必要性”，在padding=0，1，2的时候特征图大小相同，<strong>方便对齐</strong>。</p><p>随着网路层数的加深，其特征图的抽象程度变高，空间集中程度下降。这意味着5×5卷积核占比应逐渐增加。然而在具有大量滤波器的卷积层，5×5卷积核运算量太大。这催生了对Inception的第二个改进：在计算量要求较多的地方<strong>使用1×1卷积核进行降维</strong>。于是便诞生了完整版的Inception V1模块：</p><p><img src="https://s2.ax1x.com/2019/10/04/uDB4kn.png" alt="uDB4kn.png"></p><h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p>GoogLeNet是一个大量使用了Inception模块堆叠的一个神经网络，其结构如下（图太大了，这里就不放完整图片了）：</p><p><img src="https://s2.ax1x.com/2019/10/04/uDrWin.png" alt="#uDrWin.png"></p><p>值得一提的是，考虑到深层网络的梯度消失问题（当时还没出现批归一化和残差结构），GoogLeNet使用了在网络的中间隐藏层使用了<strong>辅助分类器</strong>（auxiliary classifiers），其训练时给出的分类结果的损失的以0.3的权重加到总损失上，以在一定程度上解决梯度消失问题。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><ul><li><p>很多文章中都有提到，Inception结构使用不同大小的卷积核能够适应不同尺度的特征。虽然并没有在原论文中看见相关阐述，但我觉得有点道理。论文中提到Inception在目标检测任务中有更出色的效果，这很可能与其能适应不同尺度特征有关。</p><p><img src="https://s2.ax1x.com/2019/10/04/uDo1Ds.png" alt="如图，图中三只狗狗所占图片区域大小不同"></p></li><li><p>作者并没有在原论文中提到Inception结构起作用的原因，但我认为Inception结构和ResNet的残差结构有异曲同工之妙（虽然ResNet的诞生在GoogLeNet之后）。残差结构能让神经网络自己通过调整参数来选择是否趋近于恒等映射，而Inception能让神经网络自己选择卷积核大小（3×3、5×5 convolutions），或是将这层作为全连接（1×1 convolutions，Inception结构最左边的那个1×1卷积核作用相当于全连接），抑或是池化（3×3 Max Pooling）。</p></li></ul><h1 id="Inception-V2-amp-V3"><a href="#Inception-V2-amp-V3" class="headerlink" title="Inception V2&amp;V3"></a>Inception V2&amp;V3</h1><p>论文链接：<a href="https://arxiv.org/abs/1512.00567" target="_blank" rel="noopener">Rethinking the Inception Architecture for Computer Vision</a></p><h2 id="通用设计准则"><a href="#通用设计准则" class="headerlink" title="通用设计准则"></a>通用设计准则</h2><p>该论文提出了4个神经网络的设计准则，并根据这些准则改进Inception。以下列出关键的两条：</p><ul><li>避免一次性大幅压缩（大尺寸卷积、池化等）特征图的尺寸，否则会造成<strong>表征性瓶颈</strong>，特征图中的信息会大量损失。</li><li>高维度的特征更容易局部处理，解耦更多的特征，加速网络训练。</li></ul><h2 id="分解（Factorization）大尺寸卷积"><a href="#分解（Factorization）大尺寸卷积" class="headerlink" title="分解（Factorization）大尺寸卷积"></a>分解（Factorization）大尺寸卷积</h2><p>作者提出，大尺寸卷积的计算量和它的尺寸是不成比例的。于是<strong>将原来的5×5卷积改成了两个3×3卷积</strong>：</p><p><img src="https://s2.ax1x.com/2019/10/07/uRSdMD.png" alt="uRSdMD.png"></p><p>然后减少了28%的计算量。</p><h2 id="分解为不对称的卷积"><a href="#分解为不对称的卷积" class="headerlink" title="分解为不对称的卷积"></a>分解为不对称的卷积</h2><p>然后作者想把3×3分解成更小的卷积……尝试了分解成两个2×2，节省了11%的计算量。然后尝试了分解成1×3和3×1，节省了33%计算量。于是便多出了如下两类不对称分解的Inception模块：</p><p><img src="https://s2.ax1x.com/2019/10/07/uRCVIA.png" alt="uRCVIA.png"></p><p>左图模块特性：</p><ul><li>在网络的浅层表现不佳，但在网络的中层有较好的效果。</li><li>由于比原版模块增加了一层非线性层，提高了模型的表达能力。</li></ul><p>右图模块特性：</p><ul><li>能够维持特征的高维度，符合上述通用设计准则的第二条。</li></ul><h2 id="减少特征图尺寸"><a href="#减少特征图尺寸" class="headerlink" title="减少特征图尺寸"></a>减少特征图尺寸</h2><p>当网络需要将一个尺寸为 2d×2d、维度为 k 的特征图转换为一个尺寸为 d×d、维度为 2k 的特征图时，问题就来了：如果先减小尺寸，那么将会损失大量信息，造成准则第一条中的表征性瓶颈；如果先增大维度，那么计算量将翻3倍。如何高效地减小特征图尺寸呢？作者提出了以下结构：</p><p><img src="https://s2.ax1x.com/2019/10/07/uRkv3n.png" alt="uRkv3n.png"></p><p>该结构在增加特征维度、减少特征图尺寸的同时避免了表征性瓶颈和计算量过大的问题。</p><h2 id="Inception-v2"><a href="#Inception-v2" class="headerlink" title="Inception-v2"></a>Inception-v2</h2><p><img src="https://s2.ax1x.com/2019/10/07/uRVxgA.png" alt="uRVxgA.png"></p><p>其中使用了三种Inception模块（图中红框处），包括3个普通分解模块和5个不对称分解堆叠模块以及2个不对称分解扩展模块。值得一提的是原网络中的7×7卷积被分解成了3个3×3卷积。</p><h2 id="Inception-v3"><a href="#Inception-v3" class="headerlink" title="Inception-v3"></a>Inception-v3</h2><p>在论文的后续中，作者对Inception v2进行了如下改进：</p><ul><li>使用RMSProp优化器</li><li>辅助分类器使用了BatchNorm</li><li>标签平滑（正则化）</li></ul><h1 id="Inception-V4-amp-Inception-Resnet"><a href="#Inception-V4-amp-Inception-Resnet" class="headerlink" title="Inception V4 &amp; Inception-Resnet"></a>Inception V4 &amp; Inception-Resnet</h1><p>论文地址：<a href="https://arxiv.org/pdf/1602.07261.pdf" target="_blank" rel="noopener">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>随着 ResNet 网络的出现及其在主流数据集上的良好表现，作者想将残差结构引入到 Inception 网络中，看看网络是否会有更好的表现；同时注意到Inception-v3的部分结构有不必要的复杂性，于是尝试在不引入残差结构的情况下改进原本的Inception结构，并将改进后的Inception结构命名为Inception-v4。</p><p>我感觉这篇论文的知识量不大，整篇论文一半都是图，看看了解下就行。</p><h2 id="Inception-v4"><a href="#Inception-v4" class="headerlink" title="Inception-v4"></a>Inception-v4</h2><p><img src="https://s2.ax1x.com/2019/10/08/uhaJjs.png" alt="uhaJjs.png"></p><p>图中是v4使用的三个Inception模块。分别命名为Inception-A、Inception-B、Inception-C。除了所有的池化层都使用了<strong>Avg Pooling</strong>以外，没有什么特别的变动。另外网络整体结构也发生了一些改变，这里直接用网图了：</p><p><img src="https://img-blog.csdnimg.cn/2018102913400312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6YzE1ODA2,size_27,color_FFFFFF,t_70" alt></p><h2 id="Inception-Resnet"><a href="#Inception-Resnet" class="headerlink" title="Inception-Resnet"></a>Inception-Resnet</h2><p><a href="https://imgchr.com/i/uhwGmn" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/10/08/uhwGmn.md.png" alt="uhwGmn.md.png"></a></p><p><img src="https://img-blog.csdnimg.cn/20181029135504384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3p6YzE1ODA2,size_27,color_FFFFFF,t_70" alt></p><p>上图是 Inception-Resnet-v1 的模块和结构， Inception-Resnet-v2只是在v1的基础上使用了Inception-v4的stem结构。</p><h1 id="相关面试题"><a href="#相关面试题" class="headerlink" title="相关面试题"></a>相关面试题</h1><p><strong>Q：GoogLeNet中为什么采用小的卷积核？</strong></p><p>A：多个小卷积核的叠加能起到和大卷积核一样的效果，并且运算量更小。</p><p><strong>Q：3×3 卷积核 与 5×5 卷积核相比的优点</strong></p><p>A：速度更快，性能更好</p><p><strong>Q：InceptionV1~V4系列介绍，以及每一版的改进，优缺点介绍</strong></p><p>A：v1提出Inception模块，提供四个通道，方便模型自己在全连接、小卷积、大卷积、池化之间做出选择；v2提出分解卷积和不对称分解卷积，降低了运算量，以及提出了解决表征性瓶颈的结构；v3在v2的基础上修改了优化器、添加了辅助分类器的BN和标签平滑；v4提出了几种新的Inception模块，取得了更好的性能。</p><p><strong>Q：1×1 卷积有什么作用？</strong></p><p>A：跨通道信息整合（参考Inception模块和Res模块）；降维减少计算量（参考Inception模块）</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="http://baijiahao.baidu.com/s?id=1601882944953788623&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">一文概览Inception家族的「奋斗史」</a></p><p>[2]<a href="https://blog.csdn.net/zzc15806/article/details/83504130" target="_blank" rel="noopener">【深度学习】GoogLeNet系列解读 —— Inception v4</a></p><p>[3]<a href="https://blog.csdn.net/weixin_39953502/article/details/80966046" target="_blank" rel="noopener">inception-v1,v2,v3,v4——论文笔记</a></p><p>以及文中所述的论文链接。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Inception-V1&quot;&gt;&lt;a href=&quot;#Inception-V1&quot; class=&quot;headerlink&quot; title=&quot;Inception V1&quot;&gt;&lt;/a&gt;Inception V1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.or
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="Inception" scheme="http://a-kali.github.io/tags/Inception/"/>
    
  </entry>
  
  <entry>
    <title>线性回归与逻辑回归</title>
    <link href="http://a-kali.github.io/2019/09/03/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://a-kali.github.io/2019/09/03/线性回归与逻辑回归/</id>
    <published>2019-09-03T13:21:16.000Z</published>
    <updated>2019-11-26T15:12:40.205Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><p>​    <strong>线性回归</strong>通常是解决连续数值预测问题，利用数理统计的回归分析，来确定变量之间的相互依赖关系，线性方程通常表示如下:</p><script type="math/tex; mode=display">h_\theta(x)=\theta_0 +\theta_1x_1+\theta_2x_2+……+\theta_nx_n=θ^Tx</script><p>线性回归梯度下降方程：</p><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha \frac{\partial J(\theta_j)}{\partial \theta_j}</script><p>其中 $J(\theta_j)$ 为损失函数，$\alpha$为学习率。</p><h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1><p>​    <strong>逻辑（Logistic，又称 Sigmoid）回归</strong>常用于解决二分类问题，用于估算某种事物的可能性。Sigmoid 函数公式如下：</p><script type="math/tex; mode=display">g(z)=\frac{1}{1+e^{-z}}</script><p>该函数的值域为 (0, 1)，其值的意义为输入特征被分到 1 类的概率。逻辑回归的本质是在线性回归之后加了一层函数映射。将线性回归方程带入到逻辑回归方程中，得到逻辑回归表达式：</p><script type="math/tex; mode=display">h_\theta(x) = g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;线性回归&quot;&gt;&lt;a href=&quot;#线性回归&quot; class=&quot;headerlink&quot; title=&quot;线性回归&quot;&gt;&lt;/a&gt;线性回归&lt;/h1&gt;&lt;p&gt;​    &lt;strong&gt;线性回归&lt;/strong&gt;通常是解决连续数值预测问题，利用数理统计的回归分析，来确定变量之间的相互
      
    
    </summary>
    
      <category term="机器学习" scheme="http://a-kali.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://a-kali.github.io/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
      <category term="逻辑回归" scheme="http://a-kali.github.io/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
</feed>
