<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2021-02-09T00:40:18.999Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>经济心理学</title>
    <link href="http://a-kali.github.io/2021/02/09/%E7%BB%8F%E6%B5%8E%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    <id>http://a-kali.github.io/2021/02/09/经济心理学/</id>
    <published>2021-02-09T00:38:44.000Z</published>
    <updated>2021-02-09T00:40:18.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-随机性的判断误区"><a href="#1-随机性的判断误区" class="headerlink" title="1    随机性的判断误区"></a>1    随机性的判断误区</h1><p>我们生活在一个充满随机性和偶然性的世界之中。随机性的表现不仅包括抛硬币的正反，还包括一些股票的涨跌等。随机性是我们生活中不可分割的一部分，而我们对随机性的理解却往往出现偏差。<strong>对于随机性的认识往往存在这样四种误区：</strong></p><ol><li><strong>过度解释，无中生有；</strong></li><li><strong>错误模拟，过度求变；</strong></li><li><strong>赌徒谬误，高估反转；</strong></li><li><strong>手热错觉，高估惯性。</strong></li></ol><h2 id="（一）过度解释"><a href="#（一）过度解释" class="headerlink" title="（一）过度解释"></a>（一）过度解释</h2><p><strong>从没有结构的地方找结构：人们总把模棱两可的序列事件知觉为更有结构的事件。</strong>比如将一朵云的形状描述成某种小动物，根据星星的排列命名星座，从彩票走势图中找规律等。证券市场上也如此。</p><p><strong>虚假相关：对没有关联的两件事物进行因果关联</strong>。比如“雨神萧敬腾”事件和“丁蟹效应”。</p><p><strong>解释巧合</strong>：人们常常对一些巧合进行解释，而巧合往往不需要偶然性以外的特别的解释。</p><p><strong>公平世界信念</strong>：人们认为这个世界是公正的，人们得其所应得，所得即应得。但其实很多所得到的和损失的都是随机产生的，并不具有需要解释的原因。</p><h2 id="（二）错误模拟"><a href="#（二）错误模拟" class="headerlink" title="（二）错误模拟"></a>（二）错误模拟</h2><p>错误地理解了随机性，认为随机就是每次都不相同。比如对于音乐播放器中的随机播放，当播放器连续播放了同一首歌或者同一歌手的歌时，用户会觉得这不是随机播放；而当程序修改为随机播放时不会连续播放同一首歌或同一歌手的歌时，用户反而认为这是随机播放。</p><h2 id="（三）赌徒谬误"><a href="#（三）赌徒谬误" class="headerlink" title="（三）赌徒谬误"></a>（三）赌徒谬误</h2><p>抛硬币每次都是独立事件，第六次的结果并不会因为前面五次都是正面而出现反面。</p><h2 id="（四）手热错觉"><a href="#（四）手热错觉" class="headerlink" title="（四）手热错觉"></a>（四）手热错觉</h2><p>来源于篮球场上，人们通常会将球交给状态好、手热的球员来投球，认为其此时命中率高。在球场上或许有道理，但在市场上认为股市连续下跌多日后会继续下跌则并没有依据。</p><h1 id="2-控制幻觉"><a href="#2-控制幻觉" class="headerlink" title="2    控制幻觉"></a>2    控制幻觉</h1><p>首先假设一个情景：某一天你被歹徒绑架，歹徒告诉你可以通过玩一个俄罗斯轮盘来决定你的生死。你有两种选择：选择A是一把容量为6的放了1颗子弹的左轮手枪，选择B是在5把空手枪和1把满子弹手枪里选一把。虽然两种选择的死亡概率相同，但大多数人都会选择B，因为人们会对这个选择产生控制幻觉，即认为A是完全随机，而B可以通过自己选择手枪来控制概率。</p><p><strong>控制幻觉的定义</strong>：在完全不可控或部分不可控的情境下，个体由于不合理地高估自己对环境或事件结果的控制力而产生的一种判断偏差。</p><ul><li>彩票实验：自己填写的彩票在转让时往往会要价更高，而由他人帮忙购买的彩票在转让时要价低于前者；</li><li>扑克实验：在随机扑克游戏中，面对表现自信的对手时，受试者往往感到可控力低，押注往往更少。</li><li>掷骰子时，人们想要更大的数字时往往投掷力度会更大；</li><li>自选彩票的销量远高于随机彩票。</li></ul><p>在金融领域，交易者们认为自己的行为能够增加投资收益。于是在国内市场上经常会出现频繁交易的现象，而事实上交易次数整体是与收益成反比的。</p><p>以下三种因素能够影响到控制幻觉：</p><ol><li>个体陷入的程度；</li><li>是否具有选择权；</li><li>事物对个体的特殊意义。</li></ol><h1 id="3-启发式偏差"><a href="#3-启发式偏差" class="headerlink" title="3    启发式偏差"></a>3    启发式偏差</h1><p>人类在做判断时，往往会利用直觉，采取走捷径的方式，这种方式称之为<strong>启发式</strong>。启发式分为代表性启发式和易得性启发式。</p><h2 id="（一）代表性启发式"><a href="#（一）代表性启发式" class="headerlink" title="（一）代表性启发式"></a>（一）代表性启发式</h2><p><strong>代表性启发式</strong>：人们通常会根据A在多大程度上能代表B，或者A在多大程度上与B相似来判断事件发生的可能性。其原因如下：</p><ol><li><strong>对先验概率不敏感</strong>：比如说人们不知道人群中工程师的比例是多少，但当遇到一个符合他们对工程师刻板印象的描述时，却很可能将其归类为对工程师的描述，无视了工程师在人群中的比例（即先验概率）。</li><li><strong>对样本大小不敏感</strong>：样本越大才越具有代表性，小样本通常具有偶然性。而人们往往会在一个小样本上做出对于规律的判断，认为小样本能反映整体现象。</li><li><strong>对随机性的误解</strong>：同第一章随机性的四种误区。</li><li><strong>对向均值回归的误解</strong>：一些连续性的数据往往会有向平均值回归的趋势，而人们却忽略了这种趋势。比如某位选手在一段时间内表现较佳，人们会认为其将一直表现优异下去，但实际上这段时间只是该选手高于自身平均的一段时间。</li></ol><p>在投资领域，人们通常将好公司与好股票对等，而事实并非如此，这就是代表性启发式的例子。</p><h2 id="（二）易得性启发式"><a href="#（二）易得性启发式" class="headerlink" title="（二）易得性启发式"></a>（二）易得性启发式</h2><p><strong>易得性启发式</strong>：人们倾向于根据一个客体或事件在知觉或记忆中的易得性程度来评估其相对概率，认为容易知觉到的或回想起的被判定为更常出现。其与以下两点相关联：</p><ol><li><strong>信息的生动性</strong>：亲生经历的、越容易回想起的例子越生动，越容易被判断为容易发生的事件；</li><li><strong>时间接近性</strong>：越是最近发生的事，人们越容易认为它会再发生一次。</li></ol><p>在投资领域，人们偏爱自己熟悉了解的事物。比如人们更愿意去参与自己更熟悉的游戏（哪怕获胜概率更低）、更愿意买自己公司的股票。（从这种角度来看，茅台下跌的那天是不是就是它失去热度的那天？）</p><h1 id="4-锚定效应"><a href="#4-锚定效应" class="headerlink" title="4    锚定效应"></a>4    锚定效应</h1><p><strong>锚定效应（Anchoring Effect）</strong>：当人们对某个事件做定量估测时，会将某些特定数值作为起始值，进行上下不充分的调整。锚定效应可以根据锚是来自于内还是来自于外可以分为外部锚和内部锚。</p><h2 id="（一）外部锚"><a href="#（一）外部锚" class="headerlink" title="（一）外部锚"></a>（一）外部锚</h2><p>外部锚包括传统锚定效应和基本锚定效应。</p><p><strong>传统锚定效应</strong>：比较判断问题+绝对判断问题。</p><p><strong>基本锚定效应</strong>：单纯数字也会影响绝对的判断，使最终的估计值趋向无关信息的值。人们对数值最终的估计会偏向于那个最近出现的无关数值。</p><h2 id="（二）内部锚"><a href="#（二）内部锚" class="headerlink" title="（二）内部锚"></a>（二）内部锚</h2><p>人们会根据自身的一些经验来推算估计最终的数值。</p><h2 id="（三）锚定效应的应用"><a href="#（三）锚定效应的应用" class="headerlink" title="（三）锚定效应的应用"></a>（三）锚定效应的应用</h2><p>某些手表公司可能会生产上百万美元的手表，但这种手表通常不是用来售卖的，而是用来给顾客一个锚点，让顾客能接受该公司手表价格高于其他公司。</p><p>超市里相同类别的两种商品，贵的商品会衬托出便宜的商品的实惠，增加便宜商品的销量。但事实上可能两种商品都是高于其合理价格的。</p><h1 id="5-过度自信与后悔"><a href="#5-过度自信与后悔" class="headerlink" title="5    过度自信与后悔"></a>5    过度自信与后悔</h1><p><strong>过度自信与后悔</strong>：人们在做出判断时往往会出现一种高估自己能力、判断准确性以及信息准确性的现象，即过度自信（over-confidence）。</p><p><strong>过度自信</strong>分为两种类型：过高估计与过高定位。</p><ul><li><strong>过高估计</strong>：个体高估自身实际能力、表现、对事件控制水平以及成功几率的一种认知差。大多数公司会高估自己公司的存活率和成功率，而低估其它公司的成功率和存活率。</li><li><strong>过高定位</strong>：个体认为自身能力要高于其他人的一种倾向，又称为高于平均效应。95%的美国大学生认为自己的社交能力高于平均。</li></ul><p>通常男性比女性更容易出现过度自信。在投资领域，投资者往往会高估私人信息产生信号的准确性和自身对证券价值的估价能力，导致过度交易和低估风险。而男性投资者的年交易量通常高于女性，而投资收益低于女性。单身男性的投资组合风险最大，其后依次是已婚男性、已婚女性和单身女性。</p><p><strong>后悔</strong>：后悔是一种基于认知的消极情感，主要发生在个体意识到或者想象出如果先前采取其他的行为，将产生更好的结果时。后悔会导致两种结果：</p><ul><li><strong>后悔规避</strong>：人们当做出错误决策时，会对自己的行为感到痛苦，并对这种痛苦采取回避的方式。</li><li><strong>不作为惯性</strong>：当人失去一个行为机会时，会导致对后来类似的机会的不作为。</li><li><strong>处置效应</strong>：是指避免后悔、寻求自豪的心理导致投资者过早套现盈利股票，而过久持有亏损股票。</li></ul><h1 id="6-前景理论"><a href="#6-前景理论" class="headerlink" title="6    前景理论"></a>6    前景理论</h1><p><img src="https://i.loli.net/2021/02/07/v3YmDLFnMbKhiel.png" alt="image.png"></p><p><strong>前景理论</strong>的核心是<strong>价值函数</strong>（如上图），值得注意的是改图并不是中心对称的。前景理论可以概括为五个方面：</p><ol><li><strong>确定效应</strong>：大多数人处于收益状态时往往小心翼翼、厌恶风险，喜欢见好就收，害怕失去已有的李润。</li><li><strong>反射效应</strong>：处于损失预期时，大多数人会变得甘冒风险。反射效应与确定效应结合起来就是处置效应。一个典型的例子是交通事故中的肇事司机往往会选择逃逸而不是及时抢救止损，而股市中人们处于亏损状态时也更倾向于长期持有，不愿意及时止损来接受损失的事实。</li><li><strong>损失厌恶</strong>： 对于相同金额的收益和损失，损失带给人的痛是大于收益带来的快乐。一个典型的例子是<strong>沉没成本效应</strong>。</li><li><strong>迷恋小概率事件</strong>：人们对小概率事件往往非常重视，而对大概率事件没有给到应有的权重。对于小概率的盈利，多数人是风险喜好者；对于小概率的损失，多数人是风险厌恶者。</li><li><strong>参照依赖</strong>：人们对得失的判断往往根据参照点决定。“在一群赚6万的人中赚7万元”比“在一群赚12万元的人中赚10万元”更快乐。</li></ol><h1 id="7-禀赋效应"><a href="#7-禀赋效应" class="headerlink" title="7    禀赋效应"></a>7    禀赋效应</h1><p><strong>禀赋效应</strong>：当个人一旦拥有某样物品，那么他对该物品价值的评价比未拥有前大大提高。与得到某物品所愿意支付的金钱相比，个体出让该物品所要求的道德金钱通常更多。</p><p>禀赋效应最高的物品为公共物品或非市场化的商品，比如空气、水，其次是私人物品；而禀赋效应最低的物品是货币和代币券。 </p><p>禀赋效应有三种解释：</p><ol><li>第一种解释认为禀赋效应源于损失厌恶，人们将已拥有的物品转让出去视为损失； </li><li>第二种解释认为禀赋效应源于<strong>参照点转换理论</strong>， 损失规避具有参照依赖性，损失和收益是相对于某一参照点而言的。</li><li>第三种解释认为禀赋效应源于<strong>程数理论</strong>。</li></ol><p><strong>伪禀赋效应</strong>：人们对可能拥有但并未实质拥有的物品也会产生禀赋效应。（如果我未曾见过光明，我本可以忍受黑暗.jpg）</p><h1 id="8-偏好反转与框架效应"><a href="#8-偏好反转与框架效应" class="headerlink" title="8    偏好反转与框架效应"></a>8    偏好反转与框架效应</h1><p><strong>偏好反转</strong>：决策者在两个相同评价条件但不同的引导模式下，对方案的偏好有所差异，甚至出现逆转的现象。在期望值大体相等的一对博弈中，人们往往选择概率高而损益值小的博弈（安全博弈），却对概率低而损益值大的博弈（风险博弈）定高价。</p><p>在两个期望收益相当的赌局中，人们往往会选择那个风险小收益低的，但在转售这两场赌局的门票时，人们往往会对风险大收益高的赌局定高价。</p><p><strong>框架效应</strong>：对相同客观信息的不同表述能够显著地改变决策模型，即使框架差异程度不应对理性决策产生影响。框架效应分为三种：</p><ol><li><strong>特征框架效应</strong>：以积极的语言对事物的关键属性进行描述，比起以消极的语言描述，人们对事物或事件会更满意。瘦肉占80%比肥肉占20%的描述更能让消费者满意。</li><li><strong>目标框架效应</strong>：指当一个信息强调为不做出某种行为导致的消极后果时，比强调为做出这种行为的积极后果更有说服力（损失厌恶+后悔）。</li><li><strong>风险决策框架效应</strong>：当信息以积极的形式呈现时，个体表现出风险规避的倾向；当信息以消极的形式呈现，个体表现为风险偏好的倾向（确定效应+反射效应）。</li></ol><h1 id="9-心理账户"><a href="#9-心理账户" class="headerlink" title="9    心理账户"></a>9    心理账户</h1><p><strong>心理账户（mental accounting, MA）</strong>是个人或家庭用来管理、评估、跟踪金融活动的一种操作定式。</p><p>如果今天晚上你打算去听一场音乐会，票价是200元，在你马上要出发的时候，你发现你把最近买的价值200元的电话卡弄丢了。你是否还会去听这场音乐会？实验表明，大部分的回答者仍旧去听。可是如果情况变一下，假设你昨天花了200元钱买了一张今天晚上音乐会门票。在你马上要出发的时候，突然发现你把门票弄丢了。如果你想要听音乐会，就必须再花200元钱买张门票，你是否还会去听？结果却是，大部分人回答说不去了。</p><p>可仔细想一想，上面这两个回答其实是自相矛盾的。不管丢的是电话卡还是音乐会门票，总之是丢失了价值200元的东西，从损失的金钱上看，并没有区别。之所以出现上面两种不同的结果，其原因就是大多数人的心理账户的问题。</p><p>人们在脑海中，把电话卡和音乐会门票归到了不同的账户中，所以丢失了电话卡不会影响音乐会所在的账户的预算和支出，大部分人仍旧选择去听音乐会。但是丢了的音乐会门票和后来需要再买的门票都被归入了同一个账户，所以看上去就好像要花400元听一场音乐会了。人们当然觉得这样不划算了。</p><p><img src="https://i.loli.net/2021/02/08/erSMxkhqwPH2fUX.png" alt="image.png"></p><p>MA的原因有四种类型：</p><ol><li><strong>财富来源不同</strong>：人们对意外之财的消费倾向较高，常用于享乐；而对辛苦所得的钱消费倾向低，常用于日常开支。</li><li><strong>消费类型不同</strong>：人们在日常消费中很可能不会买很贵的衣服，但在节日送礼时可能会消费这件衣服，因为日常消费和节假日消费不在同一个心理账户中。</li><li><strong>存储方式不同</strong>：存在银行的钱、在证券市场里的钱、以及手上的现金有不同的心理账户。</li><li><strong>时间划分不同</strong>：出租车司机每天都要交份子钱，所以通常每天都会工作到挣够预期的钱才下班，其心理账户是按天计算的；而如果心理账户按月计算的话，司机可以选择在身体不适时选择休息，而在平时身体健康时多拉点活。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-随机性的判断误区&quot;&gt;&lt;a href=&quot;#1-随机性的判断误区&quot; class=&quot;headerlink&quot; title=&quot;1    随机性的判断误区&quot;&gt;&lt;/a&gt;1    随机性的判断误区&lt;/h1&gt;&lt;p&gt;我们生活在一个充满随机性和偶然性的世界之中。随机性的表现不仅包括
      
    
    </summary>
    
      <category term="经济与金融" scheme="http://a-kali.github.io/categories/%E7%BB%8F%E6%B5%8E%E4%B8%8E%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="经济" scheme="http://a-kali.github.io/tags/%E7%BB%8F%E6%B5%8E/"/>
    
      <category term="心理学" scheme="http://a-kali.github.io/tags/%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Self-Attention and Transformer</title>
    <link href="http://a-kali.github.io/2021/02/09/Self-Attention-and-Transformer/"/>
    <id>http://a-kali.github.io/2021/02/09/Self-Attention-and-Transformer/</id>
    <published>2021-02-09T00:37:24.000Z</published>
    <updated>2021-02-09T00:38:24.041Z</updated>
    
    <content type="html"><![CDATA[<p>关于attention的详细介绍在<a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">Attention机制详解（一）——Seq2Seq中的Attention</a>有详细介绍，在此不做多赘述。</p><p>添加了Attention的LSTM依然存在一些问题：</p><ul><li>梯度消失，无法捕捉到长程的依赖关系；</li><li>运算量大，且无法并行。</li></ul><p>以上问题在时序模型中普遍存在，于是一种能够拟合时序数据的非时序模型应运而生，那就是Transformer，而Transformer中的关键结构就是Self-attention。</p><h1 id="Transformer的结构"><a href="#Transformer的结构" class="headerlink" title="Transformer的结构"></a>Transformer的结构</h1><p><img src="https://i.loli.net/2021/01/21/xubkHCYoVmiX7Dv.png" alt="image.png"></p><p>上图是一个Transformer的结构，其中Multi-Head Attention就是多个Self-Attention的结合。</p><h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><h2 id="Self-Attention的作用"><a href="#Self-Attention的作用" class="headerlink" title="Self-Attention的作用"></a>Self-Attention的作用</h2><p>Transformer模型的左半部分称为Encoder，由多个Encoder模块组成，单个模块的结构可以看作下图。其输入为字符串中的多个字符经过Embedding后的<strong>相互无关的多个词向量</strong>，经过Self-Attention后得到一个新<strong>相互关联的向量</strong>，这就是Self-Attention的作用。</p><p><img src="https://i.loli.net/2021/01/21/HaduK5krWozRsMO.png" alt="image.png"></p><h2 id="Self-Attention的原理"><a href="#Self-Attention的原理" class="headerlink" title="Self-Attention的原理"></a>Self-Attention的原理</h2><p>Self-Attention模块维护了三个可训练的矩阵，分别为$W_Q, W_K, W_V$，使用这三个矩阵与词向量相乘得到$q, k, v$三个向量。</p><p><img src="https://i.loli.net/2021/01/21/wX3fTFsq6NDvd8H.png" alt="image.png"></p><p>使$q_i$和$k_j$相乘得到attention score，该值表示翻译第i个词时，第j个词与其的相关度。显然，当前单词与其自身的attention score一般最大。然后除以8对该词进行缩放（这个除数由k向量的长度开根号得到，原论文中向量长度为64），使用softmax进行归一化。</p><p><img src="https://i.loli.net/2021/01/21/X5jcazOdS3irEgY.png" alt="image.png"></p><p>最后使用softmax得到的结果对多个v向量进行加权求和，得到当前词的结果z，并将得到的所有z输入到下一个Encoder模块。</p><p><img src="https://i.loli.net/2021/01/21/QvorCjHTumKJOy4.png" alt="image.png"></p><p>值得一提的是在Decoder中无法并行预测所有词，需要循环地一个个预测单词，因为要用上一个位置的输入当作attention的query。</p><h1 id="什么是Multi-head？"><a href="#什么是Multi-head？" class="headerlink" title="什么是Multi-head？"></a>什么是Multi-head？</h1><p>Multi-head就是我们可以有不同的Q,K,V表示的Self Attention，最后再将其结果结合起来，如下图所示：</p><p><img src="https://i.loli.net/2021/01/21/iL3yU8FOH9WmCMB.png" alt="image.png"></p><h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>#copy</p><p>由于Transformer中没有循环以及卷积结构，为了使模型能够利用序列的顺序，作者们需要插入一些关于tokens在序列中相对或绝对位置的信息。因此，作者们提出了<strong>位置编码(Positional Encoding)</strong>的概念。Positional Encoding和token embedding相加，作为encoder和decoder栈的底部输入。Positional Encoding和embedding具有同样的维度，因此这两者可以直接相加。</p><p>在位置编码的论文中，作者使用了不同频率的正弦函数和余弦函数作为位置编码用来表示每个词在句子中的位置，具体可参考<a href="https://zhuanlan.zhihu.com/p/98641990" target="_blank" rel="noopener">博客</a>。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">Attention机制详解（二）——Self-Attention与Transformer</a><br>[2]<a href="https://blog.csdn.net/weixin_40871455/article/details/86084560" target="_blank" rel="noopener">transformer 模型（self-attention自注意力）</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/98641990" target="_blank" rel="noopener">对Transformer中的Positional Encoding一点解释和理解</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">【NLP】Transformer模型原理详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于attention的详细介绍在&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47063917&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention机制详解（一）——Seq2Seq中的Attention&lt;/a&gt;有
      
    
    </summary>
    
    
      <category term="self-attention" scheme="http://a-kali.github.io/tags/self-attention/"/>
    
      <category term="transformer" scheme="http://a-kali.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>EfficientNet简述</title>
    <link href="http://a-kali.github.io/2021/01/18/EfficientNet%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2021/01/18/EfficientNet简述/</id>
    <published>2021-01-18T01:01:09.000Z</published>
    <updated>2021-01-18T01:41:23.718Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p><p>EfficientNet作者系统地研究了模型缩放，并发现对网络<strong>深度、宽度和分辨率</strong>进行平衡可以带来更好的性能，而前人的文章多是放大其中的一个以达到更高的准确率。EfficientNet仅使用了很小的参数量就超越了当时的SOTA模型。</p><p><img src="https://i.loli.net/2021/01/18/zeOAPEBmIoHFCyi.png" alt="image.png"></p><p>作者发现只对模型的深度、宽度和分辨率其中一个维度进行扩张能得到性能提升，但是会有较大的局限性。作者认为<strong>各个维度之间的扩张不应该是相互独立的</strong>，比如说，对于更大分辨率的图像，应该使用更深、更宽的网络，这就意味着需要平衡各个扩张维度，而不是在单一维度张扩张。</p><p>对此作者提出了<strong>复合缩放法(compound scaling method)</strong>，方程式如下，其中约束(s.t.)限制了模型的复杂度。在约束式中宽度和分辨率都有一个平方项，这是因为如果增加宽度或分辨率两倍，其计算量是增加四倍，但是增加深度两倍，其计算量只会增加两倍。</p><p><img src="https://i.loli.net/2021/01/18/WZAmwraFBPnHfiq.png" alt="image.png"></p><p>求解方式：</p><ol><li>固定公式中的φ=1，然后通过网格搜索（grid search）得出最优的α、β、γ，得出最基本的模型EfficientNet-B0.</li><li>固定α、β、γ的值，使用不同的φ，得到EfficientNet-B1, …, EfficientNet-B7</li></ol><p>φ的大小对应着消耗资源的大小，相当于：</p><ol><li>当φ=1时，得出了一个最小的最优基础模型；</li><li>增大φ时，相当于对基模型三个维度同时扩展，模型变大，性能也会提升，资源消耗也变大。</li></ol><p>值得一提的是EfficientNet中使用了<strong>移动翻转瓶颈卷积(mobile inverted bottleneck convolution，MBConv)模块</strong>，该模块引入了深度分离卷积和SENet的思想。</p><p>参考文献：</p><p>[1]<a href="https://zhuanlan.zhihu.com/p/96773680" target="_blank" rel="noopener">令人拍案叫绝的EfficientNet和EfficientDet</a><br>[2]<a href="https://www.jianshu.com/p/2ac06d97a830" target="_blank" rel="noopener">速度与精度的结合 - EfficientNet 详解</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/111115509" target="_blank" rel="noopener">EfficientNet-B0解读</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/258386372" target="_blank" rel="noopener">EfficentNet详解之MBConvBlock</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EfficientNet: Rethinking Model Scaling for Convolutional N
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
  </entry>
  
  <entry>
    <title>RANSAC算法概述</title>
    <link href="http://a-kali.github.io/2020/12/30/RANSAC%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/30/RANSAC算法概述/</id>
    <published>2020-12-30T09:14:17.000Z</published>
    <updated>2020-12-30T09:15:36.758Z</updated>
    
    <content type="html"><![CDATA[<p><strong>RANSAC（Random Sample Consensus，随机取样一致）</strong>是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。</p><p>一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图）。最小二乘法会使用包括异常样本的所有的数据样本进行计算，得到拟合参数，但其会受到异常点的影响。而RANSAC就是为了解决这个问题。</p><p><img src="https://i.loli.net/2020/12/29/r9S51AEdvij6N3O.png" alt="image.png"></p><p>RANSAC算法通过反复选择数据中的以组随机子集进行验证，概述如下：</p><ol><li>随机选择一组点作为<strong>局内点</strong>，用这部分局内点拟合一个模型；</li><li>将所有满足该模型的点加入一个新的局内点集合；</li><li>使用新的局内点集合去拟合一个新的模型，并测试模型的准确度（即满足模型的点                                                                                          占所有点的比例）；</li><li>重复步骤2、3，最终得到一个准确度达到预期的模型。</li></ol><p>RANSAC常用于删除特征匹配过程中的异常点。</p><p><img src="https://i.loli.net/2020/12/29/eVio9WpMPXntUIA.png" alt="image.png"></p><p>参考博客：<a href="https://blog.csdn.net/robinhjwy/article/details/79174914" target="_blank" rel="noopener">RANSAC算法理解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;RANSAC（Random Sample Consensus，随机取样一致）&lt;/strong&gt;是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。&lt;/p&gt;
&lt;p&gt;一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图
      
    
    </summary>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="RANSAC" scheme="http://a-kali.github.io/tags/RANSAC/"/>
    
  </entry>
  
  <entry>
    <title>SIFT特征提取和描述</title>
    <link href="http://a-kali.github.io/2020/12/28/SIFT%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%8F%8F%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/28/SIFT特征提取和描述/</id>
    <published>2020-12-28T10:26:10.000Z</published>
    <updated>2020-12-30T09:12:19.343Z</updated>
    
    <content type="html"><![CDATA[<p>SIFT，即<strong>尺度不变特征变换（Scale-invariant feature transform，SIFT）</strong>，是用于图像处理领域对局部特征的一种描述方法。主要原理是<strong>通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点的值生成一个用于稳定描述该特征点的向量</strong>。该算法常用于特征匹配领域。</p><p>SIFT算法具有如下一些特点：</p><ol><li>SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</li><li>区分性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；</li><li>多量性，即使少数的几个物体也可以产生大量的SIFT特征向量；</li><li>高速性，经优化的SIFT匹配算法甚至可以达到实时的要求；</li><li>可扩展性，可以很方便的与其他形式的特征向量进行联合。</li></ol><h1 id="一、图像尺度空间"><a href="#一、图像尺度空间" class="headerlink" title="一、图像尺度空间"></a>一、图像尺度空间</h1><p>在一定范围内，无论物体是大还是小，人眼都能分辨出来；但计算机对不同尺度下的物体分辨能力却很低。所以要让机器能够对物体在不同尺度下有一个统一的认知，就需要考虑图像在不同尺度下都存在的特点。</p><p>不同尺度的图像可以通过<strong>高斯模糊</strong>（或称为<a href="https://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/">高斯滤波</a>）来模拟：<br>$$<br>L(x, y, \sigma)=G(x, y, \sigma)*I(x, y)<br>$$<br>其中I为图像，G为高斯函数：<br>$$<br>G(x, y, \sigma)=\frac{1}{2\pi\sigma^2}e^{\frac{x^2+y^2}{2\sigma^2}}<br>$$<br>其中σ用于控制滤波器权重，σ越大，权重数值分布越均匀，即周边值更大、中心值更小，得到的图像越模糊。</p><p><img src="https://i.loli.net/2020/12/27/LU4sBYMQXWIb8Fx.png" alt="image.png"></p><p>通过这个方法可以得到不同尺度空间的图像，图像越模糊，就相当于得到了更远距离/更小尺度空间的图像。</p><h1 id="二、高斯差分金字塔"><a href="#二、高斯差分金字塔" class="headerlink" title="二、高斯差分金字塔"></a>二、高斯差分金字塔</h1><p>对于一张图像进行n-1次上/下采样操作，得到的n个不同分辨率的结果可以组合成一个<strong>图像金字塔</strong>。对金字塔的每一层进行m次不同程度的高斯模糊，最终得到n×m张图像。</p><p><img src="https://i.loli.net/2020/12/27/HerEtTBQoWiIXD2.png" alt="image.png"></p><p>之后对金字塔每一层得到的模糊结果分别求差，得到n×(m-1)个二维数组，得到的这个结果就是<strong>高斯差分金字塔（Difference of Gaussian, DOG）</strong>。DOG中较大的值表示该点对于不同的模糊程度变化更大，该点更有可能是特征的关键点/边缘。找出<strong>局部极值点</strong>需要对每个点与其周围的26个点（同平面8个，上下各9个）点进行比较。</p><p><img src="https://i.loli.net/2020/12/28/9HfCAV4gOTuz57l.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/28/QX2VGIU7OmFjH98.png" alt="image.png"></p><p>DOG用数学公式表示如下：<br>$$<br>D(x,y,\sigma)=L(x, y, k\sigma)-L(x, y, \sigma)<br>$$</p><h1 id="三、特征描述"><a href="#三、特征描述" class="headerlink" title="三、特征描述"></a>三、特征描述</h1><h2 id="1-特征点的方向"><a href="#1-特征点的方向" class="headerlink" title="1    特征点的方向"></a>1    特征点的方向</h2><p>每个像素点L(x, y)的梯度的模m(x, y)以及方向θ(x, y)计算如下：<br>$$<br>m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L{(x,y-1)}]^2}\<br>\theta(x,y)=arctan \frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}<br>$$<br>每个像素点可以得到<strong>位置(x, y)、尺度m和方向θ</strong>三个信息。对于每个特征点，统计其邻域所有像素点在8个方向上的尺度和，尺度和最大的方向视为该特征点的<strong>主方向</strong>，占比超过主方向80%的视为<strong>辅方向</strong>。具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，于是一个特征点就产生了多个坐标、尺度相同，但方向不同的特征点。</p><p><img src="https://i.loli.net/2020/12/28/qjAZ2HrOoeJ8law.png" alt="image.png"></p><h2 id="2-生成特征描述"><a href="#2-生成特征描述" class="headerlink" title="2    生成特征描述"></a>2    生成特征描述</h2><p>本部分将使用邻域像素的方向和尺度为该特征点生成一个唯一的指纹，称为<strong>描述符</strong>。首先在关键点周围采用16×16的邻域，将该16×16区域进一步划分为4×4子块。由于子块中的每一个像素都具有8个方向中的一个，并且具有尺度。于是对于每一个子块都能用一个长度为8的向量来表示该子块所有像素在8个方向上的尺度和。</p><p>最终对于每一个特征点，我们得到了一个总长度为4×4×8=128的特征描述符。</p><p><img src="https://i.loli.net/2020/12/28/26wiWJ8cZzmL9aV.png" alt="image.png"></p><p>计算两个特征点描述符之间的欧氏距离即可进行匹配。原算法中使用的是kd树进行搜索匹配，这里不作详细描述。</p><p><img src="https://i.loli.net/2020/12/28/rKe1gRlLtJNPi4k.png" alt="image.png"></p><h1 id="四、OpenCV中的SIFT算法"><a href="#四、OpenCV中的SIFT算法" class="headerlink" title="四、OpenCV中的SIFT算法"></a>四、OpenCV中的SIFT算法</h1><p>由于算法版权问题，SIFT算法只能在OpenCV3.4及以下版本才能使用。</p><p>特征描述：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sift = cv2.xfeatures2d.SIFT_create()  <span class="comment"># 创建一个SIFT对象</span></span><br><span class="line">kp, des = sift.detectAndCompute(img_gray, <span class="literal">None</span>) <span class="comment"># 返回关键点对象和以及每个关键点的特征向量</span></span><br><span class="line">show_kp_img = cv2.drawKeypoints(img_gray, kp, img)  <span class="comment"># 在图像中标出关键点</span></span><br></pre></td></tr></table></figure><p>特征匹配：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一对一匹配</span></span><br><span class="line">bf = cv2.BFMatcher(crossCheck=<span class="literal">True</span>)  </span><br><span class="line">matchs = bf.match(des1, des2)</span><br><span class="line">matches.sort(key=<span class="keyword">lambda</span> x: x.distance)</span><br><span class="line"><span class="comment"># 可视化匹配结果</span></span><br><span class="line">matched_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:<span class="number">10</span>], <span class="literal">None</span>, flags=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SIFT，即&lt;strong&gt;尺度不变特征变换（Scale-invariant feature transform，SIFT）&lt;/strong&gt;，是用于图像处理领域对局部特征的一种描述方法。主要原理是&lt;strong&gt;通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>OpenCV与角点检测</title>
    <link href="http://a-kali.github.io/2020/12/28/OpenCV%E4%B8%8E%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/28/OpenCV与角点检测/</id>
    <published>2020-12-28T10:24:08.000Z</published>
    <updated>2020-12-28T10:25:13.604Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Harris角点检测原理"><a href="#Harris角点检测原理" class="headerlink" title="Harris角点检测原理"></a>Harris角点检测原理</h1><p>Harris角点检测的基本原理：类似于边缘检测，只不过边缘检测只判断单个方向的梯度，而角点检测判断多个方向的梯度/相似性。</p><p>对于一张灰度图，计算在点(x, y)处平移(Δx, Δy)后的自相似性c：<br>$$<br>c(x, y;Δx, Δy)=\sum_{(u, v)\in W(x,y)}w(u,v)(P(u,v)-P(u+Δx, v+Δy))^2<br>$$<br>W(x, y)表示以点(x, y)为中心的窗口；而w(x, y)是每个像素的权重，既可以是常数，也可以是高斯加权函数；P(u, v)表示坐标为(u, v)的像素值。上述公式表示计算窗口滑动前后，窗口中的每个像素及其对应像素的差值加权总和。</p><p>对平移后的像素点进行泰勒一阶展开，可得：<br>$$<br>P(u+Δx, v+Δy)\approx P(u,v)+P_x(u,v)Δx+P_y(u,v)Δy<br>$$<br>于是相似性c简化后如下，此时可以看出这是一个椭圆函数：<br>$$<br>c(x, y;Δx, Δy)\approx\ AΔx^2+2CΔxΔy+BΔy^2<br>$$<br>其中<br>$$<br>A=I_x(x,y)^2\<br>B=I_y(x,y)^2\<br>C=I_x(x,y)I_y(x,y)<br>$$<br>经过对角化消除C后得：<br>$$<br>c(x, y;Δx, Δy)\approx\ \lambda_1Δx^2+\lambda_2Δy^2<br>$$<br>可以看出λ越大，表示平移前后的滑窗灰度值差别越大。λ1大表示在横坐标上相差较大，λ2大表示在纵坐标上相差较大。当λ1和λ2的值都很大且值相近时，判断(x, y)为物体角点。</p><p><img src="https://i.loli.net/2020/12/24/ZGvlFgDmr7i1yOT.png" alt="image.png"></p><p>于是可以通过计算<strong>角点响应值</strong>R来判断该点是否为角点。<br>$$<br>R=\lambda_1\lambda_2-k (\lambda_1+\lambda_2)^2<br>$$<br>其中k为系数，通常取值0.04 ~ 0.06。R&gt;&gt;0表示该点为角点，R≈0表示该点为平坦区，R&lt;&lt;0表示该点为边界。</p><h1 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.cornerHarris(img, blockSize, ksize, k)</span><br></pre></td></tr></table></figure><ul><li>img：输入图像；</li><li>blockSize：滑动窗口的大小；</li><li>ksize：Sobel求导中使用的矩阵大小；</li><li>k：计算角点响应值的系数。</li></ul><p>检测结果：</p><p><img src="https://i.loli.net/2020/12/24/8YvEOeaNGot7qSz.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Harris角点检测原理&quot;&gt;&lt;a href=&quot;#Harris角点检测原理&quot; class=&quot;headerlink&quot; title=&quot;Harris角点检测原理&quot;&gt;&lt;/a&gt;Harris角点检测原理&lt;/h1&gt;&lt;p&gt;Harris角点检测的基本原理：类似于边缘检测，只不过边缘检
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="角点检测" scheme="http://a-kali.github.io/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>图像直方图与均衡化</title>
    <link href="http://a-kali.github.io/2020/12/28/%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%B8%8E%E5%9D%87%E8%A1%A1%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/12/28/图像直方图与均衡化/</id>
    <published>2020-12-28T10:21:39.000Z</published>
    <updated>2020-12-28T10:22:58.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a>图像直方图</h1><p><strong>图像直方图</strong>即对图像中每个像素值出现次数的统计直方图。</p><p><img src="https://i.loli.net/2020/12/21/xt3kON27wSyeZ85.png" alt="image.png"></p><p>OpenCV：<code>cv2.calcHist(images, channels, mask, histSize, ranges)</code></p><ul><li>images：输入图像；</li><li>channels：输入一个包含[0, 1, 2]的列表选择通道，全选表示BGR，[0]表示灰度图；</li><li>mask：与images对齐的掩码，用于标记需要统计的像素，全选则输入None；</li><li>histSize：直方图横坐标的个数；</li><li>ranges：像素值范围，默认为[0, 256]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'cat.jpg'</span>)</span><br><span class="line">color = (<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate(color):</span><br><span class="line">    hist = cv2.calcHist([img], [i], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    plt.plot(hist, color=col)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/21/c4CFThHt6OUIVZr.png" alt="image.png"></p><h1 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h1><p><strong>直方图均衡化</strong>是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。其基本思想是对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减。从而达到清晰图像的目的。</p><p><img src="https://i.loli.net/2020/12/21/DfW9auiAH875oBJ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/21/KpfIYFsvorSGyRT.png" alt="image.png"></p><p>步骤：</p><ol><li>确定图像的灰度级（通常情况下，如果我们的图像是彩色。需要将其转换为灰度图像，其灰度级一般是0-255）；</li><li>统计每一个灰度在原始图像上的像素所占总体的比例，即每个灰度的概率；</li><li>计算累加概率（将低灰度级的概率累加到高灰度级的概率上）；</li><li>根据公式求映射结果：</li></ol><p>$$<br>SS(i)=int((max(pix)-min(pix))*S(i)+0.5)<br>$$</p><p><img src="https://i.loli.net/2020/12/21/k1VlTa9vjCKhszf.png" alt="image.png"></p><p>OpenCV代码：<code>cv2.equalizeHist(img)</code></p><p>虽然直方图均衡化后能让图像整体更加清晰，但有时候会使得原本突出的局部特征变得模糊（如图中的人脸）。</p><p><img src="https://i.loli.net/2020/12/21/Bep6arx71ok8FLb.png" alt="image.png"></p><p>可以使用<strong>自适应直方图均衡化</strong>解决这个问题。将图像划分为多个小区域（在opencv中默认为8×8），对每一个划分子域分别进行直方图均衡化。最后使用双线性插值对每一个子域的边界进行拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a CLAHE object (Arguments are optional).</span></span><br><span class="line">clahe = cv2.createCLAHE(clipLimit=<span class="number">2.0</span>, tileGridSize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">img = clahe.apply(img)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像直方图&quot;&gt;&lt;a href=&quot;#图像直方图&quot; class=&quot;headerlink&quot; title=&quot;图像直方图&quot;&gt;&lt;/a&gt;图像直方图&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;图像直方图&lt;/strong&gt;即对图像中每个像素值出现次数的统计直方图。&lt;/p&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="直方图" scheme="http://a-kali.github.io/tags/%E7%9B%B4%E6%96%B9%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（二）：骨干网络的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（二）：骨干网络的改进/</id>
    <published>2020-12-21T00:17:53.000Z</published>
    <updated>2020-12-21T00:21:32.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CSPDarknet53"><a href="#CSPDarknet53" class="headerlink" title="CSPDarknet53"></a>CSPDarknet53</h1><p>在Darknet53的基础上参考CSPNet加入了CSP(Cross Stage Paritial)结构。整个backbone划分为5个模块，每个模块下采样2倍。</p><p>如下图所示，CSP结构仅仅是将特征的一部分直接concat到block的末尾，这个操作使得该block中的每个dense layer的梯度不再直接参与更浅层的梯度计算，而是新计算出一个值（于是可以在反向传播到浅层时，清除深层的dense layer梯度信息），大量减少了内存的消耗和计算瓶颈。</p><p><img src="https://i.loli.net/2020/12/21/hrMEojey9fZ13LR.png" alt="image-20201219060045088.png"></p><h1 id="Mish激活函数"><a href="#Mish激活函数" class="headerlink" title="Mish激活函数"></a>Mish激活函数</h1><p>$$<br>Mish = x*tanh(ln(1+e^x))<br>$$</p><p><img src="https://i.loli.net/2020/12/19/7AFq31YoTClmSDt.png" alt="image.png"></p><p>可以看出Mish比ReLU更加平滑，在每个点上都是可微的，优化效果更好；同时允许了较小的负梯度流入，更好地保证信息的流动。在YOLOv4中仅backbone使用了Mish，其它地方都用的Leaky ReLU。</p><h1 id="Dropblock"><a href="#Dropblock" class="headerlink" title="Dropblock"></a>Dropblock</h1><p>Dropblock与Dropout类似。Dropout会随机丢弃网络中的一些信息，但卷积层对随机丢弃的信息并不敏感，即使随机丢弃一些像素，卷积也能从相邻的像素学到相同的信息。</p><p>于是Dropblock对整个局部区域进行丢弃，相当于对特征图进行CutOut操作。</p><p><img src="https://i.loli.net/2020/12/19/PiCgEXtkaUsK9cl.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CSPDarknet53&quot;&gt;&lt;a href=&quot;#CSPDarknet53&quot; class=&quot;headerlink&quot; title=&quot;CSPDarknet53&quot;&gt;&lt;/a&gt;CSPDarknet53&lt;/h1&gt;&lt;p&gt;在Darknet53的基础上参考CSPNet加入了CSP(C
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（一）：输入端的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%BE%93%E5%85%A5%E7%AB%AF%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（一）：输入端的改进/</id>
    <published>2020-12-21T00:12:33.000Z</published>
    <updated>2020-12-21T00:15:33.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mosaic"><a href="#Mosaic" class="headerlink" title="Mosaic"></a>Mosaic</h1><p>YOLOv4中提出的<strong>Mosaic数据增强</strong>是参考2019年底提出的<strong>CutMix数据增强</strong>的方式，但CutMix只使用了两张图片进行拼接，而Mosaic则采用了4张图片，<strong>随机缩放</strong>、<strong>随机裁剪</strong>、<strong>随机排布</strong>的方式进行拼接。一张图相当于4张图，可以减少训练所需的batch size。</p><p><img src="https://i.loli.net/2020/08/17/XseLfUlb6SiREVW.png" alt="image.png"></p><h1 id="CmBN"><a href="#CmBN" class="headerlink" title="CmBN"></a>CmBN</h1><p>在梯度累积时，模型不同batch的参数梯度会累积在一起一次性反向传播更新参数，但BN的统计量却是在每个batch迭代更新的，这使得反向传播时会产生偏差。<strong>CmBN(Cross mini-Batch Normalization)</strong>则是在梯度累积的同时保证BN的统计量只累积更新，在反向传播的同时更新统计量。</p><p><img src="https://i.loli.net/2020/12/17/5rEOw9ouQyHj3n8.png" alt="image.png"></p><h1 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h1><p><strong>SAT(Self Adversarial Training)</strong>即自对抗训练，文中没有详细说明，可以参考论文Adversarial Examples for Semantic Segmentation and Object Detection。</p><p>第一阶段，将权重设置为固定，图片设置为可导，对目标检测正确的区域采用反向label（即错误的label），计算Loss并进行反向传播，对图片产生扰动，叠加到图片上实现攻击。此时图片肉眼看上去变化不大，但对于神经网络来说，图片已经更接近于反向label。在第二阶段就是正常训练，把被攻击了的图片输入网络进行训练。作者将其归纳为一种数据增强操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mosaic&quot;&gt;&lt;a href=&quot;#Mosaic&quot; class=&quot;headerlink&quot; title=&quot;Mosaic&quot;&gt;&lt;/a&gt;Mosaic&lt;/h1&gt;&lt;p&gt;YOLOv4中提出的&lt;strong&gt;Mosaic数据增强&lt;/strong&gt;是参考2019年底提出的&lt;stro
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】MaxPooling和AveragePooling</title>
    <link href="http://a-kali.github.io/2020/12/21/MaxPooling%E5%92%8CAveragePooling/"/>
    <id>http://a-kali.github.io/2020/12/21/MaxPooling和AveragePooling/</id>
    <published>2020-12-21T00:10:42.000Z</published>
    <updated>2020-12-30T09:32:08.922Z</updated>
    
    <content type="html"><![CDATA[<p>在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了<strong>辨识度更高的特征</strong>，并且提供了<strong>非线性</strong>。</p><p>而Average Pooling通常用来做特征对齐，在<strong>减小特征图尺寸</strong>的同时，保证特征的<strong>完整性</strong>。比如说DenseNet的模块之间大多采用Average Pooling进行连接。或者是用于网络的最后一层，<strong>代替平铺操作</strong>，将三维特征图转化为一维向量。</p><p><strong>Q：MaxPooling的反向传播是如何进行的？</strong></p><p>梯度正常传播给最大值，并将其余非最大值的损失/梯度置零。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了&lt;strong&gt;辨识度更高的特征&lt;/strong&gt;，并且提供了&lt;strong&gt;非线性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而Average Pooling通常用来做特征对齐，在
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="池化" scheme="http://a-kali.github.io/tags/%E6%B1%A0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Dropout</title>
    <link href="http://a-kali.github.io/2020/12/13/Dropout/"/>
    <id>http://a-kali.github.io/2020/12/13/Dropout/</id>
    <published>2020-12-13T00:13:47.000Z</published>
    <updated>2020-12-13T00:15:00.739Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout有什么作用？</strong></p><p>解决过拟合的问题。</p><p><strong>Q：Dropout如何实现？</strong></p><p>使该层每个神经元的激活层都有一定概率输出0。</p><p><strong>Q：Dropout 反向传播的处理</strong></p><p>反向传播时忽略被Dropout的神经元。</p><p><strong>Q：Dropout是失活神经元还是失活连接？</strong></p><p>失活神经元并清除其周围连接。</p><p><strong>Q：Dropout为什么能防止过拟合？</strong></p><ol><li>使模型泛化能力更强，不依赖于某些局部的特征；</li><li>Dropout可以看作多个共享部分参数的模型的集成（其实这条跟上面那条是共通的）；</li></ol><p><strong>Q：Dropout在训练和测试时有何不同？</strong></p><p>Dropout在测试时不会失活神经元。并且在训练时根据失活率p对神经元权重进行缩放，即除以1-p；或者在测试时乘以p。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout有什么作用？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决过拟合的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：Dropout如何实现？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使该层每个神经元的激活层都有一定概率输出0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：D
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="Dropout" scheme="http://a-kali.github.io/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】FLOPs与参数量的计算</title>
    <link href="http://a-kali.github.io/2020/12/13/FLOPs%E4%B8%8E%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97/"/>
    <id>http://a-kali.github.io/2020/12/13/FLOPs与参数量的计算/</id>
    <published>2020-12-12T20:36:07.000Z</published>
    <updated>2021-01-20T01:17:19.603Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h1><p><strong>FLOPs(floating point operations)</strong>，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。注意不要和FLOPS(floating point operations per second)搞混。</p><p>拿卷积操作举例。这里需要注意，卷积的计算量如果从output map size考虑的话，是可以与input map size无关的。因为卷积的每一次矩阵相乘都一定输出output map的一个像素点，但如果从input map下手的话，还需要考虑stride和padding。</p><p>所以<strong>卷积层</strong>的FLOPs计算公式为：<br>$$<br>FLOPs=(2×C_i×K^2)×(H×W×C_o)<br>$$<br>其中Ci和Co分别为输入输出的通道数，K为卷积核大小，H和W分别为output map的高和宽。</p><p>这个公式前半部分的运算就是每计算一个输出元素所需要的计算量，就是output map的元素数量。之所以要×2是因为一次卷积运算需要进行乘法和加法两种操作（相乘后求和），比如说两个3×3大小的矩阵相乘求和得到一个元素值，需要进行3×3次乘法操作和3×3-1次加法操作，再加上神经网络中的bias，运算量一共为2×3×3。</p><p>同理可得<strong>全连接层</strong>的FLOPs计算公式为：<br>$$<br>FLOPs=2×I×O<br>$$</p><h1 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h1><p>卷积参数量的计算比较简单，这里带过一下：<br>$$<br>params=(K^2×C_i+1)×C_o<br>$$</p><p>实际工程上要计算这两个值的话的话，感觉github上有挺多包的……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FLOPs&quot;&gt;&lt;a href=&quot;#FLOPs&quot; class=&quot;headerlink&quot; title=&quot;FLOPs&quot;&gt;&lt;/a&gt;FLOPs&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;FLOPs(floating point operations)&lt;/strong&gt;，意指浮点运算数，
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="FLOPs" scheme="http://a-kali.github.io/tags/FLOPs/"/>
    
      <category term="参数量" scheme="http://a-kali.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV中的平滑、腐蚀膨胀和边缘检测</title>
    <link href="http://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/13/OpenCV中的平滑、腐蚀膨胀和边缘检测/</id>
    <published>2020-12-12T20:32:16.000Z</published>
    <updated>2020-12-30T09:13:33.947Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像平滑与滤波"><a href="#图像平滑与滤波" class="headerlink" title="图像平滑与滤波"></a>图像平滑与滤波</h1><p>图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，这些图像滤波都是一些简单的卷积操作。</p><ul><li>均值滤波：即在一个卷积核的感受野内的平均值作为该像素点的值，opencv代码为<code>cv2.blur(img, (3, 3))</code>。</li><li>高斯滤波：高斯滤波会根据距离取周围的像素点加权和，越近的像素权重越大。opencv代码为<code>cv2.GaussianBlur</code>。由于高斯滤波对中心点权重最高，使得其对椒盐类的噪声处理效果较差。</li><li>中值滤波：即在一个卷积核的感受野内的中值作为该像素点的值，opencv代码为<code>cv2.midianBlur(img, (3, 3))</code>。中值滤波对中心点的取值较为绝对化，故对椒盐类的噪声处理能力较强。</li></ul><p>下面三张图分别为原图、高斯滤波结果、中值滤波结果。</p><p><img src="https://i.loli.net/2020/12/05/tHxuK4iXhCP85cz.png" alt="image.png"></p><h1 id="图像的形态学操作"><a href="#图像的形态学操作" class="headerlink" title="图像的形态学操作"></a>图像的形态学操作</h1><h2 id="1-腐蚀与膨胀"><a href="#1-腐蚀与膨胀" class="headerlink" title="1    腐蚀与膨胀"></a>1    腐蚀与膨胀</h2><p>进行膨胀(dilation)操作时，使用卷积核遍历图像，使用内核覆盖区域的最大相素值代替锚点位置的相素。这一最大化操作将会导致图像中的亮区扩大，因此名为膨胀 。腐蚀(erosion)则与膨胀相反。在OpenCV中代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line">cv2.erode(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 腐蚀</span></span><br><span class="line">cv2.dilate(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 膨胀</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/05/XHDSuCR7yxOYIin.png" alt="image.png"></p><h2 id="2-开运算与闭运算"><a href="#2-开运算与闭运算" class="headerlink" title="2    开运算与闭运算"></a>2    开运算与闭运算</h2><p>开运算指的是先腐蚀后膨胀的操作，闭运算则是先膨胀后腐蚀的操作。通常用于消除噪点、沟壑，平滑物体轮廓。在OpenCV中代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # 开运算</span><br><span class="line">cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)  # 闭运算</span><br></pre></td></tr></table></figure><h2 id="3-梯度计算"><a href="#3-梯度计算" class="headerlink" title="3    梯度计算"></a>3    梯度计算</h2><p>使用膨胀的结果减去腐蚀的结果可以计算得到图像的梯度，但OpenCV中提供了更便捷的操作：<code>cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</code>。</p><h1 id="Sobel算子"><a href="#Sobel算子" class="headerlink" title="Sobel算子"></a>Sobel算子</h1><p>Sobel算子是计算机视觉领域的一种重要处理方法。主要用于获得数字图像的<strong>一阶梯度</strong>，常用于<strong>边缘检测</strong>（注意与轮廓检测相区分）。Sobel算子是把图像中每个像素的上下左右四领域的<strong>灰度值加权差</strong>，在边缘处达到极值从而检测边缘。</p><p>该算子包含两组3x3的矩阵，分别为横向及纵向，将之与图像作平面<strong>卷积</strong>，即可分别得出横向及纵向的差值。如果以A代表原始图像，Gx及Gy分别代表经横向及纵向边缘检测的图像，其公式如下：</p><p><img src="https://i.loli.net/2020/12/04/rIq3OVLCfDnWvpu.png" alt="image.png"></p><p>在OpenCV中，可通过<code>cv2.Sobel(src, ddepth, dx, dy, ksize)</code>调用Sobel算子。具体操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'pic.png'</span>, cv2.IMREAD_GRAYSCALE)  <span class="comment"># 读取灰度图</span></span><br><span class="line">sobelx = cv2.Sobel(img, cv2.CV_64F, <span class="number">1</span>, <span class="number">0</span>, ksize=<span class="number">3</span>)  <span class="comment"># x方向的梯度</span></span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)  <span class="comment"># 取绝对值将复数转为正数</span></span><br><span class="line">sobely = cv2.Sobel(img, cv2.CV_64F, <span class="number">0</span>, <span class="number">1</span>, ksize=<span class="number">3</span>)  <span class="comment"># y方向的梯度</span></span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x和y方向分别求到的梯度加权求和</span></span><br><span class="line">sobelxy = cv2.addWeighted(sobelx, <span class="number">0.5</span>, sobely, <span class="number">0.5</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>求得的结果如下所示，其中左边是原图，右边是边缘检测结果：</p><p><img src="https://i.loli.net/2020/12/04/CgwknPWiDm12b7x.png" alt="image.png"></p><p>因为直接求两个方向的梯度效果较差，所以此处是对两个方向分别求梯度再加权平均，而不是直接求两个方向的梯度。</p><h1 id="Canny边缘检测"><a href="#Canny边缘检测" class="headerlink" title="Canny边缘检测"></a>Canny边缘检测</h1><p>Canny边缘检测算法是对图像进行的一系列用于边缘检测的操作步骤：</p><ol><li>使用高斯滤波器对图像进行平滑处理，滤除噪声；</li><li>计算图像中每个像素点的梯度强度和方向；</li><li>使用非极大值抑制消除边缘检测带来的杂散效应；</li><li>应用双阈值检测找出潜在的边缘，抑制孤立弱边缘。</li></ol><p><img src="https://i.loli.net/2020/12/05/f3BTRk6cFPYgqHG.png" alt="image.png"></p><p>OpenCV：<code>cv2.Canny(img, min_threshold, max_threshold)</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像平滑与滤波&quot;&gt;&lt;a href=&quot;#图像平滑与滤波&quot; class=&quot;headerlink&quot; title=&quot;图像平滑与滤波&quot;&gt;&lt;/a&gt;图像平滑与滤波&lt;/h1&gt;&lt;p&gt;图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="边缘检测" scheme="http://a-kali.github.io/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】手撕卷积</title>
    <link href="http://a-kali.github.io/2020/12/03/%E6%89%8B%E6%92%95%E5%8D%B7%E7%A7%AF/"/>
    <id>http://a-kali.github.io/2020/12/03/手撕卷积/</id>
    <published>2020-12-03T09:18:27.000Z</published>
    <updated>2020-12-03T09:19:44.350Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(z, K, b, padding=<span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多通道卷积前向过程</span></span><br><span class="line"><span class="string">    :param z: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数</span></span><br><span class="line"><span class="string">    :param K: 卷积核,形状(C,D,k1,k2), C为输入通道数，D为输出通道数</span></span><br><span class="line"><span class="string">    :param b: 偏置,形状(D,)</span></span><br><span class="line"><span class="string">    :param padding: padding</span></span><br><span class="line"><span class="string">    :param strides: 步长</span></span><br><span class="line"><span class="string">    :return: 卷积结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    padding_z = np.lib.pad(z, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (padding[<span class="number">0</span>], padding[<span class="number">0</span>]), (padding[<span class="number">1</span>], padding[<span class="number">1</span>])), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)  <span class="comment"># 在H和W维度上进行padding</span></span><br><span class="line">    N, _, height, width = padding_z.shape</span><br><span class="line">    C, D, k1, k2 = K.shape</span><br><span class="line">    conv_z = np.zeros((N, D, <span class="number">1</span> + (height - k1) // strides[<span class="number">0</span>], <span class="number">1</span> + (width - k2) // strides[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> np.arange(N):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> np.arange(D):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> np.arange(height - k1 + <span class="number">1</span>)[::strides[<span class="number">0</span>]]:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> np.arange(width - k2 + <span class="number">1</span>)[::strides[<span class="number">1</span>]]:</span><br><span class="line">                    conv_z[n, d, h // strides[<span class="number">0</span>], w // strides[<span class="number">1</span>]] = np.sum(padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]</span><br><span class="line">    <span class="keyword">return</span> conv_z</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="卷积" scheme="http://a-kali.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>CenterLoss简述</title>
    <link href="http://a-kali.github.io/2020/12/03/CenterLoss%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/03/CenterLoss简述/</id>
    <published>2020-12-03T09:18:01.000Z</published>
    <updated>2020-12-12T20:43:25.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h1><p>对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图为神经网络在MNIST数据集上进行手写体数字分类任务得到的特征分布，在类与类之间能看到很明显的界限，能够便于softmax对特征进行分类。但是仍然可以看出部分不同类的样本距离很近，而同类的样本分布距离较远，这有可能对最终的分类结果有影响。</p><p><img src="https://i.loli.net/2020/12/02/4M5jQxUcpW3H6G1.png" alt="image.png"></p><p>而<strong>Center Loss</strong>的提出就是为了<strong>缩小类内(intra-class)距离</strong>。其公式表示如下：<br>$$<br>L_c=\frac{1}{2}\sum^m_{i=1}‖x_i-c_{yi}‖^2_2<br>$$<br>其中$c_{yi}$表示第y个类别的特征中心，$x_i$表示全连接层之前的特征，$m$表示mini-batch的大小。该损失函数试图使每个样本的特征向其类别中心靠近。</p><p>至于类中心怎么更新迭代，这里就不写了（公式太复杂了yingyingying）。反正最终得到的特征分布如下：</p><p><img src="https://i.loli.net/2020/12/02/pdn8rZzMEN7OaoA.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Center-Loss&quot;&gt;&lt;a href=&quot;#Center-Loss&quot; class=&quot;headerlink&quot; title=&quot;Center Loss&quot;&gt;&lt;/a&gt;Center Loss&lt;/h1&gt;&lt;p&gt;对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图
      
    
    </summary>
    
    
      <category term="损失函数" scheme="http://a-kali.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>CycleGAN简述</title>
    <link href="http://a-kali.github.io/2020/11/19/CycleGAN%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/11/19/CycleGAN简述/</id>
    <published>2020-11-19T04:18:04.000Z</published>
    <updated>2020-11-19T04:19:12.186Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks)</p><p>PyTorch代码：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p><p>CycleGAN（或许）是首个将GAN用于风格迁移的网络。</p><p>CycleGAN的作用本质是Image2Image的风格迁移，并不是传统意义上的生成。</p><p><img src="https://i.loli.net/2020/11/16/x2lpJWfVCsqhyja.png" alt="image.png"></p><p>相比于其它Image2Image的网络，CycleGAN并不需要成对的样本数据用来训练，而仅仅需要两组无关的图片，其中目标样本通常为一组风格相似的图片集。</p><p><img src="https://i.loli.net/2020/11/16/Af6MzV79s8NCoTE.png" alt="image.png"></p><p>为了避免网络将X中所有样本映射到Y中的某一个样本，CycleGAN同时包含Y→X的映射，要求网络能够对自己生成的图片进行还原。这使得F(X)必须包含X的内容信息，因此更容易学习Y整体的风格信息（大概）。</p><p><img src="https://i.loli.net/2020/11/16/ivueyVzcxMwRrAk.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](Unpaired Image-to-Image Translation using Cycle-Co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="GAN" scheme="http://a-kali.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】如何进行多标签分类</title>
    <link href="http://a-kali.github.io/2020/11/12/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/"/>
    <id>http://a-kali.github.io/2020/11/12/如何进行多标签分类/</id>
    <published>2020-11-11T16:37:46.000Z</published>
    <updated>2020-11-11T16:38:38.377Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：多标签分类怎么解决？</strong></p><p>解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。</p><p>第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这应该是最简单常见的方法，但该方法忽略了标签与标签之间的相关性。</p><p>第二种是使用CNN+RNN+Embedding的策略（参考文献：<a href="https://arxiv.org/abs/1604.04573" target="_blank" rel="noopener">CNN-RNN: A Unified Framework for Multi-label Image Classification</a>）。该方法考虑到了各个标签之间的关联性，将每个标签映射到高维空间上。每次运行预测一个标签，并根据该标签预测下一个标签。理论上该方法的准确率会比较高，但时间效率很低。</p><p><img src="https://i.loli.net/2020/11/11/S5p13cqzibZQfK2.png" alt="image.png"></p><p><a href="https://www.kaggle.com/c/imet-2019-fgvc6" target="_blank" rel="noopener">Kaggle: iMet Collection 2019 - FGVC6</a> 大都会文物分类竞赛就是多标签分类任务，几乎所有参赛者都使用第一种方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：多标签分类怎么解决？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。&lt;/p&gt;
&lt;p&gt;第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>DB：基于可微二值化的实时场景文本检测</title>
    <link href="http://a-kali.github.io/2020/11/12/DB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8F%AF%E5%BE%AE%E4%BA%8C%E5%80%BC%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/11/12/DB：基于可微二值化的实时场景文本检测/</id>
    <published>2020-11-11T16:36:11.000Z</published>
    <updated>2021-01-15T03:15:22.431Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1911.08947" target="_blank" rel="noopener">Real-time Scene Text Detection with Differentiable Binarization</a></p><p>Github：<a href="https://github.com/MhLiao/DB（代码结构很复杂）" target="_blank" rel="noopener">https://github.com/MhLiao/DB（代码结构很复杂）</a></p><p><strong>Structure</strong></p><p>DB(Differentiable Binarization)是一个轻量级的、基于分割的场景文本检测(Scene Text Detection, STD)模型。该模型的原理简洁易懂，在本文中就简单介绍一下。</p><p><img src="https://i.loli.net/2020/11/04/MF73kDbCcueraNL.png" alt="image.png"></p><p>上图展示了DB的模型结构。前半部分是常见的32倍下采样+8倍上采样+特征融合，以及参考FPN采用了后四层反卷积的输出concat到一起进行预测。图中的pred是3×3卷积，输出两张map分别用来表示概率和阈值。使用阈值map对概率map进行二值化后，就将输出结果分为了前景（文本域）和背景。</p><p>这个自适应阈值能把分布比较密集的文本域给隔开，避免混淆成一个文本域。</p><p>值得一提的是阈值map只在训练的时候使用，在测试时仅使用固定阈值。估计在测试时使用DB并不能得到比较有效的提升，出于运算量的考虑决定去掉这部分。</p><p><strong>Loss</strong></p><p>损失函数分为三部分：概率图损失，阈值损失，二值图损失。其中概率图和二值图都使用交叉熵损失函数，而阈值损失使用的是L1损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1911.08947&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-time Scene Text Detection with Differentiable Binariz
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本检查" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%9F%A5/"/>
    
      <category term="DB" scheme="http://a-kali.github.io/tags/DB/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Batch Normalization</title>
    <link href="http://a-kali.github.io/2020/11/12/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E5%BD%92%E4%B8%80%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/11/12/神经网络中的归一化/</id>
    <published>2020-11-11T16:32:30.000Z</published>
    <updated>2020-12-12T23:52:51.697Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：BN解决了什么问题？</strong></p><p>解决两个问题：</p><ol><li>Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入分布变化会非常剧烈，这就使得高层需要不断去重新适应底层的参数更新。</li><li>梯度消失：由于之前Sigmoid一类的激活函数的存在，数据在网络中传播时整体分布逐渐往非线性函数的取值区间的上下限两端靠近，导致反向传播时低层神经网络的梯度消失，神经网络收敛变慢。</li></ol><p><strong>Q：BN的运作方式</strong></p><p>通过一定的规范化手段，把每层神经网络任意神经元输入值的分布强行拉回到均值为0方差为1的标准正态分布。让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。同时固定该层的输入分布，使后一层的神经元不用反复重新适应分布的变化。</p><p>但经过这一步后大部分值落入激活函数的线性区内，使得激活函数失去了其本身的非线性意义，网络表达能力下降。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了缩放平移操作(y=scale*x+shift)。核心思想应该是想找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</p><p><strong>Q：手撕BN</strong></p><p>BatchNorm2D（常用于卷积神经网络）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm2D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, channel, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = np.zeros(channel) <span class="comment"># 用于测试时</span></span><br><span class="line">        self.running_var = np.ones(channel)   <span class="comment"># 同上</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps                        <span class="comment"># 接近于0的数，用于避免分母为0</span></span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="comment"># input.shape: (B, C, H, W)</span></span><br><span class="line">        len_ch = input.size(<span class="number">1</span>)</span><br><span class="line">        output = np.zeros(input.size())</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len_ch):</span><br><span class="line">            in_ch = input[:, i, :, :]</span><br><span class="line">            total_elem = in_ch.numel()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> self.training:</span><br><span class="line">                <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">                mean = in_ch.sum() / total_elem</span><br><span class="line">                var = ((in_ch - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">                out_ch = (in_ch - mean) / (var + self.eps) ** <span class="number">0.5</span>  <span class="comment"># 归一化</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新参数</span></span><br><span class="line">                var_unbiased = ((in_ch - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">                self.running_mean[i] = self.running_mean[i] * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">                self.running_var[i] = self.running_var[i] * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out_ch = (in_ch - self.running_mean[i]) / (self.running_var[i] + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">            out_ch = self.gamma * out_ch + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">            output[:, i, :, :] = out_ch</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>BatchNorm1D大概也能根据以上代码进行修改（我瞎写的，仅供参考）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BatchNorm1D</span><span class="params">()</span>:</span></span><br><span class="line">    gamma, beta = <span class="number">1</span>, <span class="number">0</span>  <span class="comment"># 缩放因子γ和平移因子β，能训练的参数</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, momentum=<span class="number">0.1</span>, eps=<span class="number">1e-5</span>)</span>:</span></span><br><span class="line">        self.running_mean = <span class="number">0</span></span><br><span class="line">        self.running_var = <span class="number">1</span></span><br><span class="line">        self.momentum = momentum   </span><br><span class="line">        self.eps = eps           </span><br><span class="line">        self.training = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line"></span><br><span class="line">        total_elem = input.numel()</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            <span class="comment"># 计算均值和方差，并归一化</span></span><br><span class="line">            mean = input.sum() / total_elem</span><br><span class="line">            var = ((input - mean) ** <span class="number">2</span>).sum() / total_elem</span><br><span class="line">            output = (input - mean) / (var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">                </span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            var_unbiased = ((input - mean) ** <span class="number">2</span>).sum() / (total_elem - <span class="number">1</span>)</span><br><span class="line">            self.running_mean = self.running_mean * (<span class="number">1</span> - self.momentum) + mean * self.momentum</span><br><span class="line">            self.running_var = self.running_var * (<span class="number">1</span> - self.momentum) + var_unbiased * self.momentum</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output = (input - self.running_mean) / (self.running_var + self.eps) ** <span class="number">0.5</span></span><br><span class="line">            </span><br><span class="line">        output = self.gamma * output + self.beta  <span class="comment"># 缩放平移</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p><strong>Q：BN能防止过拟合吗？为什么？</strong></p><p>BN能一定程度上缓解过拟合。BN使得模型在训练时的输出不仅仅根据当前的输入样本信息，还包含了同一batch其他样本的信息。训练时，同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的。相当于在神经网络中进行了数据增强。</p><p><strong>Q：BN 有哪些参数？</strong></p><p>可训练的参数有缩放因子和平移因子，统计参数有均值和方差，超参数有动量，2D的超参数还包含通道数。</p><p><strong>Q：BN 的反向传播推导</strong></p><ul><li><input disabled type="checkbox"> TODO</li></ul><p><strong>Q：BN 在训练和测试的区别？</strong></p><p>训练时使用的是当前batch的样本统计量进行归一化，测试时使用的是在训练过程中更新迭代计算得到的均值和方差进行归一化。</p><p><strong>Q：BN通常放在什么位置？</strong></p><p>BN通常放在激活函数前。因为BN的作用本来就是为了调整上一层的输出分布，让激活层更好地使用这些输出值。</p><p><strong>Q：BN可以防止过拟合吗？</strong></p><p>BN可以一定程度上缓解过拟合。在样本shuffle训练的情况下，某个样本在不同epoch遇到的同一个batch的其他样本都是不一样的，于是会产生不同的均值和标准差，相当于在模型内部做了数据增强。</p><p><strong>Q：BN和Dropout同时用会怎样？怎样才能同时使用？</strong></p><p>Dropout在训练（或测试）阶段会根据神经元保留率来对神经元权重进行缩放，这会导致测试时隐藏层输出值的方差跟训练时不同。而BN此时已经根据训练数据统计固定了方差参数，无法适应改变后的方差。多层累积下来产生方差偏移，影响模型效果。所以只有在Dropout在所有BN后面时能同时使用。</p><p><strong>Q：有什么其它的归一化方法？</strong></p><ul><li>IN(Instance Norm)：实例归一化。与BN的区别在于，BN使用整个batch的统计量作为参数进行归一化，而IN仅使用当前样本的统计量。IN常用于风格迁移任务中。</li><li><a href="https://a-kali.github.io/2020/10/12/AdaIN%EF%BC%9A%E5%9F%BA%E4%BA%8E%E9%80%82%E5%BA%94%E6%80%A7%E5%AE%9E%E4%BE%8B%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E4%BB%BB%E6%84%8F%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/#more">AdaIN</a>：自适应的实例归一化。在IN的基础上，将缩放和平移参数分别固定为目标风格图像的标准差和均值。在风格迁移中可以快速适应任意风格。</li><li><a href="https://zhuanlan.zhihu.com/p/54530247" target="_blank" rel="noopener">LN(Layer Norm)</a>：与BN的区别在于，BN是对于一个batch样本的单个通道进行归一化，LN是对单个样本的所有通道进行归一化。可用于RNN或者小batch。</li><li>GN(Group Norm)：组归一化。和LN类似，比LN多一个超参数G，G表示分组的数量。同样用来解决在小batch时BN效果较差的问题。</li></ul><p><img src="https://i.loli.net/2020/11/11/ZtDTfcHxUvyeioj.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：BN解决了什么问题？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决两个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Internal Covariate Shift：深度神经网络涉及到很多层的叠加，而每一层的参数更新会导致上层的输入数据分布发生变化，通过层层叠加，高层的输入
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
      <category term="BN" scheme="http://a-kali.github.io/tags/BN/"/>
    
      <category term="Normalization" scheme="http://a-kali.github.io/tags/Normalization/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】IoU和mIoU</title>
    <link href="http://a-kali.github.io/2020/11/03/%E3%80%90%E9%9D%A2%E8%AF%95%E9%A2%98%E3%80%91IoU%E5%92%8CmIoU/"/>
    <id>http://a-kali.github.io/2020/11/03/【面试题】IoU和mIoU/</id>
    <published>2020-11-03T14:53:10.000Z</published>
    <updated>2020-11-11T16:31:05.748Z</updated>
    
    <content type="html"><![CDATA[<p>害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。</p><p>虽然老夫从来没做过基于检测框的目标检测项目。</p><p><strong>Q1：啥是IoU？如何计算IoU？</strong></p><p>IoU就是交并比嘛，两个框相交的面积除以合并的面积。</p><p>定义bbox1, bbox2为两个长度为 4 的数组，用于表示两个检测框左上和右下坐标点。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BoxIoU</span><span class="params">(bbox1, bbox2)</span>:</span></span><br><span class="line">x11, y11, x12, y12 = bbox1</span><br><span class="line">    x21, y21, x22, y22 = bbox2</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算相交面积</span></span><br><span class="line">    iw = max(min(x12, x22) - max(x11, x21), <span class="number">0</span>)</span><br><span class="line">    iy = max(min(y12, y22) - max(y11, y21), <span class="number">0</span>)</span><br><span class="line">    inter = iw * iy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算合并面积</span></span><br><span class="line">area1 = (x12 - x11) * (y12 - y11)</span><br><span class="line">    area2 = (x22 - x21) * (y22 - y21)</span><br><span class="line">uni = area1 + area2 - inter</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算交并比</span></span><br><span class="line">    iou = inter / uni</span><br><span class="line">    <span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure><p>其实以上应该都只是基于常规检测框的IoU计算，如果是非矩形检测框或者分割任务中的IoU则要另当别论。</p><p>对于分割任务，定义mask1, mask2为两个相同大小的二维二值numpy数组。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SegIoU</span><span class="params">(mask1, mask2)</span>:</span></span><br><span class="line"><span class="keyword">return</span> (mask1 &amp; mask2).sum() / (mask1 | mask2).sum()</span><br></pre></td></tr></table></figure><p><strong>Q2：啥是mIoU？如何计算mIoU？</strong></p><p>mIoU即均交并比。对于每个类计算一遍IoU后取平均就行了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;害，要开始正儿八经地准备刷面试题了，今个咱就拿这个IoU开刀。&lt;/p&gt;
&lt;p&gt;虽然老夫从来没做过基于检测框的目标检测项目。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q1：啥是IoU？如何计算IoU？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;IoU就是交并比嘛，两个框相交的面积除以合并的面积。
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="IoU" scheme="http://a-kali.github.io/tags/IoU/"/>
    
      <category term="手撕" scheme="http://a-kali.github.io/tags/%E6%89%8B%E6%92%95/"/>
    
  </entry>
  
</feed>
