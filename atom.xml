<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-08-11T15:52:37.981Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hourglass &amp; CornerNet &amp; CenterNet</title>
    <link href="http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/"/>
    <id>http://a-kali.github.io/2020/08/11/Hourglass-CornerNet-CenterNet/</id>
    <published>2020-08-10T16:26:01.000Z</published>
    <updated>2020-08-11T15:52:37.981Z</updated>
    
    <content type="html"><![CDATA[<p>最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。</p><h1 id="Hourglass"><a href="#Hourglass" class="headerlink" title="Hourglass"></a>Hourglass</h1><p>论文链接：<a href="https://arxiv.org/abs/1603.06937" target="_blank" rel="noopener">Stacked Hourglass Networks for Human Pose Estimation</a></p><p>Stacked Hourglass Neworks（以下简称Hourglass）由多个Hourglass模块堆叠而成，其模块对特征图进行下采样后，将特征图上采样到原来的大小，形似沙漏，故名Hourglass。</p><p>Hourglass原本是用于做<strong>人体姿态估计（Human pose estimation）</strong>，其中比较关键的一步就是检测人体上的关键点。人体不同的部位特征不同，所需要的特征图大小也不一样。Hourglass网络能够处理各种尺寸的人体特征，以此来捕捉人体各部位之间的空间关系。</p><p><img src="https://i.loli.net/2020/08/07/YBgedQokvwPlhHV.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/10/hvftgsOw6VFiXIE.png" alt="image.png"></p><p>每个Hourglass模块采用encoder-decoder结构，对输入图下采样提取特征后进行上采样，输出原图大小的heatmap作为关键点的标记。encoder和decoder之间使用残差结构融合前后特征。</p><p><img src="https://i.loli.net/2020/08/11/i2rt3GMlKbUakPN.png" alt="image.png"></p><p>该网络的思想和结构后被其它检测网络广泛采用。</p><h1 id="CornerNet"><a href="#CornerNet" class="headerlink" title="CornerNet"></a>CornerNet</h1><p>论文链接：<a href="https://arxiv.org/abs/1808.01244" target="_blank" rel="noopener">CornerNet: Detecting Objects as Paired Keypoints</a><br>代码链接：<a href="https://github.com/umich-vl/CornerNet" target="_blank" rel="noopener">https://github.com/umich-vl/CornerNet</a></p><p>CornerNet是一个通过检测对角来进行目标检测的网络。主要原理是采用Hourglass作为基本结构，输出检测框对角的Heatmap。</p><p>这个设计真的很<del>反人类</del>反人工智能，因为对角上应该是没有什么特征的。但是该网络的设计思想非常有意思，给后来的Anchor Free类型的目标检测网络提供了思路。</p><p><img src="https://i.loli.net/2020/08/10/GrVdznPJ1qT9pw5.png" alt="image.png"></p><p>CornerNet的整体结构如下图所示，其骨干部分使用Hourglass，输出部分有两个分支模块，分别表示<strong>左上角点预测分支</strong>和<strong>右下角点预测分支</strong>，每个分支模块包含一个<strong>corner pooling</strong>层和3个输出：<strong>heatmaps、embeddings和offsets</strong>。</p><p><img src="https://i.loli.net/2020/08/10/gDtnTWBj3PkXozR.png" alt="image.png"></p><h3 id="输出端"><a href="#输出端" class="headerlink" title="输出端"></a>输出端</h3><ul><li><p><strong>Heatmaps</strong>对角点的位置进行预测，最有可能是角点的位置输出值越高。其gt是基于角点的高斯值；</p></li><li><p><strong>Offset</strong>输出取整计算时丢失的精度信息（感觉用处不是很大）；</p><p><img src="https://i.loli.net/2020/08/11/cpO1Ueq6WPazGK3.png" alt="image.png"></p></li><li><p><strong>Embedding</strong>用来对左上角点和右下角点进行匹配。其输出一个vector，当两个角点的vector距离较小时，则认为这两点为成对点（这个和人脸匹配有点像）。这部分由两个损失函数实现，第一部分用来缩小成对点向量的距离，第二部分用来放大非成对点向量之间的距离。</p><p><img src="https://i.loli.net/2020/08/11/WBEQnLpIN2qCHx3.png" alt="image.png"></p></li></ul><h3 id="Corner-Pooling"><a href="#Corner-Pooling" class="headerlink" title="Corner Pooling"></a>Corner Pooling</h3><p>因为CornerNet是预测左上角和右下角两个角点，但是这两个角点在不同目标上没有相同规律可循，如果采用普通池化操作，那么在训练预测角点支路时会比较困难。考虑到对于每一个左上角点，其所有的特征都在其右下方。因此如果左上角角点经过池化操作后能有其右下方的信息，那么就有利于该点的预测。右下角点同理。</p><p>Corner Pooling便是一种对某个像素的右下方（或左上方）的所有像素进行池化的操作。</p><p><img src="https://i.loli.net/2020/08/11/2lW4Oi1k6qhzF3f.png" alt="image.png"></p><hr><blockquote><p>前方施工⚠</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近get到一个project做停车位检测，参考了几篇论文之后决定用关键点检测的方法，于是顺便读了如下几篇关键点检测相关的神经网络论文。&lt;/p&gt;
&lt;h1 id=&quot;Hourglass&quot;&gt;&lt;a href=&quot;#Hourglass&quot; class=&quot;headerlink&quot; title
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="关键点检测" scheme="http://a-kali.github.io/tags/%E5%85%B3%E9%94%AE%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
      <category term="Hourglass" scheme="http://a-kali.github.io/tags/Hourglass/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>GAN综述：对抗生成网络在图像视频生成领域的算法与应用</title>
    <link href="http://a-kali.github.io/2020/08/10/GAN%E7%BB%BC%E8%BF%B0%EF%BC%9A%E5%AF%B9%E6%8A%97%E7%94%9F%E6%88%90%E7%BD%91%E7%BB%9C%E5%9C%A8%E5%9B%BE%E5%83%8F%E8%A7%86%E9%A2%91%E7%94%9F%E6%88%90%E9%A2%86%E5%9F%9F%E7%9A%84%E7%AE%97%E6%B3%95%E4%B8%8E%E5%BA%94%E7%94%A8/"/>
    <id>http://a-kali.github.io/2020/08/10/GAN综述：对抗生成网络在图像视频生成领域的算法与应用/</id>
    <published>2020-08-10T15:54:01.000Z</published>
    <updated>2020-08-10T16:18:30.149Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2008.02793" target="_blank" rel="noopener">Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>生成对抗网络(GAN)能在不依赖先验假设的情况下合成视觉内容，生成高分辨率逼真图像和视频。大量内容生成型应用基于GAN而创建。本文主要对GANs在视觉方面的算法和应用进行了综述，涵盖了关于稳定GAN训练的关键技术，以及在图像翻译、图像处理、视频合成和神经渲染中的应用。</p><h1 id="2-Introdution"><a href="#2-Introdution" class="headerlink" title="2    Introdution"></a>2    Introdution</h1><p>GAN是由Goodfellow等人提出的深度学习架构，它由<strong>生成网络(generator network)</strong>和<strong>判别网络(discriminator network )</strong>所组成。生成网络尽可能生成逼真样本，判别网络则尽可能去判别该样本是真实样本还是生成出来的假样本。</p><hr><blockquote><p>前方施工……</p></blockquote><hr><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://www.jianshu.com/p/331a078036ee" target="_blank" rel="noopener">GAN万字长文综述</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2008.02793&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generative Adversarial Networks for Image and Video Synthe
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="GAN" scheme="http://a-kali.github.io/tags/GAN/"/>
    
      <category term="综述" scheme="http://a-kali.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>SPFCN：全卷积网络实现停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/04/SPFCN%EF%BC%9A%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/04/SPFCN：全卷积网络实现停车位检测/</id>
    <published>2020-08-04T15:33:29.000Z</published>
    <updated>2020-08-07T13:00:42.460Z</updated>
    
    <content type="html"><![CDATA[<p>论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection</p><p>Github：<a href="https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection" target="_blank" rel="noopener">https://github.com/tjiiv-cprg/SPFCN-ParkingSlotDetection</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>摘要：对于配备自动停车系统的车辆，车位检测的准确性和速度是至关重要的。本文提出了一个基于FCN的检测器，在保证准确性的同时实现更快的速度和更小的模型尺寸。作者制定了一个策略来选择最佳感受野的卷积核，并在每次训练epoch结束后自动删除冗余通道。该模型能够联合检测停车位的角和线特征，并能在常规的处理器上有效地实时运行。该模型在2.3 GHz的CPU上能达到 30 FPS，车位角定位误差1.51±2.14 cm (std. err)，车位检测精度98%，总体满足车载移动终端速度和精度要求。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>该模型使用<a href="https://blog.csdn.net/wangzi371312/article/details/81174452" target="_blank" rel="noopener">Stacked Hourglass Network</a>作为基本结构，同时输出标志点、入口线、边界线的heatmap。此处原理和CenterNet类似。</p><p><img src="https://i.loli.net/2020/08/04/PQqr8x3LBTjpStO.png" alt="image.png"></p><p>该模型的主要特点在于其<strong>SP模块（Select-Prune Module）</strong>，SP模块分为Select模块和Prune模块两部分：</p><ul><li><p><strong>Select Module</strong>：Select模块主要用于选择拥有更合适的感受野的卷积核。考虑到移动端使用类Inception结构对算力要求过大，不能满足实时性。Select模块使用<strong>贡献评估网络（Contribution Evaluation Networks, CEN）</strong>对不同感受野的卷积核进行评估，选择贡献度最高的的卷积核。CEN只是一个简单的MLP模块，接收不同卷积核的输出作为输入，并输出一个贡献值，最终只留下平均贡献值最高的卷积核。即训练阶段结束后，整个模块最终会退化成一个卷积核。</p><p><img src="https://i.loli.net/2020/08/04/JGntQPWVTpEKy2H.png" alt="image.png"></p></li><li><p><strong>Prune Module</strong>：Prune模块负责修剪卷积核中的通道。网络交替进行训练和修剪，在修剪阶段会自动评估每个卷积通道的贡献度，并剔除贡献度低于定值的通道。其贡献度由通道的权重计算得出。如果某个卷积核没有低于定值的通道，则会自动剔除贡献值最低的通道。</p><p><img src="https://i.loli.net/2020/08/04/Us6pSbQlrJyvOE4.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/08/04/snCaumJKZ2qb9Ve.png" alt="image.png"></p></li></ul><p>最后对<strong>鸟瞰图（bird eye view, BEV）</strong>进行输入网络进行分割，输出BEV的heatmap。</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3    Experiments"></a>3    Experiments</h1><p>数据集使用的是<a href="https://cslinzhang.github.io/deepps/" target="_blank" rel="noopener">DeepPS数据集</a>，该数据集中包含 9527(training)+2138(validating) 张BEV图像。作者将训练样本转化为224<em>×</em>224的灰度图作为网络的输入，点线标记作为labels。</p><p>整个训练过程分为三个阶段，分别为<strong>预训练阶段</strong>、<strong>选择阶段</strong>和<strong>修剪阶段</strong>，不同阶段使用不同的损失函数。</p><p><img src="https://i.loli.net/2020/08/05/6zqsPuD5icKgjZx.png" alt="image.png"></p><p>预训练阶段的前5个epoch只用heatmaps的FocalLoss用来预热（warm-up），随后10个epoch使用完整的损失函数；在选择阶段，从第一层到最后一层逐层进行选择，每层选择结束后微调2-3个epoch；在修剪阶段，损失函数再次恢复到只有heatmap的FocalLoss，训练100个epoch的同时进行修剪；最后再去掉两个正则化操作，微调15个epoch。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文：SPFCN: Select and Prune the Fully Convolutional Networks for Real-time Parking Slot Detection&lt;/p&gt;
&lt;p&gt;Github：&lt;a href=&quot;https://github.co
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="全卷积网络" scheme="http://a-kali.github.io/tags/%E5%85%A8%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
  </entry>
  
  <entry>
    <title>VPS-net：基于深度学习的停车位检测</title>
    <link href="http://a-kali.github.io/2020/08/03/VPS-net%EF%BC%9A%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%81%9C%E8%BD%A6%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/08/03/VPS-net：基于深度学习的停车位检测/</id>
    <published>2020-08-03T14:20:27.000Z</published>
    <updated>2020-08-03T15:05:57.491Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视器(AVM)进行空车位检测的精度仍有待提高。为了解决这个问题，本文提出了一种基于深度学习的空车位检测方法，即VPS-Net。VPS-net将空车位检测问题转化为两步问题：车位检测和占用分类。在停车位检测阶段，我们提出了一种基于YOLOv3的检测方法，该方法将停车位的分类与标记点的定位相结合，利用几何线索对各类停车位进行检测标记。在占用分类阶段，我们设计了一个自定义网络，该网络的卷积核大小和层数根据车位的特点进行调整。实验表明，VPS-Net在PS2.0数据集检测空停位任务中可以达到99.63%的精确率和99.31%的召回率。</p><h1 id="2-Method"><a href="#2-Method" class="headerlink" title="2    Method"></a>2    Method</h1><p>VPS-Net基于深度学习来检测各种空车位。如下图所示，VPS-Net可以处理三种典型的停车槽（横向、纵向、倾斜）。一个车位由四个顶点组成，其中两个顶点是入口线的成对标记点，另外两个顶点由于视觉的限制通常在全景图像中不可见。</p><p><img src="https://i.loli.net/2020/08/03/hgDlz7RiSqMwO28.png" alt="image.png"></p><p>下图显示了用于检测空闲停车槽的VPS-Net的基本原理。VPS-Net将空车位检测分为车位检测和占用分类两个步骤，结合了多目标检测网络和分类网络的优点。在停车槽检测阶段，首先使用基于yolov3的检测器同时检测标记点和停车槽头，然后利用几何线索匹配成对的标记点，确定停车槽的方向，最后通过车位的类型、方位和成对标记点推断出两个不可见的顶点，得到完整的车位。检测到停车槽后，将其在图像中的位置转移到占用分类部分，将检测到的停车位规则化为一个统一大小的120x46像素，然后通过一个DCNN来区分它是否是空置的。一旦检测到空车位，将其位置发送给PAS的决策模块进行进一步处理。</p><p><img src="https://i.loli.net/2020/08/03/NVnCFWv5o621eB3.png" alt="image.png"></p><p>车位的类型由车位头决定，车位头包含入口线的成对标点。因此，停车槽头和标记点的检测是停车槽检测的第一步，也是最重要的一步。我们将停车槽头的分类与标记点的定位结合到一个多目标检测问题中，从而可以很容易地根据检测结果推断出各种类型的停车位。为此，我们分别定义了“直角头”、“钝角头”、“尖角头”和“T/L形”四种车位头。</p><ol><li><p>使用yolov3检测出停车<strong>标志点</strong>(marking point)和<strong>车位头</strong>(parking slot heads)；（中心点、宽、高）</p><p><img src="https://i.loli.net/2020/08/03/D4Xy5ZOaG76rbU8.png" alt="image.png"></p></li><li><p>根据检测结果和几何特征推断出停车位的<strong>方向、成对点、车位类型</strong>；</p><ul><li>将包含在同一个车位头框框内的两个标志点归类为成对点(paired marking point)；</li><li>如果一个车位头内只包含了一个或零个标志点，但车位头的置信度足够高，则直接计算出缺失点所在位置；</li><li>如果一个车位头内包含了两个以上的标志点，则选取离对角最近的两个点。</li></ul><p><img src="https://i.loli.net/2020/08/03/69tSpzeVuhCqa8w.png" alt="image.png"></p></li><li><p>根据车位头类型和长宽推断出车位类型（垂直、平行、倾斜）</p><p><img src="https://i.loli.net/2020/08/03/UXGMjQbfJCVDZdE.png" alt="image.png"></p></li><li><p>根据以上特征和车位类型推断出<strong>被遮挡点的位置</strong>;</p></li></ol><p><img src="https://i.loli.net/2020/08/03/K9ns1f3hSlFaXot.png" alt="image.png"></p><p>分类部分比较简单，在此不多赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;由于视觉环境的复杂性，当前公园辅助系统(PAS)采用独立全景监视
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>人脸识别综述——从传统方法到深度学习</title>
    <link href="http://a-kali.github.io/2020/08/02/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0%E2%80%94%E2%80%94%E4%BB%8E%E4%BC%A0%E7%BB%9F%E6%96%B9%E6%B3%95%E5%88%B0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    <id>http://a-kali.github.io/2020/08/02/人脸识别综述——从传统方法到深度学习/</id>
    <published>2020-08-01T16:14:39.000Z</published>
    <updated>2020-08-01T16:20:00.607Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="综述" scheme="http://a-kali.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="人脸识别" scheme="http://a-kali.github.io/tags/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>业务数据分析师入门</title>
    <link href="http://a-kali.github.io/2020/07/17/%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%B8%88%E5%85%A5%E9%97%A8/"/>
    <id>http://a-kali.github.io/2020/07/17/业务数据分析师入门/</id>
    <published>2020-07-17T05:44:12.000Z</published>
    <updated>2020-07-17T05:45:08.303Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-数据分析概念、作用和步骤"><a href="#1-数据分析概念、作用和步骤" class="headerlink" title="1    数据分析概念、作用和步骤"></a>1    数据分析概念、作用和步骤</h1><p><strong>数据分析</strong>是指用适当的统计方法对收集来的大量数据进行分析，将他们加以汇总和理解并消化，以求最大化地开发数据的功能，发挥数据的作用。</p><p>数据分析的<strong>作用</strong>：</p><ul><li>现状分析：企业运营情况、业务发展。通常使用日报、月报来完成。</li><li>原因分析：在现状上展开原因分析，通常使用专题分析来完成。</li><li>预测分析：对企业发展趋势做预测，为企业提供参考和决策依据。</li></ul><p>数据分析的<strong>步骤</strong>：</p><ol><li>明确分析目的和思路：目的、思路、分析框架（如PEST）、体系化；</li><li>数据收集：数据库、出版物、互联网、市场调查；</li><li>数据处理：整理成适合数据分析的形式；</li><li>数据分析：将数据整理分析成结论，数据挖掘是一类高级的数据分析方法；</li><li>数据展现：表格、图像；</li><li>撰写报告：分析、结论、建议或解决方法。</li></ol><h1 id="2-数据分析方法论"><a href="#2-数据分析方法论" class="headerlink" title="2    数据分析方法论"></a>2    数据分析方法论</h1><p>确定分析思路以营销、管理等理论为指导，一般把这些数据分析相关的营销、管理等理论统称为<strong>数据分析方法论</strong>。</p><h2 id="PEST分析法"><a href="#PEST分析法" class="headerlink" title="PEST分析法"></a>PEST分析法</h2><p><a href="https://baike.baidu.com/item/PEST/10805117?fr=aladdin" target="_blank" rel="noopener">PEST</a>是指对<strong>政治（Political）、经济（Economic）、技术（Technological）和社会（Social）</strong>这四类影响企业的主要外部环境因素进行分析。</p><ul><li>政治：新政策，经济政策，市场道德标准，文化宗教等；</li><li>经济：社会经济结构，经济发展水平，经济体制，经济状况等；</li><li>社会：人口，流动性，消费心理，生活方式变化，价值观等；</li><li>技术：新技术的发明、传播等。</li></ul><h2 id="5W2H分析法"><a href="#5W2H分析法" class="headerlink" title="5W2H分析法"></a>5W2H分析法</h2><p>5W2H分析法是从回答中发现解决问题的线索的方法，广泛应用于企业营销管理活动等方面。</p><p><img src="https://i.loli.net/2020/07/12/mUvrcf5yDjSgQYL.png" alt></p><h2 id="逻辑树分析法"><a href="#逻辑树分析法" class="headerlink" title="逻辑树分析法"></a>逻辑树分析法</h2><p>逻辑树分析法是将一个已知问题当成树干，然后考虑这个问题和哪些问题相关。每想到一个问题就可以在所在树干下加一个树枝，并表明树枝代表什么问题。以此来将问题逐步分解。</p><p><img src="https://i.loli.net/2020/07/12/8hiW9d2t4DN6pPg.png" alt="image.png"></p><h2 id="4P营销理论"><a href="#4P营销理论" class="headerlink" title="4P营销理论"></a>4P营销理论</h2><p>4P营销理论将营销要素概括为如下四类：<strong>产品（Product）、价格（Price）、促销（Promotion）、渠道（Place）</strong>。如果要了解公司的整体运营情况，就可以采用4P营销理论进行分析指导。</p><p><img src="https://i.loli.net/2020/07/12/rHb25QgRoEukc13.png" alt="image.png"></p><h2 id="用户使用行为理论"><a href="#用户使用行为理论" class="headerlink" title="用户使用行为理论"></a>用户使用行为理论</h2><p> 用户使用行为是指用户行为获取、使用物品或服务所采取的各种行动，一般按照以下过程：<strong>认知产品、熟悉、试用、使用、成为忠实用户</strong>。</p><p><img src="https://i.loli.net/2020/07/12/KUDOub7vPJ5ZSdh.png" alt="image.png"></p><h1 id="3-常用数据分析方法"><a href="#3-常用数据分析方法" class="headerlink" title="3    常用数据分析方法"></a>3    常用数据分析方法</h1><h2 id="对比分析法"><a href="#对比分析法" class="headerlink" title="对比分析法"></a>对比分析法</h2><p>将两个或两个以上的数据进行比较，分析其中差异。包括与目标值对比、不同时期对比、同级别对比、行业内对比、活动效果对比。</p><h2 id="分组分析法"><a href="#分组分析法" class="headerlink" title="分组分析法"></a>分组分析法</h2><p><img src="https://i.loli.net/2020/07/12/m6SlcrROhxHqi71.png" alt="image.png"></p><h2 id="结构分析法"><a href="#结构分析法" class="headerlink" title="结构分析法"></a>结构分析法</h2><p><img src="https://i.loli.net/2020/07/12/ZQfzjATtUWeupEO.png" alt="image.png"></p><h2 id="平均分析法"><a href="#平均分析法" class="headerlink" title="平均分析法"></a>平均分析法</h2><p><img src="https://i.loli.net/2020/07/12/YFLxZ91EAJrWcf5.png" alt="image.png"></p><h2 id="杜邦分析法"><a href="#杜邦分析法" class="headerlink" title="杜邦分析法"></a>杜邦分析法</h2><p>杜邦分析法是利用主要<strong>财务指标</strong>之间的内在联系，对企业财务状况以及经济效益进行综合分析评价的方法。</p><p>![image-20200712161837746](C:\Users\STEVE JOBS\AppData\Roaming\Typora\typora-user-images\image-20200712161837746.png)</p><h2 id="漏斗图分析法"><a href="#漏斗图分析法" class="headerlink" title="漏斗图分析法"></a>漏斗图分析法</h2><p>漏斗图是一个适合业务流程比较规范、周期比较长、各流程环节涉及复杂业务比较多的管理分析工具。漏斗图能很快在复杂的环节中找出主要因素。</p><p><img src="https://i.loli.net/2020/07/12/NXEUSLAfxFCyhr2.png" alt="image.png"></p><h1 id="4-数据图表讲解"><a href="#4-数据图表讲解" class="headerlink" title="4    数据图表讲解"></a>4    数据图表讲解</h1><p><strong>选择恰当的图表（重要）：</strong></p><p><img src="https://i.loli.net/2020/07/12/thQ7YKC2PgEkpRA.png" alt="image.png"></p><h2 id="展现数据的表格技巧"><a href="#展现数据的表格技巧" class="headerlink" title="展现数据的表格技巧"></a>展现数据的表格技巧</h2><ol><li>突出显示单元格：对满足条件的单元格突出显示。（Excel：开始-样式-条件格式-突出显示单元格规则）</li><li>项目选取： 同上。（Excel：开始-样式-条件格式-项目选取规则）</li><li>数据条： 在表格单元格中显示柱状图，直观显示数值大小。（Excel：开始-样式-条件格式-数据条）</li><li>图标集：对数值分类，不同类别数值显示不同图标。（Excel：开始-样式-条件格式-图标样式）</li></ol><p>效果图：</p><p><img src="https://i.loli.net/2020/07/12/yQDGZrlNw2uRk8P.png" alt="image.png"></p><h2 id="各种图表展示"><a href="#各种图表展示" class="headerlink" title="各种图表展示"></a>各种图表展示</h2><p><img src="https://i.loli.net/2020/07/12/WCUdyFXJHxpl2Sw.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/YgKNH75vTlp9UeQ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/lPu4S6imsHEaY3G.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/HeV5PzsAmir2gpR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/9DWL2VoxKpZwJsR.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/cUNn8bHgLjXBJhp.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/TieQ2JuEaODytkg.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/1MjT3GmWYsHeDtl.png" alt="image.png"></p><h1 id="5-数据关键指标"><a href="#5-数据关键指标" class="headerlink" title="5    数据关键指标"></a>5    数据关键指标</h1><h2 id="5-1-产品经理常用数据指标（KPI）"><a href="#5-1-产品经理常用数据指标（KPI）" class="headerlink" title="5.1    产品经理常用数据指标（KPI）"></a>5.1    产品经理常用数据指标（KPI）</h2><p><img src="https://i.loli.net/2020/07/12/kO5VWU74otcK2Fj.png" alt="image.png"></p><h2 id="5-2-电商类网站分析指标"><a href="#5-2-电商类网站分析指标" class="headerlink" title="5.2    电商类网站分析指标"></a>5.2    电商类网站分析指标</h2><p><img src="https://i.loli.net/2020/07/12/WoRqaPCTliugHfI.png" alt="image.png"></p><h3 id="5-2-1-内容指标"><a href="#5-2-1-内容指标" class="headerlink" title="5.2.1    内容指标"></a>5.2.1    内容指标</h3><p><strong>转化率</strong>：转化率 = 进行了相应动作的访问量 / 总访问量。用于衡量网站内容对访问者的吸引程度和网站的宣传效果。</p><p><strong>回访者比率</strong>：转化率 = 回访者数 / 独立访问者数。衡量网站内容对访问者的吸引程度和网站实用性，你的网站是否有令人感兴趣的内容使访问者再次访问你的网站。</p><ul><li>积极回访者比率：积极回访者比率 = 访问超过11页的用户 / 总访问数。衡量有多少访问者度网站内容高度感兴趣。</li><li>忠实访问者比率：忠实访问者比率 = 访问时间在xx分钟以上的用户 / 总访问数。</li></ul><p><strong>忠实访问者指数</strong>：忠实访问者指数 = 大于xx分钟的访问页数 / 大于xx分钟的访问者数。衡量“忠实访问者”是否真的在浏览页面。</p><p>忠实访问者量：忠实访问者量 = 大于xx分钟的访问页数 / 总访问页数。表现该网站是否吸引了错误的访问者。</p><p>访问者参与指数：访问者参与指数 = 总访问数 / 独立访问者数。</p><p><strong>回弹率</strong>：回弹率 = 只访问了一页的访问数 / 总访问数。表示访问者只看到了一页的比率。如果这个指标高则表明可能网页布局需要优化。</p><p>浏览用户相关：</p><ul><li>浏览用户比率：少于1分钟的访问者数 / 总访问数。</li><li>浏览用户指数：少于1分钟的访问页数 / 少于1分钟的访问者数。</li><li>浏览用户量：少于1分钟的浏览页数 / 所有浏览页数。</li></ul><h3 id="5-2-2-商业指标"><a href="#5-2-2-商业指标" class="headerlink" title="5.2.2    商业指标"></a>5.2.2    商业指标</h3><p><img src="https://i.loli.net/2020/07/12/IBUNDniwFX2gm59.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/tGmynaOofjDg14A.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/07/12/X4LSfPsdbxJ1EzB.png" alt="image.png"></p><h3 id="5-2-3-其它指标"><a href="#5-2-3-其它指标" class="headerlink" title="5.2.3    其它指标"></a>5.2.3    其它指标</h3><p><img src="https://i.loli.net/2020/07/12/rsoViW5gzMTZwqx.png" alt="image.png"></p><h1 id="6-数据分析报告"><a href="#6-数据分析报告" class="headerlink" title="6    数据分析报告"></a>6    数据分析报告</h1><p><strong>数据分析报告</strong>是根据数据分析方法，运用数据来反映某项事物的结论，并提出解决办法的分析应用文体。</p><p>数据分析报告的<strong>原则</strong>：</p><ul><li>规范性：术语规范；</li><li>重要性：选取重要的、关键的指标；</li><li>谨慎性：真实、严谨、实事求是；</li><li>创新性：特征挖掘。</li></ul><p>数据分析报告的<strong>作用</strong>：展示分析结果、验证分析质量、提供决策参考。</p><p>数据分析报告的<strong>种类</strong>：专题分析报告（单一性、深入性）、综合分析报告（全面性、联系性）、日常数据通报（进度性、规范性、时效性）。</p><p><strong>数据分析报告的构成</strong>：</p><ul><li>标题页：简洁的标题；</li><li>目录；</li><li>前言：分析背景、分析目的、分析思路；</li><li>正文：通过图表和文字系统全面地表述分析的过程和结果，表明所有数据分析事实和观点，各部分之间具有逻辑关系。</li><li>结论和建议；</li><li>附录：名词解释、计算方法、原始数据等。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-数据分析概念、作用和步骤&quot;&gt;&lt;a href=&quot;#1-数据分析概念、作用和步骤&quot; class=&quot;headerlink&quot; title=&quot;1    数据分析概念、作用和步骤&quot;&gt;&lt;/a&gt;1    数据分析概念、作用和步骤&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;数据分析&lt;/st
      
    
    </summary>
    
      <category term="数据分析" scheme="http://a-kali.github.io/categories/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
    
      <category term="数据分析" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>证券投资概述</title>
    <link href="http://a-kali.github.io/2020/05/04/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/05/04/证券投资概述/</id>
    <published>2020-05-04T05:14:24.000Z</published>
    <updated>2020-05-04T05:15:28.716Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-证券家族四兄弟"><a href="#1-证券家族四兄弟" class="headerlink" title="1    证券家族四兄弟"></a>1    证券家族四兄弟</h1><h2 id="1-1-债券"><a href="#1-1-债券" class="headerlink" title="1.1    债券"></a>1.1    债券</h2><p><strong>定义：政府和企业借债，向投资人开具的借条。规定了期限，还本付息。</strong></p><p>现代债券出现于十二世纪的意大利，国家<strong>将巨额债务拆分成标准化的小块出售，向百姓筹集资金，并定期还本付息。</strong></p><p>债券的种类有上百种，但可以进行以下几种归类。债券根据期限可以分为<strong>短期债券、中期债券、长期债券</strong>，划分界限为一年和十年。通常来说债券偿还期越长风险越大，收益也越高。</p><p>债券根据发行人的不同可以分为<strong>政府债券、金融债券、企业债券</strong>。政府债券是信用最好、收益最稳定、风险最小的债券，由政府的权力进行保障。政府债券可分为<strong>中央政府债券</strong>和<strong>地方政府债券</strong>，中央政府债券也被称为<strong>金边债券</strong>。金融债券的风险和收益通常高于政府债券，金融企业通常包括商业银行、证券公司、保险公司。这些企业资金雄厚、实力强大。在中国这类企业通常有国资背景。企业债券为在证券交易所上市的企业发行的债券，风险较大。</p><p>债券的风险小，收益较低。在股市熊市的时候可以作为一种不错的投资选项。<strong>债券基金</strong>相比于债券流动性更强，购买三天后即可赎回。</p><p>债券按照信用等级可以被分为<strong>垃圾债</strong>和<strong>投资债</strong>。</p><h2 id="1-2-股票"><a href="#1-2-股票" class="headerlink" title="1.2    股票"></a>1.2    股票</h2><p><strong>定义：企业向投资人发行的共担风险、共享收益的所有权凭证。</strong></p><p>普通股（A股）的三大性质：</p><ol><li><strong>有限责任</strong>：投资人所需承担的风险取决于投资的金额。</li><li><strong>分红</strong>：按照股份分红，企业挣的越多分红越多。</li><li><strong>投票权</strong>：股东享有企业决策的投票权，股份越多权重越大。</li></ol><p>优先股：优先比普通股的股东分红，但股东不能参与公司决策。</p><p>决策权股（B股）：企业授权给管理层的股票，其决策权重高于普通股。</p><p>股票收益来源于两个方面：<strong>分红</strong>和<strong>资本利得</strong>。分红为从企业收益中分享给投资者的红利；资本利得为股票买卖的差价。</p><h2 id="1-3-基金"><a href="#1-3-基金" class="headerlink" title="1.3    基金"></a>1.3    基金</h2><p><strong>定义：基金是代人理财的机构。代人投资证券的机构为证券投资基金。</strong></p><p>特征：</p><ol><li>共担风险，共享收益。</li><li>专家理财：将散户的钱统一交给专家投资。</li><li>分散风险：一个基金可以投资上百个股票。</li></ol><p>按照投资对象对基金分类：</p><ol><li><strong>债券基金</strong></li><li><strong>股票基金</strong></li><li><strong>混合基金</strong>：既投资债券又投资股票的基金。</li><li><strong>货币基金</strong>：投资货币性金融资产（即一年期以内的金融资产，流动性强，风险小）的基金。余额宝是一种货币基金。</li></ol><p>在股市熊市的时候可购买货币基金，在牛市的时候如果不知道该买哪只股票可全仓股票基金和混合基金。</p><p>按照基金风格分类：</p><ol><li><strong>主动型基金</strong>：只挑选少量股票且不断换股，以更高收益为目的。对基金经理要求高，交易手续费较高，为1.5%。</li><li><strong>被动型基金</strong>：挑选大量股票，分散风险，不换股。手续费低，为0.5%。其中以指数基金最为典型。指数基金是以某特定指数（比如沪深300）的成分股为投资对象的被动型基金。在证券市场可直接交易的指数基金称为<strong>ETF</strong>。</li></ol><p>大部分主动型基金跑不过指数基金。</p><p>基金还可以被分为<strong>私募基金（对冲基金）</strong>和<strong>公募基金（共同基金）</strong>。公募基金在公共场合公开募集资金，而私募基金在公开场合是买不到的。公募基金门槛低，通常散户就能投资；而私募基金则对投资金额、投资人财产情况、收入等有要求。公募基金通常收到政府和企业的严格监管，有严格的限制和流程。公募基金经理收入来源于手续费，私募基金经理来源于投资收入的一部分（投资负收益则没有收入）。私募基金收到的监管少，通常良莠不齐。</p><p>万金油投资——指数基金定投。门槛低，成本低，避免追涨杀跌。</p><p>指数基金可分为<strong>宽基指数基金</strong>和<strong>行业指数基金</strong>。宽基即面向各行各业（如沪深300），行业指数基金则对应单一行业（如白酒ETF）。指数基金还可以被分为<strong>场内基金</strong>和<strong>场外基金</strong>。场内基金在证券交易所和股票软件购买，可以像股票一样实时交易；场外基金可以在银行、支付宝、股票软件购买，不能实时交易。</p><p>比较知名的ETF：</p><ol><li>沪深300：从上海和深圳证券市场中选取最优的300只大型企业A股作为样本编制而成的成份股指数,原则上每半年调整一次样本股。</li><li>中证500：排在沪深300之后的500只中小企业A股。</li><li>上证50：上海证券市场前50，包括茅台、中国平安和招商银行等。</li><li>上证指数：上海证券综合指数，这个指数的样本是在上海证券交易所全部上市股票，这里包括A股和B股。（2600-2700低买，2900-3000高卖）</li><li>创业50：创业板前50。</li><li>创业指数：创业板前100。 </li></ol><h2 id="1-4-衍生工具"><a href="#1-4-衍生工具" class="headerlink" title="1.4    衍生工具"></a>1.4    衍生工具</h2><p><strong>定义：在股票、债券、 商品、货币的买卖合同上加上新的买卖合同，就是衍生工具。主要包括远期合约、期货、期权、互换。</strong></p><p><strong>远期合约</strong>：易双方约定在未来的某一确定时间，以确定的价格买卖一定数量的某种金融资产的合约。远期合约通常是大合同，由于未来的不确定性，远期合约有较高的被毁约的风险。</p><p><strong>期货</strong>：以某种大众产品如棉花、大豆、石油等及金融资产如股票、债券等为标的标准化可交易合约。具有多次交易（保证金+实际金额）、杠杆、高收益高风险等特性。</p><p><strong>期权</strong>：合约赋予持有人在某一特定日期或该日之前的任何时间以固定价格购进或售出一种资产的权利。需要支付权利金购买。</p><p><strong>互换</strong>：互换两个或两个以上的当事人按照共同商定的条件，在约定的时间内定期交换现金流的金融交易，可分为货币互换、利率互换、股权互换、信用互换等类别。如人民币换美元。</p><p>期货的交易为跨期交易，共分两次交易。第一次交易叫做<strong>开仓</strong>，只需交保证金；第二次交易叫<strong>平仓</strong>，需支付全额。由于期货第一次交易只需要交少量保证金，以小博大，所以期货是加杠杆的。认为货物涨价，先买后卖的订单叫<strong>多单</strong>；认为货物降价，先卖后买的订单叫<strong>空单</strong>。空单时可以向期货交易所借货卖出，并在一定期限后买回归还。</p><p>期货的意义：</p><ol><li>对企业来说可以锁住风险，安心经营。</li><li>期货能直观反映一个货物的涨跌。</li><li>具有国家战略意义（比如可以用人民币结算石油期货）。</li></ol><h2 id="加餐：证券玩家、投资者与投机者"><a href="#加餐：证券玩家、投资者与投机者" class="headerlink" title="加餐：证券玩家、投资者与投机者"></a>加餐：证券玩家、投资者与投机者</h2><p>证券市场上存在三种人：证券玩家、投资者和投机者。</p><p>证券玩家对市场上的风吹草动十分敏感，每天都在追涨杀跌做短线。这类人通常挣不到钱。</p><p>投资者通常研究优质企业，进行长线投资。</p><p>投机者介于投资者和证券玩家之间，关心大趋势。比如贸易战、加息、汇率变动等。</p><h1 id="2-证券市场"><a href="#2-证券市场" class="headerlink" title="2    证券市场"></a>2    证券市场</h1><p>证券市场根据职能可以分为<strong>证券发行市场</strong>和<strong>证券交易市场</strong>，根据交易点可分为<strong>场内市场</strong>和<strong>场外市场</strong>，场内即证券交易所内，场外通常是在商业银行柜台进行交易。证券市场根据服务对象可分为<strong>一板、二板、三板</strong>。一板通常给大型企业上市，二板是创业板，三板是场外市场，被称为<strong>股权代办交易系统</strong>，证券公司在三板代办不能在主板上市的公司股票。</p><h2 id="2-1-证券发行市场"><a href="#2-1-证券发行市场" class="headerlink" title="2.1    证券发行市场"></a>2.1    证券发行市场</h2><p>证券发行的方式包括<strong>私募</strong>和<strong>公募</strong>。私募为私下发行证券，向特定投资人发行新证券；公募在市场上公开募集，需要向市场和社会公开信息，比如发行人的经济状况、证券的特点等。股票发行需要公开<strong>招股说明书</strong>，内容包括企业的经营状况、主营业务、未来收入等。</p><p>证券发行还可以被分为<strong>直接发行</strong>和<strong>间接发行</strong>。直接发行是直接将证券发给投资人，间接发行是将证券通过<strong>证券公司</strong>（又称<strong>券商</strong>、<strong>投行</strong>）发行。间接发行分为<strong>代销</strong>和<strong>包销</strong>。代销时，证券公司仅作为销售平台，不负责任；包销时，证券公司收购所有证券并加手续费卖出，卖不出去的证券只能证券公司自己留着。</p><p>发行定价的方式：</p><ol><li><strong>市盈率(PER)定价</strong>：市盈率指股票价格除以每股盈利的比率。成熟的市场国家市盈率通常为15倍，我国通常按照20倍市盈率定价。</li><li><strong>净资产定价</strong>：净资产指指企业的资产总额减去负债以后的净额。股票的价格按照每股净资产的一定倍数定价。</li><li><strong>竞价</strong>。</li></ol><p>在我国主要是市盈率定价。</p><h3 id="注册制"><a href="#注册制" class="headerlink" title="注册制"></a>注册制</h3><p>注册制由<strong>核准制</strong>发展而来。核准制下监管部门对上市企业制定相应标准，并对符合标准的企业进行实质性审核；而注册制下监管部门制定严格的标准并审核相关材料，剩下的由证券交易所和证券公司进行安排。注册制启用后，上市企业将变得远多于核准制。注册制时成熟的市场经济国家的证券市场常规的办法。注册制目前在我国创业板试点。</p><p>注册制的影响：</p><ol><li>新股不会受到严格的审查监管，股民对炒新股会更加谨慎。</li><li>大浪淘沙，留住好企业。市场的入口和出口都会放得更开，仙股（股价低于1元）更容易退市。</li><li>注册制将留住好企业，引导市场变得成熟，牛市更长熊市更短。</li></ol><h3 id="加餐：上市公司割韭菜的套路"><a href="#加餐：上市公司割韭菜的套路" class="headerlink" title="加餐：上市公司割韭菜的套路"></a>加餐：上市公司割韭菜的套路</h3><ol><li>假装回购（回购：上市公司将股票的一部分买回来注销，减少股票量来抬高股价），实际上并没有回购或者回购数量没有预定的那么多，吸引散户抬高股价。</li><li>无意义的增发股票圈钱，但并没有用获得的钱做利于公司发展的事。</li><li>不分红。</li></ol><h2 id="2-2-证券交易市场"><a href="#2-2-证券交易市场" class="headerlink" title="2.2    证券交易市场"></a>2.2    证券交易市场</h2><p><strong>证券交易所</strong>是以拍卖形式交易旧证券的场所。证券交易所里有两类人：<strong>内部管理人员</strong>和<strong>交易所会员</strong>。交易所会员的注册要求非常高，上海证券交易所的注册资本为500万元。交易所会员有两类人：<strong>经纪人</strong>和<strong>交易商</strong>。经纪人即是证券公司，普通人通过证券公司进行投资；交易商是有大型证券订单的大公司。</p><p>证券市场交易时间为每周一到周五上午时段9:30-11:30，下午时段13:00-15:00。周六、周日及上海证券交易所、深圳证券交易所公告的休市日不交易。由于我国市场太不成熟，所以股票交易由原先的T+0制度改为T+1制度，并设定了涨跌停板。</p><p>在上市公司股票价格出现重大波动，上市公司有重大决策时，为了引起投资者注意，停止股票交易，称为<strong>停牌</strong>；而<strong>摘牌</strong>指将不符合条件的上市公司踢出证券市场。</p><p>我国为了一定程度上维护投资者利益，实施了<strong>ST(Special Treatment)制度</strong>。当一个企业变成仙股，就会被扣上ST的帽子。如果企业继续下行，ST会变成*ST，临近退市边缘。ST制度会让部分垃圾企业成为巨婴，如今我国正严格退市制度，将垃圾企业清理出市。</p><h1 id="3-证券基本分析"><a href="#3-证券基本分析" class="headerlink" title="3    证券基本分析"></a>3    证券基本分析</h1><h2 id="3-1-宏观环境"><a href="#3-1-宏观环境" class="headerlink" title="3.1    宏观环境"></a>3.1    宏观环境</h2><p>宏观环境主要包括三个方向：<strong>社会大环境、大政方针、经济周期与反周期</strong>。</p><p>社会大环境：人口老龄化（医疗）、人民收入提高（消费）等。</p><p>大政方针：中国制造2025、一带一路、雄安新区等。</p><p>经济周期：人们在预料到经济周期前通常会对股票进行买入或卖出，所以股市通常是经济周期的先行指标。部分行业不受经济周期影响，比如食品行业、烟草行业、医疗行业等；部分行业受经济周期影响较大，比如钢铁、冶金、建材行业。</p><p>反周期的政策：经济周期处于萧条期时不利于经济发展，此时政府会推出反周期的政策来刺激经济。</p><h2 id="3-2-行业对股市的影响"><a href="#3-2-行业对股市的影响" class="headerlink" title="3.2    行业对股市的影响"></a>3.2    行业对股市的影响</h2><p>行业：指一组提供同类相互密切替代商品或服务的公司。</p><p>判断一个行业的时候一定要知道其挣钱模式，即其面向的客户、出售的产品等。</p><p>行业和人一样有从幼年、青年、中年到老年的生命周期。幼年期的企业处于创业期，以亏损为主；青年期的企业竞争对手多，风险大；中年企业产品定型、市场成熟，可以作为主要投资方向。</p><p>周期股与防御股：珠宝、建材、钢铁、有色金属受经济影响大，建议在牛市时作为主要买入对象；食品、烟酒、医药、调料等受经济影响小，可以在熊市的时候投资。</p><h2 id="3-3-企业对股市的影响"><a href="#3-3-企业对股市的影响" class="headerlink" title="3.3    企业对股市的影响"></a>3.3    企业对股市的影响</h2><p><strong>得天独厚的自然条件、品牌技术专利、研发投入、顾客的转化成本</strong>是企业无法被替代的重要条件。举例：</p><ul><li>茅台在赤水河旁边，所处气候适合酿造高品质的白酒；</li><li>茅台有很多酱香酒的专利，无法被模仿；</li><li>华为麒麟980芯片研发投入3亿美元，令普通公司望而却步;</li><li>微软和苹果的PC端操作系统用户、腾讯QQ和微信用户转化成本非常高；</li><li>滴滴网约车、摩拜共享单车的模式门槛低，容易产生竞争对手。</li></ul><h2 id="加餐：粗看股票"><a href="#加餐：粗看股票" class="headerlink" title="加餐：粗看股票"></a>加餐：粗看股票</h2><ol><li>看F10中的<strong>每股收益、市盈率、企业主营业务、竞争对手、竞争对手的股价</strong>。其中以市盈率为主。如果是高成长的行业，30-40倍的市盈率是可以接收的；如果是成熟行业，市盈率通常在15倍以下为佳；如果是衰退行业，市盈率尽量选择在10倍以下。（股票中的F10的意思是指在分析软件中按键盘上的F10可以查看该公司基本面的文本资料，指向该股的基本公开信息，包括股本、股东、财务数据、公司概况和沿革、公司公告、媒体信息等等，都可快速查到。）</li><li>看<strong>毛利率</strong>（销售利润/销售收入）：毛利率＞20%为良，＞40%为优；</li><li>看<strong>净资产收益率</strong>（(资产-负债)/收益）：净资产＞15%为优。</li></ol><p>这三个指标不能只看某一年的数据，某一年的数据不能作为常态，应当看近几年的数据。</p><p>K线图可以看是否处于历史低位或者单日U型、V型底部。</p><h1 id="4-证券技术分析"><a href="#4-证券技术分析" class="headerlink" title="4    证券技术分析"></a>4    证券技术分析</h1><h2 id="4-1-图形识别"><a href="#4-1-图形识别" class="headerlink" title="4.1    图形识别"></a>4.1    图形识别</h2><p><strong>压力（阻力）与支撑</strong>：支撑位是指在股价下跌时遇到买方大量买入，从而止跌回稳的价位；其对应的是阻力位。</p><p>压力位与支撑位的转换：压力位意味着股价上方有一群套牢盘，许多人之前在该点位买入并随着股价下跌而亏损，想在该点位卖出。如果股价突破了压力位继续上涨，在压力位卖出的人又会后悔，想在该点买入，此时压力位就转换成了支撑位。支撑位同理。</p><p>当股价多次试图突破压力位时没成功时，就会在压力位套牢一大批人，留下多个压力位，股价转跌。支撑位同理。</p><p><img src="https://i.loli.net/2020/05/03/nV26gKahL9QOcUP.png" alt="ISBW11D8MS_@P29_PR_QST1.png"></p><p><strong>趋势线</strong>就是由压力点或支撑点组成的呈上涨/下跌/水平趋势的<strong>上下轨线</strong>，通过趋势线可以看出股价的趋势；当趋势不明显或者水平的时候，趋近上轨时卖出，趋近下轨时买入。</p><p><img src="https://i.loli.net/2020/05/03/b8owYM2zQTdnDSP.png" alt="DU5`FA_HCC80SGLI16_D__3.png"></p><p>根据压力和支撑理论，可以衍生出头肩底、W底、圆弧底等图形理论，在此不多赘述。</p><p><strong>缺口理论</strong>：</p><p><img src="https://i.loli.net/2020/05/03/cR98qnYl2w4iQaj.png" alt="V@N_B9_P_UL0MLZ_O2UB4_V.png"></p><h2 id="加餐：蜡烛图与K线形态组合"><a href="#加餐：蜡烛图与K线形态组合" class="headerlink" title="加餐：蜡烛图与K线形态组合"></a>加餐：蜡烛图与K线形态组合</h2><p>蜡烛图：</p><p><img src="https://i.loli.net/2020/05/04/75FuKJWsMceBUCf.png" alt="_`8Y2IE_MQA_8QNJE_FKGXF.png"></p><p>连续行情+同价线=行情结束</p><p>连续行情+同价线+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/gKlEu63dLDshIbW.png" alt="5X__V`G@_RN1@ZE3V_JC_C5.png"></p><p>连续行情+反向K线=行情反转</p><p><img src="https://i.loli.net/2020/05/04/BAm2JskVGLyQh4e.png" alt="_KI4__0FV8A2M_S4_`PEQPS.png"></p><p><img src="https://i.loli.net/2020/05/04/t8AsOuDdEZ5aTpM.png" alt="DM0KUNY_9SN65H_V__~_`PL.png"></p><p>成交密集区的跳空缺口=横盘结束</p><p><img src="https://i.loli.net/2020/05/04/xiguQbB4oXSypm1.png" alt="7_MJWHJ___N_@1_1`R_XLMF.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-证券家族四兄弟&quot;&gt;&lt;a href=&quot;#1-证券家族四兄弟&quot; class=&quot;headerlink&quot; title=&quot;1    证券家族四兄弟&quot;&gt;&lt;/a&gt;1    证券家族四兄弟&lt;/h1&gt;&lt;h2 id=&quot;1-1-债券&quot;&gt;&lt;a href=&quot;#1-1-债券&quot; class
      
    
    </summary>
    
      <category term="经济与金融" scheme="http://a-kali.github.io/categories/%E7%BB%8F%E6%B5%8E%E4%B8%8E%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="证券投资" scheme="http://a-kali.github.io/tags/%E8%AF%81%E5%88%B8%E6%8A%95%E8%B5%84/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：使用RNN进行情感识别</title>
    <link href="http://a-kali.github.io/2020/03/03/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%9A%E4%BD%BF%E7%94%A8RNN%E8%BF%9B%E8%A1%8C%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    <id>http://a-kali.github.io/2020/03/03/论文笔记：使用RNN进行情感识别/</id>
    <published>2020-03-03T08:31:11.000Z</published>
    <updated>2020-03-03T09:22:57.794Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1701.08071" target="_blank" rel="noopener">Emotion Recognition From Speech With Recurrent Neural Networks</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>在<strong>自动语音识别(Automated Speech Recognition, ASR)</strong>任务中，语音被模型转化成文字。但是在人们的对话过程中除了文本以外还有其它重要的信息，比如<strong>语调、情感、响度</strong>。这些信息在语音理解中亦扮演者十分重要的角色。本文将围绕其中“情感”部分进行概述，即<strong>语音情感识别(Speech Emotion Recognition, SER)</strong>。</p><p>在语音情感识别方面存在一些难点：</p><ol><li><strong>情感是主观的</strong>，不同人对于同一段语音，理解出的情感可能不同。</li><li><strong>同一段语音可能包含多种情感</strong>。（可以通过<a href="https://zhuanlan.zhihu.com/p/86745594" target="_blank" rel="noopener">CTC损失函数</a>解决）</li><li><strong>数据来源</strong>：从电影中截取的语音可能和现实中存在偏差。通常会找专业演员来演绎各种情感来制造数据。</li></ol><h1 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2    Related works"></a>2    Related works</h1><p>大部分文献将语音情感识别视为一个分类问题，对每一个utterance分配一个label。utterance即为一小段语音，是语音的最小单元。</p><p>在深度学习之前，大多研究提取底层的手工特征，用传统分类器进行分类，比如HMM（隐马尔可夫模型）或GMM（高斯混合模型）。</p><p>深度学习出现后，有人把utterance分帧计算低层特征，用三层全连接层，对输出概率聚合成utterance水平的特征（用简单的统计量，比如最大值，最小值，平均值等），最后用ELM（Extreme Learning Machine）分类。</p><p>后面出现了纯深度学习和端到端的架构模型。有人使用Attention CNN，有人用DBN，还有人用迁移学习把语音识别的任务（数据集）迁移到语音情感识别中。</p><h1 id="3-Data-and-preprocessing"><a href="#3-Data-and-preprocessing" class="headerlink" title="3    Data and preprocessing"></a>3    Data and preprocessing</h1><p>IEMOCAP（Interactive Emotional Dyadic Motion Capture）被选作数据集，因为它有详尽的获取方法，免费的学术许可，较长的语音时长和良好的标注。</p><p>大约包括12个小时，含有视频，音频和人脸关键点的数据。由南加利福尼亚大学戏剧系的10位专业演员表演所得。评估者对每个utterance给出评价（10个情感选项），当一半以上的评估者对某个utterance的评估一致时，该utterance才分配到评估的感情。本文中选取其中4种情感用于分析（生气，兴奋，中立和伤心），只有这些样本才被考虑到本文工作中，下图是标签分布。</p><p><img src="https://img2018.cnblogs.com/blog/1160281/201811/1160281-20181113191458804-356625669.png" alt="img"></p><p>原始信号的采样率是16kHz，直接使用计算量很大，需要尽量保持信息的同时减小计算量。本文对utterance进行分帧，帧长为200ms，帧移为100ms，在帧上计算声学特征（用了哪些声学特征见下文介绍），然后把这些特征合在一起作为utterance的特征输入到模型。关于帧长的选取，论文从30ms到200ms都做过实验发现效果差别不大，而较长的帧可以导致比较少的帧，能减小计算量，所以使用了200ms。</p><p>对于语音信号的特征主要有三种，一是声学特征，也就是声波的一些属性；二是音律特征，指的是停用词，韵律（押韵，平仄），响度，这个特征依赖于说话人，所以没有用这类特征；三是语义学特征，就是语音对应的文字内容的信息。</p><p>本文只使用了声学特征，使用的是python库PyAudioAnalysis的API提供的34个特征，主要包括3个时域特征（过零率，能量，能量熵），5个谱特征，13个MFCC特征，13个音阶特征。也就是一帧的声音用34维的向量来表示。</p><h1 id="4-Approach"><a href="#4-Approach" class="headerlink" title="4    Approach"></a>4    Approach</h1><p>因为一个utterance只对应一个标签，但是有很多帧，有些帧是不包含情感的，所以输入序列和输出序列难以一一对应，为了应对这个问题，可以使用CTC（Connectionist Temporal Classification）的方法。</p><p>CTC模型中的LSTM的输入时间步和输出时间步T为78，因为每个语音样本划分成了78帧。情感标签有4个，加上空白符，得到大小为5的字符集合。真实输出只有一个标签，所以在这些长度为78的输出序列中，经过B转换后能得到一个真实情感标签的那些序列才是我们要的序列，用CTC的方法来使得这些序列产生的概率最大。</p><p>注：文本大量参考这篇文章<a href="https://www.cnblogs.com/liaohuiqiang/p/9954088.html" target="_blank" rel="noopener">论文笔记：Emotion Recognition From Speech With Recurrent Neural Networks</a>，以自己的语言整理一遍笔记，旨在巩固记忆加深理解，并非完全原创。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1701.08071&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Emotion Recognition From Speech With Recurrent Neural Netw
      
    
    </summary>
    
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="RNN" scheme="http://a-kali.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>DAIC-WOZ抑郁评估数据格式</title>
    <link href="http://a-kali.github.io/2020/02/09/DAIC-WOZ%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F/"/>
    <id>http://a-kali.github.io/2020/02/09/DAIC-WOZ抑郁评估数据格式/</id>
    <published>2020-02-09T14:52:27.000Z</published>
    <updated>2020-02-11T13:02:26.459Z</updated>
    
    <content type="html"><![CDATA[<p>DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的数据。该代理能够自动对人们进行访谈，并在语言(verbal)和非语言(nonverbal)指标上识别精神疾病。收集的数据包括音频和视频记录以及大量的的问卷回答；这部分语料库包括一个名为Ellie的动画虚拟面试官主持的Oz访谈，由另一个房间里的真人面试官控制。数据已被转录和注释的各种语言的和非语言的特征。</p><h1 id="Data-description"><a href="#Data-description" class="headerlink" title="Data description"></a>Data description</h1><p>数据包中包含<strong>编号300-492、共189个数据样本</strong>（其中 342,394,398,460 因技术原因被移除）。数据包格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Pack\</span><br><span class="line">300_P</span><br><span class="line">301_P</span><br><span class="line">...</span><br><span class="line">492_P</span><br><span class="line">util</span><br><span class="line">documents</span><br><span class="line">train_split.csv</span><br><span class="line">dev_split.csv</span><br><span class="line">test_split.csv</span><br></pre></td></tr></table></figure><p>部分样本需要提醒：</p><ul><li>373 - 在5:52-7:00有一个中断，助手进入房间解决一个小的技术问题，会议继续进行并结束。</li><li>444 - 在4:46-6:27左右有一个中断，参与者的手机响了，助手进入房间帮助他们关机。</li><li>451,458,480 - 会话在技术上是完整的，但是缺少了Ellie(虚拟人类)部分的记录。参与者的成绩单仍然包括在内，但没有面试官的问题。</li><li>402 - 视频结尾被删减了约2分钟。</li></ul><p><strong>train_split_Depression_AVEC2017.csv</strong>：该文件包含参与者id、PHQ8二进制标签(PHQ8得分&gt;= 10)、PHQ8得分和参与者性别，以及PHQ8问卷的每个问题的唯一回答。详细信息参见documents文件夹下的scherer_etal2015_VowelSpace.pdf。</p><p><strong>dev_split_Depression_AVEC2017.csv</strong>：同上。</p><p><strong>test_split_Depression_AVEC2017.csv</strong>：仅包含id和性别。</p><p>每个样本文件夹下文件组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">XXX_P\ </span><br><span class="line">  XXX_CLNF_features.txt </span><br><span class="line">  XXX_CLNF_features3D.txt </span><br><span class="line">  XXX_CLNF_gaze.txt </span><br><span class="line">  XXX_CLNF_hog.bin </span><br><span class="line">  XXX_CLNF_pose.txt </span><br><span class="line">  XXX_CLNF_AUs.csv   </span><br><span class="line">  XXX_AUDIO.wav </span><br><span class="line">  XXX_COVAREP.csv </span><br><span class="line">  XXX_FORMANT.csv </span><br><span class="line">  XXX_TRANSCRIPT.csv</span><br></pre></td></tr></table></figure><p>util文件夹组织如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">util\ </span><br><span class="line">  runHOGread_example.m </span><br><span class="line">  Read_HOG_files.m</span><br></pre></td></tr></table></figure><h1 id="File-description-and-feature-documentation"><a href="#File-description-and-feature-documentation" class="headerlink" title="File  description  and  feature  documentation"></a>File  description  and  feature  documentation</h1><p>这部分表述的是每个样本文件夹下各个文件的作用。</p><h2 id="1-CLNF-framework-output"><a href="#1-CLNF-framework-output" class="headerlink" title="1    CLNF framework output"></a>1    CLNF framework output</h2><p>这部分是由CLNF人脸关键点检测算法输出的数据，包括以下文件：</p><ul><li>XXX.CLNF_features.txt<ul><li>包含68个2D人脸关键点；</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success, x0,  x1,…,  x67,  y0,  y1,…, y67。分别表示 帧、时间点、置信度、是否检查成功，各个关键点坐标；</li><li>值之间由逗号分隔，虽然后缀是txt但应该当作csv文件处理。</li></ul></li><li>XXX_CLNF_AUs.csv<ul><li>AU表示Action Unit，是<a href="http://www.360doc.com/content/15/0128/13/10690471_444446832.shtml" target="_blank" rel="noopener">面部表情编码系统</a>(Facial Action Coding System, FACS)的运动单元。每一个AU代表一个表情元素；</li><li>文件格式：frame,  timestamp,  confidence,  success,  AU01_r,  AU02_r, AU04_r,  AU05_r,  AU06_r, AU09_r,  AU10_r,  AU12_r,  AU14_r, AU15_r,  AU17_r,  AU20_r,  AU25_r,  AU26_r,  AU04_c, AU12_c,  AU15_c,  AU23_c,  AU28_c,  AU45_c。其中AUX_r表示该面部包含该AU的概率，而AUX_c则用二值表示是否包含该AU。</li></ul></li><li>XXX.CLNF_features3D.txt<ul><li>包含68个3D人脸关键点；</li><li>格式与2D的类似，只是多了个坐标轴。以摄像机为坐标(0,0,0)，单位为毫米。</li></ul></li><li>XXX.CLNF_gaze.txt<ul><li>文件包含4个视线向量。前两个表示以世界为坐标空间，双眼的视线向量；后两个表示以头为坐标空间，双眼的视线向量。</li><li>文件格式：frame,  timestamp(seconds),  confidence,  detection_success,  x_0,  y_0,  z_0,  x_1,  y_1,  z_1, x_h0,  y_h0,  z_h0,  x_h1,  y_h1,  z_h1</li></ul></li><li>XXX.CLNF_hog.bin<ul><li>Felzenswalb’s  HoG</li></ul></li><li>XXX.CLNF_pose.txt<ul><li>pose文件包含两个坐标，X,Y,Z是位置坐标，Rx,Ry,Rz是头部旋转坐标。位置是以毫米为单位的世界坐标，旋转是以弧度和欧拉角为单位的(为了得到一个合适的旋转矩阵，使用R= Rx * Ry * Rz)。</li><li>文件格式：frame_number,  timestamp(seconds),  confidence,  detection_success,  X,  Y,  Z,  Rx,  Ry,  Rz</li></ul></li></ul><h2 id="2-Audio-file"><a href="#2-Audio-file" class="headerlink" title="2    Audio file"></a>2    Audio file</h2><ul><li>XXX_AUDIO.wav<ul><li>耳机录音频率为16kHz。音频文件中可能包含少量虚拟面试官的信息，在处理时使用记录文件(transcript files)来缓解这个问题。</li></ul></li></ul><h2 id="3-Transcript-file"><a href="#3-Transcript-file" class="headerlink" title="3    Transcript file"></a>3    Transcript file</h2><ul><li>XXX_TRANSCRIPT.csv</li></ul><h2 id="4-Audio-features"><a href="#4-Audio-features" class="headerlink" title="4    Audio features"></a>4    Audio features</h2><ul><li>XXX_COVAREP.csv</li><li>XXX_FORMANT.csv</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DAIC-WOZ数据库是抑郁分析访谈语料库(Distress Analysis Interview Corpus, DAIC) 的一部分，该语料库主要包含临床访谈记录，旨在支持对焦虑、抑郁和创伤后应激障碍等心理困扰状况的诊断。这些访谈数据被收集起来，作为训练一个计算机代理的
      
    
    </summary>
    
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>3D点云深度学习综述之 Shape Classification</title>
    <link href="http://a-kali.github.io/2020/02/08/3D%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%E4%B9%8B-Shape-Classification/"/>
    <id>http://a-kali.github.io/2020/02/08/3D点云深度学习综述之-Shape-Classification/</id>
    <published>2020-02-08T15:45:21.000Z</published>
    <updated>2020-02-09T14:57:27.169Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1    摘要"></a>1    摘要</h1><p>3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术，已经成功地应用于解决各种二维视觉问题。然而，由于使用深度神经网络处理点云所面临的独特挑战，对点云的深度学习仍处于起步阶段。最近，点云上的深度学习变得越来越流行，人们提出了许多方法来解决这一领域的不同问题。目前点云分类方法可以主要被分为两类：<strong>基于投影的网络（Projection-based Networks）</strong>和<strong>点基网络（Point-based Networks）</strong>。本文将从这两个方向对用于点云分类的神经网络方法进行了综述。</p><p><img src="https://i.loli.net/2020/02/09/KUAF1fT6tzu3nVl.png" alt="64NDV_C@`6_PE_XEMYT9OL6.png"></p><h1 id="2-基于投影的网络"><a href="#2-基于投影的网络" class="headerlink" title="2    基于投影的网络"></a>2    基于投影的网络</h1><p>基于投影的方法将三维点云投影到不同的表达模式中(例如：多视图、体积表示)，用于特征学习和形状分类。</p><h2 id="2-1-多视图表示"><a href="#2-1-多视图表示" class="headerlink" title="2.1    多视图表示"></a>2.1    多视图表示</h2><p>多视图（Multi-view）方法通常<strong>提取一个3D对象从多个角度投影的特征，之后再通过某种方法将多个特征融合起来</strong>。而如何将视图特征融合起来则是这个方法的关键。MVCNN开创了这种方法的先例，其简单地将多个视图的特征max-pools到一个全局描述符（global descriptor）中。而max-pooling仅仅保留了单一视图的最大值元素，损失了大量信息。MHBN在此之上进行改进，通过双线性池化（bilinear pooling）来得到一个全局的描述符。后续有许多网络进行了进一步的改进，在此不一一赘述。</p><h2 id="2-2-体积表示"><a href="#2-2-体积表示" class="headerlink" title="2.2    体积表示"></a>2.2    体积表示</h2><p>早期的研究者们通常<strong>使用3D卷积网络对基于3D点云的体积表示（Volumetric representation）进行特征提取</strong>。Daniel等人提出了一种体积占用网络VoxNet来实现3D对象识别。Wu等人提出了一种基于卷积深度信念的3D ShapeNets来学习不同3D形状的点分布。在体素网格上，三维形状通常由二进制变量的概率分布来表示。虽然取得了令人鼓舞的性能，但这些方法无法很好地扩展到稠密的三维数据，因为计算和内存占用是随分辨率的立方增长的。为此，引入了层次结构和紧凑的图结构(如八叉树)来降低这些方法的计算和内存开销。Wang等人提出了一种基于八叉树的OctNet来进行三维形状分类。将在最细叶八分区采样的三维模型的平均法向量输入网络，将3D- cnn应用于被三维形状表面占据的八分区。与基于密集输入网格的基线网络相比，OctNet对高分辨率点云的内存和运行时间要求要少得多。leet等人提出了一种称为点网格的混合网络，它集成了点和网格表示来实现高效的点云处理。在每个嵌入式网格单元中采样的点的数量是恒定的，这使得网络可以通过使用3D卷积来提取几何细节。</p><h1 id="3-点基网络"><a href="#3-点基网络" class="headerlink" title="3    点基网络"></a>3    点基网络</h1><p>点基网络使用最原始的点作为网络的输入。由于其能保留更多的原始特征，点基网络成为当前3D点云识别的主流方法。该方法可以被分为<strong>点级多层感知机（Pointwise MLP）、卷积神经网络、图神经网络、基于数据索引网络（Data indexing-based networks）和其它神经网络</strong>。</p><h2 id="3-1-点级多层感知机"><a href="#3-1-点级多层感知机" class="headerlink" title="3.1 点级多层感知机"></a>3.1 点级多层感知机</h2><p>这些方法<strong>使用MLPs对每个点独立建模，然后使用对称函数聚合全局特征</strong>，如图所示。由于对称函数的存在，这类网络可以保证三维点云的无序性。然而，三维点之间的几何关系并没有得到充分的考虑。</p><p><img src="https://i.loli.net/2020/02/09/8O26rgyBF3i5UvJ.png" alt="FK_UYCNROC_~JTZ_`B`C_KU.png"></p><p>这个方向的开山之作是PointNet，其使用max-pooling聚合多个MLP提取的特征。该网络独立地提取每个点的特征，没有考虑到点之间的结构信息。分层网络PointNet++于此被提出用来捕获邻近点之间的几何结构信息。</p><p>由于其简单性和强大的表示能力，许多网络都是基于PointNet开发的。Achlioptas等人提出了一种深度自编码网络学习点云表示。它的编码器遵循PointNet的设计，使用5个一维卷积层、ReLU非线性激活、批处理标准化和最大池化操作独立地学习点特征。在PATs(Point Attention Transformers)中，每个点都由其绝对位置和相对于其相邻点的相对位置来表示,然后利用GSA(Group Shuffle Attention)来捕获点之间的关系，并建立了一个排列不变、可微、可训练的端到端GSS(Gumbel Subset Sampling)层来学习层次特征。Mo-Net的架构类似于PointNet，但它采用有限的一组矩作为其网络的输入。PointWeb也是在PointNet++上构建的，它使用局部邻居的上下文来使用自适应特性调整(Adaptive Feature Adjustment, AFA)改进点特性。Duan等人提出了一种结构关系网络(SRN)，利用MLP来学习不同局部结构之间的结构关系特征。Lin等人通过为PointNet学习的输入空间和函数空间构造一个查找表来加速推理过程。在中等大小的机器上，ModelNet和ShapeNet数据集上的推理时间加快了1.5 ms，是PointNet的32倍。SRINet首先投影一个点云来获得旋转不变量表示，然后利用基于PointNet的backbone来提取全局特征，并使用基于图的融合来提取局部特征。</p><h2 id="3-2-卷积网络"><a href="#3-2-卷积网络" class="headerlink" title="3.2    卷积网络"></a>3.2    卷积网络</h2><p>与二维网格结构(如图像)定义的卷积核相比，三维点云的卷积核由于点云的不规则性而难以设计。根据卷积核的类型，目前的三维卷积网络可以分为<strong>连续卷积网络和离散卷积网络</strong>，如图所示。</p><p><img src="https://i.loli.net/2020/02/09/YNDcs95ty12ig6p.png" alt="_OW`QQ_M_`4TFGIAJRAWGHW.png"></p><h3 id="3-2-1-3D连续卷积网络"><a href="#3-2-1-3D连续卷积网络" class="headerlink" title="3.2.1    3D连续卷积网络"></a>3.2.1    3D连续卷积网络</h3><p><strong>3D连续卷积在连续空间上定义卷积核，其邻近点的权重与中心点的空间分布有关。</strong>3D连续卷积可以被解释为点子集的加权线性组合，通常使用MLP学习每个点的权重。</p><p>例如 RS-CNN 的关键层 RS-Conv 需要点的子集在某种程度上作为其输入，使用MLP学习局部点之间从低级关系（如Euclidear距离和相对位置）到高级关系之间的映射来实现卷积。</p><p>一些方法还使用现有的算法来执行卷积。在PointConv中，卷积被定义为相对于重要抽样的连续三维卷积的蒙特卡罗估计。卷积核由一个加权函数(通过MLP层学习)和一个密度函数(通过核化密度估计和MLP层学习)组成。为了提高记忆和计算效率，将三维卷积进一步简化为矩阵乘法和二维卷积两种运算。在相同的参数设置下，其内存消耗可减少约64倍。类似的网络还有MCCNN、SpiderCNN、PCNN、KPConv。</p><p>而一些方法则被提出用于解决三维卷积网络所面临的旋转等变问题。Esteves等人提出了以多值球函数为输入，学习旋转等变表示的三维球面卷积神经网络(Spherical CNN)。利用球面谐域内的锚点对谱进行参数化，得到局部卷积滤波器。张量场网络(Tensor field networks)被提出用于定义点积运算，它是一个可学习的径向函数和球面调和函数的乘积，这两个函数对于点的三维旋转、平移和排列是局部等价的。SPHNet以PCNN为基础，通过在体积度量函数的卷积过程中加入球谐核实现旋转不变性。</p><h3 id="3-2-2-3D离散卷积网络"><a href="#3-2-2-3D离散卷积网络" class="headerlink" title="3.2.2    3D离散卷积网络"></a>3.2.2    3D离散卷积网络</h3><p><strong>3D离散卷积在规则网格上定义卷积核，其中相邻点的权值与相对于中心点的偏移量相关。</strong></p><p>Hua等人将非均匀三维点云转化为均匀网格，并在每个网格上定义卷积核。与2D卷积(为每个像素分配权重)不同，3D卷积核为落入相同网格的所有点分配相同的权重。对于给定的点，位于同一网格上的所有相邻点的平均特征是从上一层计算出来的。然后对所有网格的平均特征进行加权求和，得到当前层的输出。Lei等人定义了一个球面卷积核，方法是将一个三维球面邻近区域划分为多个容量容器，并将每个容器与一个可学习的加权矩阵相关联。一个点的球面卷积核的输出由相邻点的加权激活值的平均值的非线性激活决定。类似的神经网络还有GeoConv、PointCNN、InterpConv、RIConv、A-CNN、ReLPV、SFCNN等。</p><h2 id="3-3-图神经网络"><a href="#3-3-图神经网络" class="headerlink" title="3.3    图神经网络"></a>3.3    图神经网络</h2><p><strong>基于图的网络将点云中的每个点视为一个图的顶点，并基于每个点的邻居为图生成有向边。然后在空间或光谱域中进行特征学习。</strong>典型的基于图的网络如图所示。</p><p><img src="https://i.loli.net/2020/02/09/JkrwP75SA2TaXCd.png" alt="MLVH~H__VJDM32NKGZ4_7LX.png"></p><h2 id="3-4-基于数据索引的网络"><a href="#3-4-基于数据索引的网络" class="headerlink" title="3.4    基于数据索引的网络"></a>3.4    基于数据索引的网络</h2><p>这类网络是<strong>基于不同的数据索引结构(如八叉树和kd-tree)构建的。在这些方法中，点特征由沿着树从叶节点到根节点的层次学习得到。</strong>其典型代表作有Kd-Net、3DContextNet、SO-Net等。</p><h1 id="4-性能对比"><a href="#4-性能对比" class="headerlink" title="4    性能对比"></a>4    性能对比</h1><p><img src="https://i.loli.net/2020/02/09/lzaiosyh2jkuqr9.png" alt="ST8DMA9EJ2DE__N48_9HN@F.png"></p><p>大概可以看出基于卷积的GeoCNN表现最佳。</p><p>参考论文：Deep Learning for 3D Point Clouds: A Survey</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-摘要&quot;&gt;&lt;a href=&quot;#1-摘要&quot; class=&quot;headerlink&quot; title=&quot;1    摘要&quot;&gt;&lt;/a&gt;1    摘要&lt;/h1&gt;&lt;p&gt;3D点云由于其在计算机视觉、自动驾驶和机器人等领域的广泛应用而受到越来越多的关注。深度学习作为人工智能的主要技术
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="3D点云" scheme="http://a-kali.github.io/tags/3D%E7%82%B9%E4%BA%91/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multi-level Attention network using text, audio and video for Depression Prediction</title>
    <link href="http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/"/>
    <id>http://a-kali.github.io/2020/02/05/Multi-level-Attention-network-using-text-audio-and-video-for-Depression-Prediction/</id>
    <published>2020-02-05T02:18:16.000Z</published>
    <updated>2020-04-13T02:58:01.538Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一种常见的心理疾病，严重影响心理和身体健康，甚至可能导致失去生命。由于缺乏诊断抑郁症的有效手段，越来越多的人对通过行为线索来自动诊断以及阶段预测抑郁症感兴趣。摘要提出了一种<strong>基于多级注意的多模态抑郁症预测网络</strong>，该网络融合了音频、视频和文本模式的特征，同时学习模式内和模式间的相关性。多层次的注意力通过选择每个模式中最具影响力的特征来加强整体学习。我们进行了详尽的实验，为音频、视频和文本模式创建不同的回归模型。建立了几种不同构型的融合模型，分析了各特征和模态的影响。就均方根误差而言，我们比当前基准高出17.52%。</p><h1 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2    Related Works"></a>2    Related Works</h1><p>在[28]中，作者对融合技术在抑郁症检测中的应用进行了综述。他们还提出了一种基于计算语言学的多模态融合检测方法。在[25]中，作者基于上下文感知特征生成技术和端到端可训练的深度神经网络对临床访谈语料库数据集的抑郁水平进行了分析。他们进一步在变压器网络中注入基于主题建模的数据增强技术。Zhang等人发布了一个用于人类行为分析的多模态自发情感语料库[44]。面部表情由3D动态测距、高分辨率视频采集和红外成像传感器捕捉。除了面部环境，还会监测血压、呼吸和脉搏率，以判断一个人的情绪状态。利用AVEC挑战中发布的数据，对音频、视频和生理参数进行调查，以观察受试者情绪状态的关键发现。在[30]中，作者融合了音频、视觉和文本线索来获取多媒体内容中的情感。他们利用特征和决策级融合技术来进行有效的决策。在[2]中，作者利用副语言、头部姿势和眼睛凝视进行多模式抑郁检测。在选定特征的统计检验的帮助下，推理机将受试者分为抑郁和健康两类。当结合多种模式时，了解每种模式在任务预测中的贡献是很重要的，注意力网络可以用来研究其相对重要性。在这篇论文中，我们在每个情态中使用注意来理解情态中低层或深层特征的相对重要性。在融合三种模式的同时，我们还使用了注意力层，并学习了注意力的权重，从而找到每种模式的重要性比例。Querishi等人的论文[32]是唯一一篇在使用数据集子集和在一层应用注意力方面最接近我们的论文。通过在多个层次上使用多层注意力，我们能够获得比它们更好的结果，并且由于注意力操作，网络的计算成本更低，从而最小化了框架的测试时间。</p><h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3    Methodology"></a>3    Methodology</h1><h2 id="3-1-Text-Modality"><a href="#3-1-Text-Modality" class="headerlink" title="3.1    Text Modality"></a>3.1    Text Modality</h2><p>由于有几个参与者使用的是英语口语词汇，所以我们对这些词汇进行了修改，用原始的完整词汇替换这些词汇，否则，在训练语言建模或其他预测的神经网络时，这些词汇就会变成不存在于词向量中的词汇。我们<strong>使用预训练的通用语句编码器（Universal Sentence Encoder）来得到Embedings</strong>。为了获得常数大小的张量，我们使用零补齐短句，并有一个常数数量的时间步为400。每个语句嵌入向量的长度为512，使得最后的数组维数为(400,512)。我们使用两层叠加的双向长短时记忆网络结构，以Embeding为输入，以PHQ分数为输出，训练语音转录的回归模型。每个BLSTM层有200个隐藏单元，其中第一层的前向层的每个隐藏单元的输出连接到第二层的前向隐藏单元的输入。同样的连接也建立在每一个隐藏单元的后向层，以创建堆叠。这两层BLSTM在每一步的输出为(batchsize,400)，并将其作为输入发送到前馈层进行回归。我们保持前馈层的节点数为(500,100,60,1)，并使用ReLU作为激活函数。</p><h2 id="3-2-Audio-Modality"><a href="#3-2-Audio-Modality" class="headerlink" title="3.2    Audio Modality"></a>3.2    Audio Modality</h2><p>对于音频模态，我们使用不同的音频特征(低级特征及其功能)创建模型。在功能(functional)方面，变分的算术平均值和协效率被应用于底层特征，并作为底层特征之上的知识抽象。语音音色是由诸如梅尔频率倒谱系数(MFCC)这样的LLD特征编码的，研究表明，低阶MFCC在情感预测和副语言语音分析任务中更重要。eGeMAPS特征集包含88个特征，包括GeMAPS特征集及其频谱特征和功能。GeMAPS特性包括频率相关特性(音高、抖动、共振峰)，能量相关功能(微光，响度，谐波噪声比)，光谱参数(α比，哈马伯格比)，谐波以及六个时序特征相关的语音比率。除了上述的这些低电平特征外，音频样本的高维深度表示是通过将音频通过深度频谱和VGG网络来提取的。这一特征在本文的其余部分被称为深度densenet特征。</p><p>对于音频特征，我们的实验只考虑了参与者所说的向量的跨度。每个特征都是挑战数据的一部分，它们有不同的采样率。功能性音频和深度densenet特征采样频率为1Hz，而BoAW采样频率为10Hz，LLD采样频率为100Hz。低级MFCC特征长度为39，低级eGEMAPS长度为23，总时间步长为140500。而功能的长度分别为78和88，时间步长分别为1300和1410。BoAW-MFCC和BoAW-eGEMAPS的特性长度为100，每个特性的时间步长为14050。深度densenet特征长度为1920维，时间步长为1415步。</p><p>在单独的音频通道，我们训练了另一个二层的BLSTM网络，每层有200个隐藏单元。我们将最后一层的输出传递给多层感知器，每一层(500,100,60,1)个节点，并以ReLU作为激活函数。</p><h2 id="3-3-Visual-Modality"><a href="#3-3-Visual-Modality" class="headerlink" title="3.3    Visual Modality"></a>3.3    Visual Modality</h2><p>对于挑战数据集中的视频特征，我们对LLD及其function进行了实验。由于深度LSTM网络也可以从数据(如function和更抽象的信息)中学习类似的特征，所以我们选择使用LLD，因为它包含的信息比其平均值和标准差更多。每个用于姿态、注视和面部动作单元(FAU)的LLD特征都以10Hz采样。这些特征的长度分别为6、8、35，都有15000个时间步长。在挑战数据中还提供了BoVW，它的长度为100，有15000个时间步长。我们使用这些特性来训练每个特征的模型，所有特征都使用一个包含200个隐藏单元的BLSTM单层，然后是一个maxpooling和回归器。我们尝试了各种不同的组合，比如所有输出的和、输出的平均值，还使用了maxpooling作为三个备选方案，但是maxpooling效果最好，所以我们在LSTM输出上使用了maxpooling。</p><h2 id="3-4-Fusion-of-Modalities"><a href="#3-4-Fusion-of-Modalities" class="headerlink" title="3.4    Fusion of Modalities"></a>3.4    Fusion of Modalities</h2><p>早期融合(early fusion)需要消耗大量的计算资源，当使用神经网络训练时可能导致过拟合，因此，<strong>后期融合(late fusion)和混合融合(hybrid fusion)</strong>模型变得更加普遍。我们提出了一种<strong>基于多级注意力机制的网络，该网络学习每个特征的重要性，并对它们进行相应的加权，从而实现更好的早期融合和预测</strong>。这样的注意力网络让我们了解到哪种模态的特征对学习更有影响。它还能表示出各个模态对预测结果准确度所做出的贡献比例。</p><p>在融合的过程中，我们在每个通道之间进行了多次实验。首先，我们融合了视频模态的LLD。我们获取注视、姿势和面部动作单元特征，将它们通过包含200个BLSTM神经元的单层网络传递进行前向传播，并对它们施加注意力。注意层的输出通过另一个具有200个单元的BLSTM层。我们对LSTM的输出进行全局最大池化，并通过128个隐藏单元的网络前向传播。<strong>(BLSTM-&gt;Attention-&gt;BLSTM-&gt;Maxpooling-&gt;FC)</strong></p><p>在第二个模型中，我们使用一个类似的网络将视频LLD与BoVWs相结合，该网络由包含200个隐藏单元的BLSTM层、注意力层和另一个BLSTM组成，然后通过一个前馈层进行回归。</p><p>第三个融合模型将视频模态的注意向量输出和文本模态的输出进行合并(combine)，并在视频回归器之前通过一个堆叠的BLSTM和一个注意力层。</p><p>第四个融合模型同时使用音频和文本模态。我们再次在每个模态中获取注意层的输出，并构建一个混合融合网络，但通过两条路径传递它们。在第一种路径中，每个模态的注意力层输出被拼接起来输入注意力层；另一条路径通过两个注意层的输出通过一个堆叠的BLSTM，该BLSTM有2层，每层有200个单元。在堆叠的BLSTM层上应用一个注意层，该输出被馈送到128个隐藏单元的前馈网络。由于使用了文本特征，因此比单独使用音频特征的模型具有更好的性能。</p><p>我们的第五个融合模型同时使用视频和文本模态，这里我们再次在视频的每个子模态上使用注意力层，然后使用另一个注意网络将它与文本模态合并。令人惊讶的是，这种融合的结果与音频和文本模态融合非常相似，学习曲线也非常相似。</p><p>我们的第六个也是最终融合模型使用了所有的模态。我们使用基于注意力的视觉模态来获得一个包含128个单元的向量，同时使用基于注意力的音频模态来获得一个128单元向量，随后从文本模态中提取信息并从中得到128位(bit)的向量。我们再次在这三种模态上使用另一个注意力层，将它们融合在一起，回归到PHQ8评分。收敛后，视频、音频和文本的注意率分别为[0.21262352, 0.21262285, 0.57475364]。</p><p><img src="https://s1.ax1x.com/2020/04/08/GWzDvq.png" alt="GWzDvq.png"></p><h1 id="4-Results"><a href="#4-Results" class="headerlink" title="4    Results"></a>4    Results</h1><p>本节详细介绍了所有回归模型的结果及其融合研究。由于测试数据的标签在挑战中不可用，所以我们在验证(dev)分区上展示了大部分结果。测试分区上的唯一结果来自基于文本的模型，我们使用该模型提交了一份报告，并从挑战中获得了所有的分数。</p><h2 id="4-1-Results-from-Text-Modality"><a href="#4-1-Results-from-Text-Modality" class="headerlink" title="4.1    Results from Text Modality"></a>4.1    Results from Text Modality</h2><p>在E-DAIC(挑战数据)和DAIC-WoZ数据的测试集上，基于注意力的BLSTM网络在文本方面的训练效果优于其他模态。这与临床医生的观察一致，即<strong>语言内容是一个重要的标志，具有明确的特征，可能直接影响患者处于哪个抑郁阶段</strong>。我们在验证集的根均方误差(RMSE)为4.37 。在测试集上，基于文本的模型能够实现平均绝对误差(MAE)为4.02，RMSE为4.73，对应的一致性相关系数(CCC)为0.67，CCC是该比赛中的主要评估系数。该模型的皮尔逊相关系数(PCC)为0.676，决定系数(r2)为0.457，根据对测试集的挑战结果，斯皮尔曼相关系数(SCC)为0.651。总体而言，该网络工作优于现有模型的8.95%。代码在15个epoch时收敛，验证损失为4.37，批处理大小为10，这些参数是根据经验选择的。预测单个文本抄写的平均测试时间为0.09秒。</p><h2 id="4-2-Results-from-Audio-Modality"><a href="#4-2-Results-from-Audio-Modality" class="headerlink" title="4.2    Results from Audio Modality"></a>4.2    Results from Audio Modality</h2><p>与基线模型相比，我们每个单独的网络在RMSE方面都表现出色。在基于音频MFCC特征的模型中，我们的表现比基线高出29.80%，而在eGEMAPS中，我们的表现则高出29.04%。对于BoAW-MFCC，我们的表现比基准高出10.44%，而对于BoAW-eGE，我们的表现比基准高出14.46%。每个单独的音频特性代码运行15个epoch，批处理大小为10。使用Functional MFCC对一个样本的平均时间要求为0.23秒，使用Functional eGEMAPS为0.14秒，使用BoAW-MFCC为0.45秒，使用BoAW-eGE为0.45秒，DS-DNet特征为0.13秒。对于音频模型，我们尝试了一个卷积神经网络架构来融合MFCC、eGEMAPS和DS-DNet特性，但是我们发现Bi-LSTMs的性能略好于卷积网络，其对序列特征有着更好的学习能力。</p><h2 id="4-3-Results-from-Visual-Modality"><a href="#4-3-Results-from-Visual-Modality" class="headerlink" title="4.3    Results from Visual Modality"></a>4.3    Results from Visual Modality</h2><p>视频特征的结果好于基线和技术水平，但仍比文本和语音模式的结果差。在视觉特征中，BoVW的表现最好，超出基线4.8%。与[32]相比，我们使用姿势特征的表现要比他们好9.3%，使用凝视的表现要比他们好6.6%，使用面部动作单元的表现要比他们好8.7%。</p><h2 id="4-4-Results-from-Fused-Modalities"><a href="#4-4-Results-from-Fused-Modalities" class="headerlink" title="4.4    Results from Fused Modalities"></a>4.4    Results from Fused Modalities</h2><p>该模型使用了所有的特征，融合了多层次的注意力，得到了超出baseline 17.52%的结果。与[32]相比较，音频-文本融合网络和视频-文本融合网络的性能分别提高了5.8%和9.19%，而全融合网络的性能略差。这不是结论性的，因为本文使用的数据集略有不同。注意机制自动权衡每种模式中的每个特征，并允许网络关注回归决策中最重要的特征。这样，网络就能了解特征与PHQ-8 scores之间的关系。</p><h1 id="5-Discussion"><a href="#5-Discussion" class="headerlink" title="5    Discussion"></a>5    Discussion</h1><p>本文提出了一种基于多级注意力的早期融合网络，融合音频、视频和文本模式来预测抑郁症的严重程度。在这项任务中，我们观察到注意力网络给予文本情态的权重最高，给予音频和视频情态的权重几乎相等。给予特定词更高的权重是与临床医生一致的，因为言语内容对诊断抑郁水平至关重要。音频和视频是同样重要的信息来源，对预防严重程度至关重要。我们对视频数据重要性较低的直觉是，我们可以从视频模态(眼球注视、面部动作单元和头部姿势)中使用有限的特征。临床医生在面对面的访谈中可以观察一个人的身体姿势(自我接触颤抖等)或记录电生理信号，从而进行诊断。</p><p>多层次注意力的使用使我们在所有个体和融合模型中获得了比基线和技术水平更好的结果。把注意力放在每个特征和形态上总体上有两方面的优势。首先，这让我们更深入和更好地理解每个特征在抑郁症预测中的重要性。其次，简化了网络的整体计算复杂度，减少了训练和测试时间。实验结果表明，基于多水平注意力的全特征融合模型较基线有17.52%的提高。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-Abstract&quot;&gt;&lt;a href=&quot;#1-Abstract&quot; class=&quot;headerlink&quot; title=&quot;1    Abstract&quot;&gt;&lt;/a&gt;1    Abstract&lt;/h1&gt;&lt;p&gt;抑郁症一直是全球精神健康疾病的主要原因。重性抑郁症(MDD)是一
      
    
    </summary>
    
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁检测" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Multimodal Fusion with Deep Neural Networks for Audio-Video Emotion Recognition</title>
    <link href="http://a-kali.github.io/2020/02/03/%E3%80%90%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E3%80%91Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/"/>
    <id>http://a-kali.github.io/2020/02/03/【论文解读】Multimodal-Fusion-with-Deep-Neural-Networks-for-Audio-Video-Emotion-Recognition/</id>
    <published>2020-02-03T13:07:57.000Z</published>
    <updated>2020-04-07T13:53:09.586Z</updated>
    
    <content type="html"><![CDATA[<p>摘要：本文提出了一种<strong>新的音频、视频和文本多模态融合用于情绪识别的深度神经网络</strong>。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平上进行早期模式融合的先进系统相比，我们提出的DNN可以实现更高水平的<a href="http://blog.sina.com.cn/s/blog_1859648c00102ylcy.html" target="_blank" rel="noopener"><strong>一致性相关系数(Concordance Correlation Coefficient, CCC)</strong></a>。该DNN在数据集开发分区上分别获得了0.606、0.534和0.170的CCCs，分别为唤起度(arousal)  、效价(valence)和喜欢度(liking)。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>最先进的传感器捕捉音频和视频信号，这为许多创新技术铺平了道路，这些技术可以实现非接触式的监测和诊断。例如，它们可以对个人进行持续的健康监测，这对于治疗和管理范围广泛的慢性疾病、神经障碍和精神健康问题，如糖尿病、高血压、哮喘、自闭症谱系障碍、疲劳、抑郁、药物成瘾等，变得越来越重要。</p><p>人们普遍认为，未来的人类环境(家庭、工作场所、公共交通等)将包含智能传感器阵列，可以支持和预测所需的行动，以一种普遍和不显眼的方式最佳地自我调节心理状态。识别个人情感状态的技术，特别是图像/视频和语音处理技术以及机器学习技术，有望在这一未来愿景中发挥关键作用。</p><p>然而，尽管有精密的传感器和技术，如何在真实场景精准识别仍然是一项挑战。首先，稳定地捕获时空信息，并为种群中的每个表达提取共同的可有效编码的时间特征同时抑制特定主题(类内)的变化是很难的。根据特定的个体行为和捕捉条件，面部表情和言语表达具有显著的时空变化。此外，创建具有代表性的大型音频视频数据集，并根据需要提供可靠的专家注释，以设计识别模型，并准确检测唤醒和效价水平是非常昂贵的。</p><p>在本文中，我们认为动态表情识别技术能够使用多种不同的模式准确地评估对象的时序情绪状态。我们提出了一种深度神经网络(DNN)结构，用于语音、人脸和文本信息的多模态融合，用于音视频情感识别。为了准确识别，本文提出的DNN提供了一个中间层次的融合，其中特征、分类器和融合函数以端到端的方式进行全局优化。</p><p>论文结构如下。第二部分介绍了文献中提出的针对不同模式的情绪识别技术。第三节介绍了多模态融合的DNN体系结构。第四部分给出了实验协议，第五部分给出了在RECOLA数据集上进行的实验，以及针对音频、视频和文本三种模式的实验结果，以及它们的融合。最后，在最后一节中对未来的工作进行了总结和展望。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2    Related Work"></a>2    Related Work</h1><p>在过去的十年中，面部表情识别(ER)一直是人们非常感兴趣的话题。人们提出了许多ER技术，可以从一张静止的面部图像中自动识别出七种常见的情绪类型——喜悦、惊讶、愤怒、恐惧、厌恶、悲伤和中性。这些面部ER的静态技术往往是基于外观或基于几何图形的方法。最近，动态面部表情识别技术已经成为一种提高性能的方法，其中表情类型是通过在一个对象的物理面部表情过程中所捕获的一系列图像或视频帧来估计的。这不仅可以在空间域提取人脸的外观信息，还可以在时间域提取人脸的演化信息。该技术可以是<strong>基于形状、基于外观或基于动作</strong>的方法。</p><p>基于形状的方法，如约束局部模型(constrained local model, CLM)，基于突出锚点标记来表示面部特征形状，这些标记的运动为识别过程提供了判别信息；基于外观的方法，如LBP-TOP从面部图像中提取图像强度或其他纹理特征来表示面部表情；基于运动的自由变形模型等面部表情时空演化方法，需要可靠的人脸定位方法。例如，Guo等人使用atlas构造和稀疏表示从动态表达式中提取时空信息。虽然计算复杂度较高，包括时间信息和空间信息，但与静态图像相比，具有更高的识别精度。</p><p>在过去的几年里，自动检测说话人的情绪状态变得越来越流行。为了提高抑郁检测和情绪检测系统的准确性，人们探索了不同的方法。这两个方向有一些相似之处。从抑郁检测方面，France等人已经表明，格式频率的变化是抑郁和自杀倾向的良好指标。Cummings等人利用能量和光谱特征对抑郁症(抑郁症和非抑郁症)进行了二分类，其准确率约为70%。Moore等人通过统计分析(平均值、中位数和标准偏差)音高、能量或说话速度等韵律特征，获得了75%的准确率。就像几乎所有涉及到机器学习技术的领域一样，神经网络在情绪检测中的应用已经变得非常流行。研究人员通常使用DNNs、长短时记忆神经网络(LSTMs)和卷积神经网络(CNNs)。</p><p>包括以往AVEC比赛结果在内的多项情绪或减压检测研究的证据表明，通过整合来自多个不同信息源的证据(主要在特征、得分或决策层面)，可以提高识别系统的准确性和可靠性。因此，人们最近对通过多模态融合来检测情绪状态，特别是语音和面部模式的检测产生了一些兴趣。Kachele等人提出了一种分级多分类器框架，该框架通过引入不同程度的确定性来自适应地组合输入模式。声音韵律和面部动作单元也被用来检测抑郁。Menget等人提出了一种利用运动历史直方图动态特征从音频和视频中识别抑郁症的多层系统。Nasir等人提出了一种多模态特征，在多分辨率建模和融合框架中捕捉抑郁行为线索。特别地，他们利用Teager基于能量和i-vector的特征，连同音素率和持续时间来预测音频，并在视频中使用多项式参数化的时间变化和从面部地标获得的区域特征。最后，Williamson等人提出了一种很有前途的系统，该系统利用语音、韵律和面部动作单元特征的互补多模态信息来预测抑郁症的严重程度，尽管每种模态的贡献并没有被讨论。</p><p>尽管有精密的传感器和强大的技术，但在现实世界场景中开发精确的情绪识别模型仍面临一些挑战。在设计过程中，代表性数据的数量是有限的。假设识别模型是使用从特定条件下提取的有限数量的标记参考样本设计的。虽然许多音视频信号可以被捕获来设计识别模型，但它们需要昂贵的专家注释来创建大规模的数据集来检测唤醒和价态水平。在操作过程中，在操作域(即、受试者的办公室、家庭等)，在各种情况下。根据特定的捕捉条件和个体行为，面部和语音表情会随着时间发生显著的动态变化。因此，识别模型并不能代表操作域内模式的类内变化。实际上，任何分布上的变化(无论是域移位还是概念漂移)都会降低系统性能。识别模型需要对特定人员、传感器和计算设备以及操作环境进行校准和调整。</p><p>众所周知，随着时间的推移，结合来自不同模式的时空信息可以提高鲁棒性和识别精度。模式也可以根据上下文或语义信息动态组合，例如记录环境中的噪声。例如，可以根据捕获条件在不同的层(分辨率)组合深度学习架构的输出响应。</p><p>本文主要研究利用深度学习体系结构来生成精确的混合情感识别系统。例如，Kim等人提出了一种层次化的3级CNN结构来组合多模态源。DNNs被认为是学习具有特定目标的转换序列，以获得将在一个系统中组合的特征。由于特征级和分数级融合不一定能获得较高的精度，因此提出了一种混合方法，通过学习特征和分类器来优化多模态融合。</p><h1 id="3-Multimodal-Fusion-with-Deep-NNs"><a href="#3-Multimodal-Fusion-with-Deep-NNs" class="headerlink" title="3    Multimodal Fusion with Deep NNs"></a>3    Multimodal Fusion with Deep NNs</h1><p>本文提出了一种高效的的DNN结构，该结构可以从多个信息源中学习受试者行为与情绪状态之间的映射。对于给定的时序面部和语音模态（包括文本信息）的AVEC SEWA 数据库，本系统旨在从中学习特征表示、分类和融合以对影本进行准确地预测。</p><p>该方法的主要内容在于联合学习每一个判别表示，以及它们的分类和融合函数。每个特征子集首先由一个或多个隐藏层独立处理。网络的这一部分学习给定模态的最佳特征表示。然后，每个块的最后隐藏层相互连接到一个或多个完全连接的层，这些层将进行特征的分类和融合。从全局的角度来看，这个网络应该学习如何转换输入特征，从而进行分类和融合，并产生一个全局决策。训练一个混合分类器来组合这些特征可以提高识别系统的整体精度。该架构使用了三种不同的信息来源——音频（语音）、视频（脸部）和文本。有关特性集的详细信息，请参阅[20]。</p><p>1)音频特征：声学特征由23个声学LLD组成，如能量、频谱和倒频谱、音高、音质和微韵律特征，每10ms的短帧提取一次。使用一个包含1000个音频单词的编码书，在6秒的时间段里计算分段级声学特征，并创建音频单词的直方图。因此，得到的特征向量有1000维。</p><p>2)视频特征：每帧（帧步20ms）提取视频特征，包括三种特征类型：按度数归一化的人脸方向；10个视点的像素坐标；49个面部标志的像素坐标。每个带有独立码本和直方图的BoVW被创建为三个面部特征类型，每个码本的大小为1000，产生一个3000维的节段级特征向量。</p><p>3)文本特征：文本特征是基于语音转录的词袋特征表示。该词典包含521个单词，其中只考虑了一元语法（unigram）。在一段6秒的时间内创建的直方图。总的来说，文本包(BoTW)包含521个特征。</p><h2 id="3-1-DNN-Architecture"><a href="#3-1-DNN-Architecture" class="headerlink" title="3.1    DNN Architecture"></a>3.1    DNN Architecture</h2><p>所提出的多模态融合的DNN体系结构如图所示。DNN对每个模态使用一对全连接层分别处理音频、视频和文本，以生成和探索同一类型特征之间的相关性。然后，第二阶段(b)合并这些独立层的输出。该层在一个块中接收前一步的输出，并提供一个与模式的本质相关联的全连接层(c)。DNN输出由单个线性神经元(d)产生，作为整个网络的回归值。最后，利用标度模(e)来缩小预测量与标签量之间的差距。不同的标度函数被认为是最好的结果-小数标度，最小最大归一化和标准差标度。最终的预测(f)通过线性函数(前一层的加权和)产生。</p><p><img src="https://i.loli.net/2020/02/04/AMvSXGaWkoDQr1d.png" alt="E@CC6K8WWH9YLDBW7UAALCT.png"></p><h1 id="4-Experimental-Methodology"><a href="#4-Experimental-Methodology" class="headerlink" title="4    Experimental Methodology"></a>4    Experimental Methodology</h1><p>RECOLA数据集包括一个训练集和一个开发集(development set)。在本研究的实验中，开发集被分为两个子集。第一个包含从开发集的14份数据中随机选择的5个数据。剩下的数据作为测试集来评估方法的性能。</p><p>在SVR实验中，原始提案被用于建立单峰和早期融合系统。后融合方法使用第一个开发子集来优化每个单模系统。使用第二个开发子集对融合函数进行了优化。</p><p>在DNN的情况下，更小的开发集被用来确定每个模式的最佳层数和神经元数，以及融合层的神经元数。第二个开发子集用于测试模型。注意，每个情感维度都有自己的体系结构，如表所示。</p><p>本次挑战采用的评价指标为CCC，其定义为:</p><p><img src="https://i.loli.net/2020/02/04/VKebGpFvu2qTNLd.png" alt="_1VUA87A@T_2AO9_Y9_4_OI.png"></p><h2 id="4-1-Preprocessing"><a href="#4-1-Preprocessing" class="headerlink" title="4.1    Preprocessing"></a>4.1    Preprocessing</h2><p>当应用延迟补偿函数时，CCC评分的结果有显著的提高。图2显示了CCC随延迟的变化。</p><p><img src="https://i.loli.net/2020/02/04/p9Gat8BS4V3ud1s.png" alt="Y__AZX_RMMEY_8~AES`42H3.png"></p><h1 id="5-Conclusion-and-Future-Work"><a href="#5-Conclusion-and-Future-Work" class="headerlink" title="5    Conclusion and Future Work"></a>5    Conclusion and Future Work</h1><p>本文提出了一种新的DNN结构来预测情绪状态。它融合了三种不同的模态：音频信号、面部特征和音频信号的对话文本。每个模态首先由两个全连接层独立编码，然后合并成一个单一的表示，然后用于估计主体的情绪状态。该网络以端到端方式训练，提供比其他提出的架构更高的CCC。通过对输入特征进行适当的归一化，并对回归的输出进行时间平滑处理，可以期望得到进一步的改进。对于未来的工作，我们计划扩展我们的工作，包括一个基于递归神经网络的最后阶段，它可以学习情绪的时间模式，从而提高整个系统的准确性。此外，我们还将评估从明确训练的卷积神经网络中提取的视觉特征的性能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘要：本文提出了一种&lt;strong&gt;新的音频、视频和文本多模态融合用于情绪识别的深度神经网络&lt;/strong&gt;。该DNN体系结构具有独立层和共享层，这些层旨在学习每种模态的表示，以及模态之间的最佳组合来得到最佳的预测结果。AVEC情绪数据集上的实验结果表明，与其他在特征水平
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="情感识别" scheme="http://a-kali.github.io/tags/%E6%83%85%E6%84%9F%E8%AF%86%E5%88%AB/"/>
    
      <category term="多模态" scheme="http://a-kali.github.io/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（14）—— 如何选刊</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8814%EF%BC%89%E2%80%94%E2%80%94-%E5%A6%82%E4%BD%95%E9%80%89%E5%88%8A/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（14）——-如何选刊/</id>
    <published>2020-02-03T05:10:55.000Z</published>
    <updated>2020-02-03T05:49:36.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何确定你的目标期刊？"><a href="#如何确定你的目标期刊？" class="headerlink" title="如何确定你的目标期刊？"></a>如何确定你的目标期刊？</h1><ul><li>期刊的类型、领域范围</li><li>期刊的声望和影响力（影响因子、引用率）</li><li>审稿和发表程序</li></ul><h2 id="1-期刊的目标和范围"><a href="#1-期刊的目标和范围" class="headerlink" title="1    期刊的目标和范围"></a>1    期刊的目标和范围</h2><p>回答三个问题：</p><ol><li>研究领域是否相关？</li><li>你的论文类型是否在该期刊发表过？</li><li>期刊的读者群体是什么？（专业/综合、亚洲/欧洲 等）</li></ol><h2 id="2-期刊的声望和影响力"><a href="#2-期刊的声望和影响力" class="headerlink" title="2    期刊的声望和影响力"></a>2    期刊的声望和影响力</h2><p>四个问题：</p><ol><li>同行评价/阅读人数如何？</li><li>期刊的影响因子是否满足你的要求？</li><li>期刊收录的数据库有哪些？</li><li>期刊的出版形式？（在线/印刷）</li><li>期刊的委员会成员、赞助者是否知名？</li></ol><h2 id="3-审稿和发表程序"><a href="#3-审稿和发表程序" class="headerlink" title="3    审稿和发表程序"></a>3    审稿和发表程序</h2><ol><li>期刊的出版频率（月刊/半月刊/季刊）</li><li>发表周期（一审/二审/发表）</li><li>接收后见刊时间（即发表到刊物上）</li><li>发表费用（版面费、彩图费是否合理？能否报销？）</li><li>是否可以公开获取（开放权限），多久能公开获取（数据库收录）</li></ol><h1 id="查找合适的投稿期刊"><a href="#查找合适的投稿期刊" class="headerlink" title="查找合适的投稿期刊"></a>查找合适的投稿期刊</h1><ol><li><strong>从每年的期刊影响因子中查找</strong>：每年的6月份左右Thomson都会发布上一年所有SCI期刊的影响因子，通过这些可以了解本专业领域的顶级期刊，选择合适自己的期刊。</li><li><strong>从参考文献中获得合适的期刊</strong>：统计在撰写论文过程中阅读的大量文献来自于哪些期刊，再根据自己论文的质量选出合适的期刊。</li><li><strong>询问同行或者导师</strong>。</li><li><strong>向期刊编辑或者主编咨询</strong>。 </li><li><strong>借助选刊工具</strong>：<ol><li>Edanz Journal Selector</li><li>Elsevier Journal Finder</li><li>Journal Article Name Estimator</li><li>Springer Journal Selector</li><li>MedSci期刊选择智能支持系统</li></ol></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;如何确定你的目标期刊？&quot;&gt;&lt;a href=&quot;#如何确定你的目标期刊？&quot; class=&quot;headerlink&quot; title=&quot;如何确定你的目标期刊？&quot;&gt;&lt;/a&gt;如何确定你的目标期刊？&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;期刊的类型、领域范围&lt;/li&gt;
&lt;li&gt;期刊的声望和影响
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（13）—— 写作语句链接</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8813%EF%BC%89%E2%80%94%E2%80%94-%E5%86%99%E4%BD%9C%E8%AF%AD%E5%8F%A5%E9%93%BE%E6%8E%A5/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（13）——-写作语句链接/</id>
    <published>2020-02-03T03:59:33.000Z</published>
    <updated>2020-02-03T04:49:42.992Z</updated>
    
    <content type="html"><![CDATA[<p>在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括<strong>连接词和副词</strong>。</p><p>根据语句的前后关系，连接词可以分为7类：</p><ul><li><p><strong>因果关系</strong>；常用连接词：therefore, consequently, thus, hence, as a result, indeed等，用来对前句的内容进行总结。</p></li><li><p><strong>并列关系</strong>；常用连接词：also, likewise, besides in addition, moreover, furthermore等，用来连接多个同等事物或特征。</p></li><li><p><strong>相反关系</strong>；常用连接词：in contrast, but, however, yet, on the other hand, surprisingly, nevertheless, instead of等，用来描述某些设想与文献描述不符的现象或者结果。</p></li><li><p><strong>相似关系</strong>（类似于并列关系）；常用连接词：similarly, likewise，用于表达两个功能或者结果具有相似性。</p></li><li><p><strong>举例关系</strong>；常用连接词：for example, for instance, specifically, such as including等。</p></li><li><p><strong>时间关系</strong>；如：later等。</p></li><li><p><strong>顺序关系</strong>（类似于时间关系）；常用连接词：then, next, finally, first, second等。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在SCI论文的写作过程中，常常会出现一些比较复杂的句子，为了能够使语句的意思更加紧密合理的表达出来，就需要使用一些连接词来简介语句的前后关系。能用来连接语句前后关系的词包括&lt;strong&gt;连接词和副词&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;根据语句的前后关系，连接词可以分为7类
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（11）—— 实在憋不出来咋办</title>
    <link href="http://a-kali.github.io/2020/02/03/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%8811%EF%BC%89%E2%80%94%E2%80%94-%E5%AE%9E%E5%9C%A8%E6%86%8B%E4%B8%8D%E5%87%BA%E6%9D%A5%E5%92%8B%E5%8A%9E/"/>
    <id>http://a-kali.github.io/2020/02/03/SCI写作入门（11）——-实在憋不出来咋办/</id>
    <published>2020-02-03T03:19:11.000Z</published>
    <updated>2020-02-03T03:56:54.403Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><p>本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。</p><p><strong>去哪里“抄”？</strong></p><ul><li>Google 图书、图书馆（书籍）</li><li>维基百科（定义、性质）</li><li>论文文献（Introduction、Methods、Discussion、Figure legend）：抄语法、词组，修改主语等。</li></ul><p><strong>查重</strong>：论文写完后，一定要去做查重。如果被审稿人查出重复率较高会退回重改，影响发表时间。查重服务可以来自于万方等数据库，或者淘宝、闲鱼商家提供的服务。</p><p><strong>语法、词汇差错</strong>：</p><ul><li>易改软件</li><li>英语润色服务</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;p&gt;本文仅为英语和写作水平有限、写作时词穷的同学提供方向，并不提倡论文抄袭。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;去哪里“抄”？&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Google 图书、图书馆（书籍）&lt;/li&gt;
&lt;li&gt;维基百科（定义、性质
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（3）—— Figure Legend</title>
    <link href="http://a-kali.github.io/2020/02/02/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94-Figure-Legend/"/>
    <id>http://a-kali.github.io/2020/02/02/SCI写作入门（3）——-Figure-Legend/</id>
    <published>2020-02-02T13:59:24.000Z</published>
    <updated>2020-02-02T14:07:04.688Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><h1 id="1-图片说明"><a href="#1-图片说明" class="headerlink" title="1    图片说明"></a>1    图片说明</h1><h1 id="2-表格注释"><a href="#2-表格注释" class="headerlink" title="2    表格注释"></a>2    表格注释</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;h1 id=&quot;1-图片说明&quot;&gt;&lt;a href=&quot;#1-图片说明&quot; class=&quot;headerlink&quot; title=&quot;1    图片说明&quot;&gt;&lt;/a&gt;1    图片说明&lt;/h1&gt;&lt;h1 id=&quot;2-表格注释&quot;&gt;&lt;a href=&quot;#2-表格注释&quot;
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>3D点云概念及性质</title>
    <link href="http://a-kali.github.io/2020/02/02/3D%E7%82%B9%E4%BA%91%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95/"/>
    <id>http://a-kali.github.io/2020/02/02/3D点云概念及处理方法/</id>
    <published>2020-02-02T06:03:11.000Z</published>
    <updated>2020-02-03T03:31:51.764Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h1><p><strong>点云与三维图像的关系：</strong>三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以实现天然的物体-背景解耦。点云数据是最为常见也是最基础的三维模型。点云模型往往由测量直接得到，每个点对应一个测量点。</p><p><strong>点云的概念</strong>：点云是在同一空间参考系下表达目标空间分布和目标表面特性的海量点集合，在获取物体表面每个采样点的空间坐标后，得到的是点的集合，称之为“点云”（Point Cloud）。</p><p><strong>点云的内容：</strong>根据激光测量原理得到的点云，包括三维坐标（XYZ）和激光反射强度（Intensity），强度信息与目标的表面材质、粗糙度、入射角方向，以及仪器的发射能量，激光波长有关。<br>根据摄影测量原理得到的点云，包括三维坐标（XYZ）和颜色信息（RGB）。<br>结合激光测量和摄影测量原理得到点云，包括三维坐标（XYZ）、激光反射强度（Intensity）和颜色信息（RGB）。</p><p><strong>点云存储格式：*</strong>.pts; *.asc ; *.dat; .stl ; .imw；.xyz；.las。LAS格式文件已成为LiDAR数据的工业标准格式，LAS文件按每条扫描线排列方式存放数据,包括激光点的三维坐标、多次回波信息、强度信息、扫描角度、分类信息、飞行航带信息、飞行姿态信息、项目信息、GPS信息、数据点颜色信息等。</p><h1 id="点云性质"><a href="#点云性质" class="headerlink" title="点云性质"></a>点云性质</h1><p>点云数据是在欧式空间下的点的一个子集，它具有以下三个特征：</p><ul><li>无序。点云数据是一个集合，对数据的顺序是不敏感的。这就意味这处理点云数据的模型需要对数据的不同排列保持不变性。目前文献中使用的方法包括将无序的数据重排序、用数据的所有排列进行数据增强然后使用RNN模型、用对称函数来保证排列不变性。由于第三种方式的简洁性且容易在模型中实现，论文作者选择使用第三种方式，既使用maxpooling这个对称函数来提取点云数据的特征。</li><li>点与点之间的空间关系。一个物体通常由特定空间内的一定数量的点云构成，也就是说这些点云之间存在空间关系。为了能有效利用这种空间关系，论文作者提出了将局部特征和全局特征进行串联的方式来聚合信息。</li><li>不变性。点云数据所代表的目标对某些空间转换应该具有不变性，如旋转和平移。论文作者提出了在进行特征提取之前，先对点云数据进行对齐的方式来保证不变性。对齐操作是通过训练一个小型的网络来得到转换矩阵，并将之和输入点云数据相乘来实现。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ul><li>[1]<a href="https://blog.csdn.net/hongju_tang/article/details/85008888" target="_blank" rel="noopener">点云概念与点云处理</a></li><li>[2]<a href="https://www.jiqizhixin.com/articles/2019-05-10-13" target="_blank" rel="noopener">PointNet系列论文解读</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;概念&quot;&gt;&lt;a href=&quot;#概念&quot; class=&quot;headerlink&quot; title=&quot;概念&quot;&gt;&lt;/a&gt;概念&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;点云与三维图像的关系：&lt;/strong&gt;三维图像是一种特殊的信息表达形式。和二维图像相比，三维图像借助第三个维度的信息，可以
      
    
    </summary>
    
    
      <category term="3D" scheme="http://a-kali.github.io/tags/3D/"/>
    
      <category term="点云" scheme="http://a-kali.github.io/tags/%E7%82%B9%E4%BA%91/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（2）—— Figure and table</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%882%EF%BC%89%E2%80%94%E2%80%94-Figure-and-table/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（2）——-Figure-and-table/</id>
    <published>2020-02-01T12:13:32.000Z</published>
    <updated>2020-02-02T13:58:41.738Z</updated>
    
    <content type="html"><![CDATA[<p>素材来源：实验万事屋</p><h1 id="文字、图片和表格的合理使用"><a href="#文字、图片和表格的合理使用" class="headerlink" title="文字、图片和表格的合理使用"></a>文字、图片和表格的合理使用</h1><p>图表可以用来表示</p><ol><li>结果：得出的数据、统计图、实验对比等</li><li>方法：流程图、示意图</li></ol><p>图表的分类：</p><ul><li>投稿时：一般要求，在尺寸、分辨率、色彩等方面有一定要求，能让审稿人看清就行；</li><li>发表时：对图片质量要求高，包括图片文字、数字、字号大小、线条粗细等都有具体的要求</li></ul><p>文字表述：</p><ol><li>描述定量化的结果</li><li>用来描述定性数据之间的关系（e.g. 同比上升/下降）</li></ol><p>图片表述：</p><ol><li>用来展示数据组之间的趋势</li><li>描述指标随时间改变的情况</li><li>定量化因素之间的复杂关系</li><li>实验方法介绍（流程图）</li><li>复杂概念介绍</li><li>可视化数据结果（e.g. 细胞图片）</li></ol><p>表格表述：</p><ol><li>展示大量数值型数据</li><li>不同项目之间的细节比较</li><li>复杂定性结果之间比较</li></ol><h1 id="图片"><a href="#图片" class="headerlink" title="图片"></a>图片</h1><h2 id="常见图片类型"><a href="#常见图片类型" class="headerlink" title="常见图片类型"></a>常见图片类型</h2><p>照片、线型图、柱状图、流程图、示意图等</p><h2 id="图片制作过程注意事项"><a href="#图片制作过程注意事项" class="headerlink" title="图片制作过程注意事项"></a>图片制作过程注意事项</h2><ol><li>图标：一个图片对应一个图标。比如说图标a只能对应一个大图中的一个小图。</li><li>标注：箭头、星号；</li><li>缩写：图片标识的时候可以使用缩写，但一定要定义；</li><li>颜色：根据杂志要求选择色彩或灰度模式。</li></ol><h1 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h1><h2 id="表格的要素"><a href="#表格的要素" class="headerlink" title="表格的要素"></a>表格的要素</h2><ol><li>数值；</li><li>列标题和列；</li><li>行标题和行；</li><li>注释和说明。</li></ol><p><img src="https://i.loli.net/2020/02/02/TiNsO81RmtFpVDH.png" alt="_SZ59_91_D_N84__N@2J4F5.png"></p><h2 id="论文表格的一般要求"><a href="#论文表格的一般要求" class="headerlink" title="论文表格的一般要求"></a>论文表格的一般要求</h2><ul><li>单位：一般情况下，单位标识在行标题的后面；</li><li>对比：通常使用横向比较而不是纵向比较；</li><li>顺序：时间顺序、大小顺序、字母顺序；</li><li>空白数据：可以使用”-“表示，或者相应缩写（e.g. ND, Not Done)。</li></ul><h2 id="表格中的线条"><a href="#表格中的线条" class="headerlink" title="表格中的线条"></a>表格中的线条</h2><ol><li>在<strong>列标题上下</strong>需要一条线条；</li><li><strong>最后一行数据下面</strong>需要一条线条；</li><li>列的<strong>副标题下面</strong>需要线条；</li><li>除了以上三种情况以外，其它的都不能使用线条！</li></ol><p><img src="https://i.loli.net/2020/02/01/YlAVc1ryZEhmwz9.png" alt="90U_Y9_G7_QE_SF_U~_413D.png"></p><h2 id="表格中的注释"><a href="#表格中的注释" class="headerlink" title="表格中的注释"></a>表格中的注释</h2><p>表格中出现多个注释的时候，可以使用*、**、#等</p><p><img src="https://i.loli.net/2020/02/02/xOKMlTQfFCY6oHS.png" alt="4~@9__~N~6Z9_PKD21@YF0F.png"></p><h2 id="表格中的数值"><a href="#表格中的数值" class="headerlink" title="表格中的数值"></a>表格中的数值</h2><ul><li>如果有百分比数值最好加上百分比数值；</li><li>如果样本数很多，最好在列标题中标注样本总数；</li><li>如果某一列中所有数据相同，需要在注释中说明指出；</li><li>对齐方式：小数点对齐、左圆括号对齐；</li><li>多次检查。表格中数据多，容易出错。</li></ul><p><img src="https://i.loli.net/2020/02/02/YjQlhKbtek8OpDs.png" alt="RZRPX_R_BVE_`FJM_NLK~R4.png"></p><h2 id="表格中的单位"><a href="#表格中的单位" class="headerlink" title="表格中的单位"></a>表格中的单位</h2><p>一般情况下，单位标识在行标题的后面：</p><p><img src="https://i.loli.net/2020/02/02/a1qHDs5I7zSQMGw.png" alt="_D_~_2_`KGKYO__GVEY__VE.png"></p><p>当个别数据单位不同时，可以备注出来：</p><p><img src="https://i.loli.net/2020/02/02/aeMoRFKJZNuTjPl.png" alt="HNSU37T_G23B_OU8F9TUEGJ.png"></p><p>未完待续~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;素材来源：实验万事屋&lt;/p&gt;
&lt;h1 id=&quot;文字、图片和表格的合理使用&quot;&gt;&lt;a href=&quot;#文字、图片和表格的合理使用&quot; class=&quot;headerlink&quot; title=&quot;文字、图片和表格的合理使用&quot;&gt;&lt;/a&gt;文字、图片和表格的合理使用&lt;/h1&gt;&lt;p&gt;图表可以用来表示
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>SCI写作入门（1）——SCI简介及写作顺序</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94SCI%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%86%99%E4%BD%9C%E9%A1%BA%E5%BA%8F/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（1）——SCI简介及写作顺序/</id>
    <published>2020-02-01T10:30:13.000Z</published>
    <updated>2020-04-14T03:29:03.734Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SCI论文组成"><a href="#1-SCI论文组成" class="headerlink" title="1    SCI论文组成"></a>1    SCI论文组成</h1><ul><li>Title：文章标题</li><li>Abstract：摘要</li><li>Introduction：引言</li><li>Methods：实验所使用的材料和方法</li><li>Results：实验结果</li><li>Discussion：对实验结果的讨论和分析</li><li>Reference：参考文献</li><li>Figures and Tables：图片、图表</li><li>Figure legends：图片、图表说明</li><li>Acknowledgements：致谢</li></ul><h2 id="1-1-Title"><a href="#1-1-Title" class="headerlink" title="1.1    Title"></a>1.1    Title</h2><p>简明、准确地总结文章内容，包含关键词，使文章更容易被所需要的人检索到。</p><h2 id="1-2-Abstract"><a href="#1-2-Abstract" class="headerlink" title="1.2    Abstract"></a>1.2    Abstract</h2><p>通过一段话将你的<strong>研究背景、内容、目的、结果以及研究意义</strong>告诉读者，使读者仅仅通过读Abstract就能明白这篇文献是否是其感兴趣的（或者使读者通过读Abstract就能对你的研究产生兴趣）。是用最少的词表述最重要的内容。</p><h2 id="1-3-Introduction"><a href="#1-3-Introduction" class="headerlink" title="1.3    Introduction"></a>1.3    Introduction</h2><p>Introduction是比较难写的一部分。在Introduction中，作者需要比Abstract更为详细地回答以下五个问题：</p><ol><li><p>为什么——为什么要做这个研究？</p></li><li><p>是什么——你的<strong>科学假说</strong>是什么？</p></li><li><p>做什么——你的<strong>研究方法</strong>是什么？</p></li><li><p>什么结果——你的<strong>研究发现</strong>是什么？</p></li><li><p>什么意义——为什么你的研究很重要？</p></li></ol><h2 id="1-4-Methods"><a href="#1-4-Methods" class="headerlink" title="1.4    Methods"></a>1.4    Methods</h2><p>详细叙述研究采用的方法，能够让读者参考该文献复现出实验研究。</p><h2 id="1-5-Results"><a href="#1-5-Results" class="headerlink" title="1.5     Results"></a>1.5     Results</h2><p>开门见山地告诉读者你发现了什么现象、得出了什么数据和结论。同时在这一部分，作者需要灵活地使用图片、表格等方式更加形象地展示研究成果。</p><h2 id="1-6-Discussion"><a href="#1-6-Discussion" class="headerlink" title="1.6    Discussion"></a>1.6    Discussion</h2><p>Discussion同样是比较难写的一部分。作者需要对自己的研究进行总结和归纳；同时需要讨论该研究与相关研究的关系，其结果是否具有一致性，如何去解释不一致性的产生；最后还要说明自己研究的局限性以及如何去改进这种局限性。提出研究和价值。<strong>Discussion直接决定了整篇文章的质量，对文章能否顺利发表有很大的影响</strong>。</p><h2 id="1-7-Reference"><a href="#1-7-Reference" class="headerlink" title="1.7    Reference"></a>1.7    Reference</h2><ul><li>列举出参考文献，为问题提供背景，为方法提供出处，为结论提供证据。 </li><li>对前人工作和观点的一种尊重和赞同。</li><li>为读者提供更多的信息来源。</li></ul><h1 id="2-SCI论文合理写作顺序"><a href="#2-SCI论文合理写作顺序" class="headerlink" title="2    SCI论文合理写作顺序"></a>2    SCI论文合理写作顺序</h1><ol><li>建议首先制作论文图片和表格，表格和图片就是论文的骨架！完成了图片和表格后，整个文章的基本框架便在心里有数。</li><li>完成表格和图片的说明（Figure Legend），趁热打铁。</li><li>完成论文的Results。一个图片/表格对应一个小结，合起来就是整个Results。</li><li>完成论文的Introduction。对于一件你已经完成的事情，写Introduction来解释你为什么要做这件事情。</li><li>完成Discussion。相当于Introduction的后续，对提出的工作进行总结。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-SCI论文组成&quot;&gt;&lt;a href=&quot;#1-SCI论文组成&quot; class=&quot;headerlink&quot; title=&quot;1    SCI论文组成&quot;&gt;&lt;/a&gt;1    SCI论文组成&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Title：文章标题&lt;/li&gt;
&lt;li&gt;Abstract：摘
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Detecting Depression with AI (AVEC2019)</title>
    <link href="http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/"/>
    <id>http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/</id>
    <published>2020-02-01T02:45:56.000Z</published>
    <updated>2020-03-17T15:02:36.976Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1907.11510v1" target="_blank" rel="noopener">AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression with AI, and Cross-Cultural Affect Recognition</a></p><h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1    Abstract"></a>1    Abstract</h1><p>视听情感挑战与工作坊(AVEC 2019)“心智状态、人工智能检测抑郁、跨文化情感识别”是第九届比赛，旨在将多媒体处理和机器学习方法用于自动视听健康和情感分析，所有参与者在相同的条件下严格竞争。该挑战的目标是为多模态信息处理提供一个通用的基准测试集，并将健康和情绪识别社区以及视听处理社区集合在一起，以比较现实生活数据中各种健康和情绪识别方法的相对优点。本文介绍了今年的主要创新点、挑战指南、使用的数据和基线系统在三个拟议任务上的表现:心理状态识别、人工智能抑郁评估和跨文化情感感知。</p><h1 id="2-Depression-Detection-with-AI"><a href="#2-Depression-Detection-with-AI" class="headerlink" title="2    Depression Detection with AI"></a>2    Depression Detection with AI</h1><p>抑郁症，尤其是重度抑郁症( major depressive disorder, MDD)，是一种常见的心理健康问题，对人的思维、感觉和行为方式有负面影响。它会导致各种情绪和身体问题，影响工作和个人生活的许多方面。世界卫生组织(WHO)在2015年宣布抑郁症是全球范围内导致疾病和残疾的主要原因:超过3亿人患有抑郁症。鉴于抑郁症的高患病率及其自杀风险，寻找新的诊断和治疗方法变得越来越重要。由于有令人信服的证据表明抑郁症和相关的精神健康障碍与行为模式的改变有关，人们越来越有兴趣使用自动人类行为分析来基于行为线索(如面部表情和说话韵律)进行计算机辅助抑郁症诊断。<strong>面部活动、手势、头部运动和表达能力</strong>等行为信号都与抑郁症密切相关。</p><p>计算机视觉可以追踪的面部表情和头部动作也是预测抑郁的好方法。据报道，更向下的凝视角度、不那么强烈的微笑、更短的平均微笑持续时间是抑郁症最显著的面部特征。此外，身体表情、手势、头部动作和语言线索也被报道为抑郁检测提供相关线索。综合所有这些证据，有人提议将情感计算技术集成到一个计算机代理中，该代理可以访问人们并识别精神疾病的语言和非语言指标。对创伤后应激障碍患者收集的数据表明，当代理人由充当WoZ的人驱动时，对其抑郁严重程度的自动评估可以实现RMSE小于5；<strong>PHQ-8 range∈[0,24] 的cutpoint分别定义为轻度、中度、中度和重度抑郁症</strong>。这些结果需要进一步研究，因为代理完全由人工智能驱动，因为向导可能会将虚拟代理驱动到一种情况，从而减轻与抑郁症相关的模式的观察，或者自主代理可能在适当地进行访谈方面存在问题。</p><h1 id="3-Distress-Analysis-Interview-Corpus"><a href="#3-Distress-Analysis-Interview-Corpus" class="headerlink" title="3    Distress Analysis Interview Corpus"></a>3    Distress Analysis Interview Corpus</h1><p>扩展遇险分析访谈语料库(E-DAIC)是WOZ-DAIC的扩展版本，包含半临床访谈，旨在支持诊断焦虑、抑郁和创伤后应激障碍等心理困扰状况。收集这些访谈是为了创建一个计算机代理来采访人们，并识别精神疾病的语言和非语言指标。收集的数据包括音频和视频记录、使用谷歌云语音识别服务自动转录的文本以及广泛的问卷回答。这些面试是由一个叫做Ellie的动画虚拟面试官进行的。在theWoZ的面试中，虚拟代理由另一个房间的人类面试官(巫师)控制，而在AI的面试中，代理以完全自主的方式使用不同的自动感知和行为生成模块。</p><p>为了达到挑战的目的，E-DAIC数据集被划分为训练、开发和测试集，同时保留了演讲者的整体多样性——在年龄、性别分布和8项患者健康问卷(PHQ8)评分方面——在这些划分内。训练和开发集包括WoZ和人工智能场景的混合，而测试集仅由自主人工智能收集的数据构成。关于扬声器在分区上的分布的详细信息见表2。</p><p><img src="https://i.loli.net/2020/02/01/teQ2Za9kod7L4M1.png" alt="K_UZ`K8@_ADCD0_THOI_~35.png"></p><h1 id="4-Baseline-Features"><a href="#4-Baseline-Features" class="headerlink" title="4    Baseline Features"></a>4    Baseline Features</h1><p>视听信号的情感识别通常依赖于特征集，这些特征集的提取基于近几十年来在视听信号处理领域的研究出来的特定技术，在语音方面有：<strong>梅尔倒频谱系数(Mel Frequency Cepstral Coefficients, MFCCs)</strong>；视觉方面则有：<strong>面部活动单元(Facial Action Units, FAUs)</strong>。</p><h2 id="4-1-Expert-knowledge"><a href="#4-1-Expert-knowledge" class="headerlink" title="4.1    Expert-knowledge"></a>4.1    Expert-knowledge</h2><p>影响感知的传统方法是在固定时间内，通过一组滑动窗口计算的统计度量，来总结随时间变化的视听信号<strong>低水平描述符(low-level descriptors, LLDs)</strong>。</p><p>在音频方面，我们计算了<strong>扩展GeMAPs特征集(extended Geneva Minimalistic Acoustic Parameter Set, eGeMAPS)</strong>，它包含88个覆盖上述声学维度，并在这里用作基线。除此之外，我们使用OpenSMILE工具包使MFCCs 1-13 的一阶和二阶差分被计算为一组声学LLDs。在视觉方面，我们使用openFace工具包提取每个视频帧的17个FAU强度，以及一个置信度。此外，还提取了姿态(pose)和凝视(gaze)的描述符。</p><h2 id="4-2-Bags-of-Words"><a href="#4-2-Bags-of-Words" class="headerlink" title="4.2    Bags-of-Words"></a>4.2    Bags-of-Words</h2><p><strong>词袋(bags-of-words, BoW)</strong>技术起源于文本处理，它代表了LLD的分布。我们使用MFCCs和eGeMAPs特征集作为声学数据，FAU的强度作为视频数据。MFCCs和eGeMAPS的LLD是经过标准化处理的。</p><p>为了生成BoW表示，声学和视觉特征都在长度为4s的音视频段上进行处理，对于SEWA数据集的每一段为100ms，对于USoM和E-DAIC数据集为1s。随机采样示例来构建字典，并从结果项频率中取对数以压缩它们的范围。整个跨模式BoW(XBoW)处理链是使用开源工具包openXBOW执行的。</p><h2 id="4-3-Deep-Representations"><a href="#4-3-Deep-Representations" class="headerlink" title="4.3    Deep Representations"></a>4.3    Deep Representations</h2><p>在去年的挑战中，我们将深度频谱特征(Deep Spectrum features)作为一种基于深度学习的音频基线特征表示。深度频谱特征的灵感来自于图像处理中常见的深度表示学习范式：将语音实例的频谱图像输入到预训练好的CNNs中，提取一组由此产生的激活值作为特征向量。</p><p>在今年的挑战中，我们使用VGG-16、AlexNet、DenseNet-121和DenseNet-201四个鲁棒的预训练模型中提取了深度频谱特征；在AVEC 2019 CES上使用AlexNet纯粹是为了与之前的AVEC 2018 CES保持一致。语音文件首先被转换成具有128个mel频段的mel谱图图像，所有挑战语料库的窗口宽度为4s, USoM和E-DAIC数据集的跳数为1s, SEWA数据集的跳数为100ms。随后，基于频谱的图像通过预先训练的网络进行转发。然后，通过激活VGG-16和AlexNet中的第二个全连接层形成4096维的特征向量，通过激活DenseNet-121和DenseNet-201网络的最后一个平均池化层分别得到1024和1920维的特征向量。</p><p>我们还提供了两个基线深度视觉表示，分别使用了VGG-16网络和ResNet-50网络，这两个网络都是使用Affwild数据集进行预训练的。首先应用openFace工具包来检测脸部区域，然后执行面部对齐。然后，我们冻结两个预先训练的模型的权重，并分别将对齐的脸部图像输入两个CNN。为了获得每个帧的深度表示，我们分别从预训练的VGG-16网络中提取第一个全连接层的输出，从预训练的ResNet-50网络中提取全局平均池化层的输出。因此，每一帧都提供了来自VGG的4096维深特征向量和来自ResNet的2048维深特征向量。</p><h1 id="5-Baseline-System"><a href="#5-Baseline-System" class="headerlink" title="5    Baseline System"></a>5    Baseline System</h1><p>对于抑郁检测基线，我们使用单层64-d GRU作为我们的递归网络，其失步正规化率为20%，然后使用64-d全连通层获得单值回归评分。为了处理偏差，我们将PHQ-8分数标签转换为浮点数，方法是在培训之前按25的倍数缩小比例。使用CCC损失函数和评价分数对网络进行训练和评价，使用原始的PHQ量表报告RMSE结果。批处理大小为15的方法得到了一致的使用，并且在不同的特性集之间优化了学习率。为了使数据适合GPU内存，为会话分配了最大的序列长度。对于MFCCs和eGeMAPS LLDs，以及诸如DeepSpectrum、ResNet和VGG等高维深表示，使用的最大序列长度为20分钟。另外，对于ResNet、VGG和深度光谱表示帧，根据维数的不同，将保留两帧中的一帧或四帧中的一帧，以便将数据加载到内存中。融合不同的视听表现是通过平均他们的分数来实现的。</p><p>DDS的基线结果见表6。结果表明，在开发集上，利用深度谱(DS-VGG)特征获取音频特征的最佳CCC评分，利用ResNet特征获取视觉特征的最佳CCC评分。这些结果表明表达的力量深层神经网络学习的大量数据时在不同的上下文中使用他们最初的设计,这是证实与ResNet视觉模型在测试集上实现最好的结果,尽管相对较低的CCC。</p><p>不同表现形式的融合在开发集上获得了最好的结果，测试集上返回的RMSE比使用AVEC 2017基线系统在DAIC-WoZ数据集上获得的RMSE稍好一些;AVEC2019年的RMSE=6.37，而AVEC 2017年的RMSE=6.97。然而，为今年的挑战开发的基线系统更加复杂，与今年的GRU-RNNs相比，它是一个简单的线性回归模型，因此，根据2017年AVEC抑郁亚挑战的最佳结果(RMSE=4.99)，应该最好地考虑相应的分数。</p><p>根据从与虚拟代理的交互中获得的抑郁程度自动感知的结果，当代理仅由人工智能驱动时，识别似乎比由人作为WoZ驱动时更具挑战性。这一观察结果为设计抑郁症诱因的设计带来了有趣的研究问题。通过强化学习，根据agent的交互方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1907.11510v1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AVEC 2019 Workshop and Challenge: State-of-Mind, Detecti
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
</feed>
