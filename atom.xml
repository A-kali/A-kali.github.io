<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2021-02-23T09:00:44.009Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BoTNet简述</title>
    <link href="http://a-kali.github.io/2021/02/23/BoTNet%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2021/02/23/BoTNet简述/</id>
    <published>2021-02-23T08:59:55.000Z</published>
    <updated>2021-02-23T09:00:44.009Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/2101.11605" target="_blank" rel="noopener">Bottleneck Transformers for Visual Recognition</a></p><p>BoTNet是CV Transformer领域的创新之作，开启了一种CV Transformer backbone新范式。之前比较受认可的backbone网络ViT是在Transformer的结构中进行更适合图像的改进，而BoTNet则是在原始的CNN中进行Attention相关的改进。</p><p><img src="https://i.loli.net/2021/02/13/GUztqQFwTR726Iu.png" alt="image.png"></p><p>其主要思路非常简单，仅仅是<strong>使用改进后的Multi-Head Self Attention (MHSA)对ResNet最后三个bottleneck blocks中的卷积操作进行替换</strong>。如下图所示：</p><p><img src="https://i.loli.net/2021/02/16/wCMnSZPVbUh1Ntp.png" alt="image.png"></p><p>下图为BoT模块中使用的MHSA层。该层一共有四个head，在二维特征图上执行all2all attention，高度和宽度分别使用<strong>相对位置编码(relative position encodings)</strong>Rh和Rw来表示，使Rh和Rw中的每个元素分别进行求和得到对应于特征图中每个像素的<strong>内容位置(content-position)编码</strong>。剩下的和原始的NLP Multi-Head Self Attention差不多。</p><p><img src="https://i.loli.net/2021/02/13/PHVW7fGR4IYsy9r.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2101.11605&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bottleneck Transformers for Visual Recognition&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
      <category term="Transformer" scheme="http://a-kali.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformers in Vision</title>
    <link href="http://a-kali.github.io/2021/02/23/Transformers-in-Vision/"/>
    <id>http://a-kali.github.io/2021/02/23/Transformers-in-Vision/</id>
    <published>2021-02-23T08:57:54.000Z</published>
    <updated>2021-02-23T08:58:48.023Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="http://xxx.itp.ac.cn/pdf/2101.01169v1" target="_blank" rel="noopener">Transformers in Vision: A Survey</a></p><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Transformer模型在NLP任务中的惊人表现激起了视觉界对其在CV中应用与研究的兴趣，这在许多任务上带来了令人兴奋的进展。本综述的目的是提供一个Transformer在计算机视觉领域的全面概述该。</p><p>我们首先介绍Transformer成功背后的基本概念，即自监督(self-supervision)和自注意力(self-attention)机制。Transformer架构利用自注意力机制在对输入进行编码获取<strong>长程依赖关系(long-range dependencies)</strong>，这使得它们具有较强的表达能力。</p><p>研究者们假定对问题的结构缺乏先验知识，在前置任务(pretext tasks)上使用Transformer模型作在大规模未标记数据集上的自监督地进行预训练。然后，在下游任务(downstream tasks)上对学习到的表示进行微调。由于编码后的特征的具有较强的泛化性和表现力，通常可得到出色的性能。</p><p>本文将涵盖Transformer在CV领域中的主流方向，包括：</p><ul><li>识别任务（图像分类、目标检测、动作识别、分割）；</li><li>生成模型；</li><li>多模态任务（视觉问答、视觉推理和视觉定位）；</li><li>视频处理（活动识别、视频预测）；</li><li>低级视觉（图像超分辨率、图像增强和彩色化）；</li><li>3D分析（点云分类和分割）。</li></ul><p>我们从结构设计和实验结果两方面比较了当前主流技术各自的优势和局限性。最后，对开放研究的方向和未来可能的工作进行了分析，希望能激发社会对Transformer在CV领域的兴趣。</p><blockquote><p>low-level version: which concerns the extraction of image properties from the retinal image.<br>低级视觉：主要关注的是从视网膜图像中提取图像的特性。</p></blockquote><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1    Introduction"></a>1    Introduction</h1><p>最近Transformer模型在多种语言任务中表现优异，例如文本分类、机器翻译和问答系统。在这些模型中，以BERT、GPT、RoBERTa和T5最为流行。Transformer模型的可扩展性对大型模型产生了极其深远的影响。</p><p>Transformer在NLP领域的突破引起了CV社区在多模态学习任务中应用该模型的兴趣。最终，Transformer被成功应用于图像识别、目标检测、分割、图像超分辨率、视频理解、图像生成、视觉问答等方向。这篇综述旨在涵盖最近这些方面的研究成果，给感兴趣的读者一个综合性的理解。</p><p>Transformer的成功因素主要包括自监督和自注意力。在大型数据集上的自监督能够在不消耗标注成本的情况下训练出复杂的模型，对给定数据集中实体之间有用的关联进行编码，得到抽象的表示。这点十分重要，因为与其他形式的深度学习模型（如卷积神经网络和递归神经网络）相比，自注意力机制能够得到最小的归纳偏置(Inductive bias)。自注意层通过学习元素集之间的关系（比如语言中单词之间的关系，图像中像素集之间的关系），以给定的顺序考虑广泛的上下文。在本综述中，我们首先介绍了Transformer中使用的这些重要概念，然后详细介绍了最近视觉Transformer的发展现状。</p><blockquote><p>CNN的inductive bias应该是locality和spatial invariance，即空间相近的grid elements有联系而远的没有，和空间不变性（kernel权重共享）</p><p>RNN的inductive bias是sequentiality和time invariance，即序列顺序上的timesteps有联系，和时间变换的不变性（rnn权重共享）</p></blockquote><h1 id="2-Foundations"><a href="#2-Foundations" class="headerlink" title="2    Foundations"></a>2    Foundations</h1><p>有两个思想对Transformer的发展起到了至关重要的作用。第一种是自监督，用于在大型无标记语料库上预训练Transformer模型，然后应用于小的有标记数据集。第二个关键思想是自注意力，它能够捕获长程信息和序列元素之间的依赖关系，而传统的RNN模型很难做到这一点。下面，我们提供了关于这两个思想的简要教程，以及开创性应用这些思想的Transformer网络的摘要。这个背景将帮助我们更好地理解即将在计算机视觉领域中使用的基于Transformer的模型。</p><h2 id="2-1-Self-supervision"><a href="#2-1-Self-supervision" class="headerlink" title="2.1    Self-supervision"></a>2.1    Self-supervision</h2><p><strong>自监督学习(Self-supervised learning, SSL)</strong>是与Transformer一起使用的核心概念，用于从大规模的未标记数据集学习。一种SSL基本思想是<strong>填空(fill in the blanks)</strong>，例如预测数据图像中被遮挡的部分，预测时序视频的上一帧或下一帧；亦或者对前置任务进行预测：比如输入图像应用了多少旋转度数，对原图中划分出来的图像补丁进行排列，给灰度图上色。</p><p>另一种自监督约束的方法是通过<strong>对比学习(contrastive learning)</strong>，该方法包含两种类型的转换。第一种转换不改变图像基本语义（如风格迁移、图像裁剪），另一种转换是对语义的变化（例如将一个物体置于另一个场景，或对图像使用对抗方法稍微修改图像的类别）。随后，对模型进行训练，使其不受转换影响，并注重对可能改变语义标签的微小变化进行建模。</p><p>自监督学习提供了一种很有前景的学习模式，因为它可以从大量现成的无注释数据中进行学习。SSL的执行分为两个阶段：首先，通过解决前置任务、训练模型来学习底层数据中有意义的表示。前置任务的伪标签是根据数据属性和任务定义自动生成的(不需要任何昂贵的手工注释)。在第二阶段，使用标记数据对第一阶段训练过的模型在下游任务上进行微调。下游任务包括图像分类、目标检测和动作识别等。</p><p>SSL的核心是定义前置任务。因此，我们可以根据前置任务将现有的SSL方法广泛地分类为合成图像或视频的<strong>生成方法</strong>，利用图像补丁或视频帧之间的关系的<strong>基于上下文的方法</strong>，以及利用多种数据模态的<strong>跨模态方法</strong>。生成方法的例子包括条件生成任务，如图像着色、超分辨率、未完成的图像、以及基于GANs的方法。基于上下文的方法包括图像补丁拼图、预测几何变换、核查视频帧的时间序列等。跨模态方法检查两种输入模态的对应关系。</p><h2 id="2-2-Self-Attention"><a href="#2-2-Self-Attention" class="headerlink" title="2.2    Self-Attention"></a>2.2    Self-Attention</h2><p>自注意机制是Transformer的一个组成部分，它对序列的所有实体之间进行交互建模。自注意力层通过聚合完整输入序列的全局信息来更新序列的每个组成部分。</p><p>自注意力通过定义三个可学习的三个权重矩阵Q, K , V分别对Queries, Keys, Values进行转换。对于序列中的给定实体，自注意力计算所有键和查询的点积，然后使用softmax操作对其进行归一化，得到注意力分数。然后，每个实体成为序列中所有实体的加权和，权重由注意力分数计算得到。</p><p><strong>Masked Self-Attention</strong>：标准的自我注意层关注所有的实体。对于训练用来预测序列下一个实体的Transformer来说，其解码器中使用Masked Self-Attention，对预测元素后方的实体进行遮挡，以防止关注序列后方的未来实体。</p><h2 id="2-3-Multi-Head-Attention"><a href="#2-3-Multi-Head-Attention" class="headerlink" title="2.3    Multi-Head Attention"></a>2.3    Multi-Head Attention</h2><p>为了得到序列中不同位置之间的多种复杂关系，Multi-Head Attention模块由多个自注意力模块组成，每个Multi-Head Attention模块都独立地包含一组Q, K , V权重矩阵。</p><p>自注意力与卷积操作的主要区别是，其权重是动态计算的，而不是像卷积中那样静态的权重（对任何输入都保持不变）。此外，自注意能保持输入点的排列和数量不变。因此，与需要网格结构的标准卷积相比，它可以轻松地应用于不规则的输入上。</p><h2 id="2-4-Transformer-Model"><a href="#2-4-Transformer-Model" class="headerlink" title="2.4    Transformer Model"></a>2.4    Transformer Model</h2><p><img src="https://i.loli.net/2021/01/23/94nurIVURWOKyXQ.png" alt="image.png"></p><p>Transformer的结构如图所示。它有一个Encoder-Decoder结构。Encoder由六个相同的层组成，每一层有两个子层：一个Multi-Head Attention，和一个对位置敏感的(position-wise)全连接层。在每一层之后使用残差连接(Residual connections)和层归一化(layer normalization)。注意，不同于一般的卷积网络特性聚合和特征转换同时执行(例如卷积层后跟一个非线性)，这两个步骤在Transformer中是解耦的，即self-attention层只执行聚合而前馈层执行转换。与Encoder类似，Decoder由六个相同的层组成。每个Decoder层有三个子层，前两个子层(多头自注意，前馈)类似于Encoder，第三个子层对相应Encoder层的输出进行多头自注意力处理。</p><h2 id="2-5-Bidirectional-Representations"><a href="#2-5-Bidirectional-Representations" class="headerlink" title="2.5    Bidirectional Representations"></a>2.5    Bidirectional Representations</h2><p>原Transformer模型的训练策略只能关注句子中给定单词左侧的上下文。而在大多数语言任务中，左右两边的上下文信息都很重要。BERT(Bidirectional Encoder Representations from Transformers)的提出对句子的左右上下文进行联合编码，以无监督的方式学习文本数据的特征表示。为了实现双向训练，其引入了两个前置任务：</p><ul><li>掩码语言模型(Masked Language Model, MLM)：即在句子中随机屏蔽15%的固定比例的词，并使用交叉熵损失对模型进行训练以预测这些屏蔽词。在预测掩蔽词时，该模型学习结合双向语境；</li><li>预测下一个句子(Next Sentence Prediction, NSP)：该模型预测一个二值标签，即这对句子是否在原文档中相邻。该训练数据可以很容易地从任何文本语料库中生成。这样就形成了一对句子A和B，有50%概率B是真实的句子(在A旁边)，否则B是一个随机的句子。NSP使模型能够捕获句子到句子的关系，这在许多语言建模任务中是至关重要的。</li></ul><h1 id="3-Transformers-amp-Self-Attention-in-Vision"><a href="#3-Transformers-amp-Self-Attention-in-Vision" class="headerlink" title="3    Transformers &amp; Self-Attention in Vision"></a>3    Transformers &amp; Self-Attention in Vision</h1><p><img src="https://i.loli.net/2021/01/24/KgJjw1BaUnWmcA4.png" alt="image.png"></p><p>上图概述了Transformer在CV领域中的主要应用。我们将以任务类型进行分组分别解释这些研究方向。</p><h2 id="3-1-Transformers-for-Image-Recognition"><a href="#3-1-Transformers-for-Image-Recognition" class="headerlink" title="3.1    Transformers for Image Recognition"></a>3.1    Transformers for Image Recognition</h2><p>卷积运算是计算机视觉中传统深度神经网络的主力军，它解决了复杂图像分类等问题。但是，<strong>卷积也有缺点：</strong></p><ol><li><strong>卷积在固定大小的窗口上运行，无法捕获长程依赖性，例如视频中不同时域的像素之间的联系；</strong></li><li><strong>卷积滤波器权重在训练后保持固定，因此操作无法动态适应输入的任何变化。</strong></li></ol><p>在本节中，我们将介绍如何通过使用Self-attention和Transformer网络来缓解常规神经网络中上述问题。 有两种主要的Self-attention设计方法：</p><ol><li>不受输入要素大小限制的全局Self-attention； </li><li>局部Self-attention对给定邻域内的关系建模。</li></ol><p>最近，NLP Transformer编码器直接在图像块上成功应用了全局Self-attention，从而减少了对手工网络设计的需求。 但Transformer本质上是数据密集型的，像ImageNet这样的大型数据集都不足以完整地训练CV Transformer，有研究指出可以将教师CNN的知识提炼为给学生Transformer，这使得ImageNet已经足够完成Transformer训练。 </p><h3 id="3-1-1-Non-local-Neural-Networks"><a href="#3-1-1-Non-local-Neural-Networks" class="headerlink" title="3.1.1    Non-local Neural Networks"></a>3.1.1    Non-local Neural Networks</h3><p>非局部神经网络(Non-local Neural Networks)的方法的灵感来自于非局部均值运算(non-local means operation)，该运算主要用于图像去噪。 此操作使用图像中其他像素值的加权和来修改给定像素。 但是，它根据小块之间的相似度选择任意距离的像素作为滤波器返回值，而不是像素周围固定大小的窗口。而非局部运算模型能够对图像空间中的长期依赖关系进行建模。基于此，Wang等人提出了<strong>基于深度神经网络的可微非局部运算（1×1卷积+高斯相似性计算），通过前馈的方式捕获空间和时间上的长程依赖性。非局部运算能够捕获特征图中任意两个位置之间的关联，而不考虑它们之间的距离。</strong>视频分类是像素之间在空间和时间上存在远距离相互关联的一个例子。</p><blockquote><p>非局部均值运算(non-local means operation)与Self-attention十分类似，其核心思想是在计算每个像素位置对应的输出的时候，不再只和邻域计算，而是和图像中所有位置计算相关性，然后将相关性作为一个权重表示其他位置和当前待计算位置的相似度。可以简单认为采用了一个和原图一样大的kernel进行卷积计算。</p><p>但是实际上如果采用逐点计算方式，不仅计算速度非常慢，而且抗干扰能力不太好，故非局部均值运算实际上是计算图像子图和子图之间的相关性。该方法可以用于计算视频流中不同帧图像的子图相关性，所以可用于目标追踪等领域。</p><p>该方法的缺点是完全忽略了像素、子图之间的距离。</p></blockquote><h3 id="3-1-2-Criss-cross-Attention"><a href="#3-1-2-Criss-cross-Attention" class="headerlink" title="3.1.2    Criss-cross Attention"></a>3.1.2    Criss-cross Attention</h3><p>虽然Self-attention机制能够对全图像的上下文信息进行建模，但这个过程对存储和算力的要求都很高。如下图(a)所示，为了对给定像素位置的全局上下文进行编码，非局部子图需要计算大量的注意力关联（图中绿色部分），复杂度高达$O(N^2)$，其中N为输入特征映射的个数。为了减少计算负担，Huang等人提出了<strong>交叉注意模块(criss-cross attention module)，对于每个像素位置只在交叉路径上生成稀疏的注意力权重</strong>，如下图(b)所示。此外，<strong>通过反复应用交叉注意力，每个像素位置可以从所有其他像素捕获上下文</strong>。与非局部模块相比，交叉注意力模块使用的显存仅为1/11，复杂度为$O(2\sqrt N)$。</p><p><img src="https://i.loli.net/2021/01/26/p3nLMkT24v7SoVg.png" alt="image.png"></p><h3 id="3-1-3-Stand-alone-Self-Attention"><a href="#3-1-3-Stand-alone-Self-Attention" class="headerlink" title="3.1.3    Stand-alone Self-Attention"></a>3.1.3    Stand-alone Self-Attention</h3><p>如上所述，卷积层具有<strong>平移同变性(translation equivariance)</strong>，但不能对感受野进行缩放，因此不能捕捉长程关联。Ramachandran等人提出用<strong>局部自注意层(local self-attention layer)</strong>代替深度神经网络中的卷积层，该层可以对感受野进行缩放，并且不增加计算成本。在基本层面上，自我注意层考虑给定像素周围特定窗口大小的所有像素位置，计算这些像素的query、key和value，然后聚合该窗口内的空间信息。将query和key的softmax分数投影后，将value向量进行聚合。对所有给定像素重复此过程，并将输出像素连接起来。</p><blockquote><p>卷积层的平移同变性：当图像中的物体的位置移动后，该图像再输入卷积层将输出与原图不同的表示。</p><p>Stand-alone Self-Attention虽然可以缩放感受野，但损失了平移同变性。</p></blockquote><h3 id="3-1-4-Local-Relation-Networks"><a href="#3-1-4-Local-Relation-Networks" class="headerlink" title="3.1.4    Local Relation Networks"></a>3.1.4    Local Relation Networks</h3><p>卷积操作的另一个缺点是权重在训练后保持固定，而不考虑输入的任何变化。Hu等人提出在局部窗口中自适应合成像素。他们引入了一个新的可微层（如下图），该层<strong>基于局部窗口内像素/特征之间的关联（相似性）来自适应其权重</strong>。这种自适应权重将几何先验引入到网络中，这对识别任务很重要。卷积被认为是一种自上而下的操作，因为它在不同位置上保持固定，而非局部的运算；该方法中的局部关系层(local relation layer)属于自底向上方法的范畴，但它被限制在一个固定的窗口大小。</p><p><img src="https://i.loli.net/2021/01/26/2jGgIyf5zwWJXLc.png" alt="image.png"></p><h3 id="3-1-5-Attention-Augmented-Convolutional-Networks"><a href="#3-1-5-Attention-Augmented-Convolutional-Networks" class="headerlink" title="3.1.5    Attention Augmented Convolutional Networks"></a>3.1.5    Attention Augmented Convolutional Networks</h3><p>Bello等人探讨使用Self-attention替代卷积的可能性。他们提出<strong>在二维中使用相对位置编码(relative position encoding)来开发一种新的Self-attention机制来保持平移同变性</strong>。大量的实验表明，对于各种现有的架构，注意力增强可以使得图像分类和目标检测系统的性能提高。</p><h3 id="3-1-6-Vectorized-Self-Attention"><a href="#3-1-6-Vectorized-Self-Attention" class="headerlink" title="3.1.6    Vectorized Self-Attention"></a>3.1.6    Vectorized Self-Attention</h3><p>Zhao等人注意到传统的卷积操作通过一个卷积核和一个非线性层来共同进行特征聚合和转换。他们提出将聚合和转换分离，使用self-attention来进行特征聚合，然后使用元素感知器层(element-wise perceptron layer)进行转换。为此，他们提出了两种可供选择的特征聚合策略：成对的self-attention和成片的self-attention。</p><h3 id="3-1-7-Vision-Transformer"><a href="#3-1-7-Vision-Transformer" class="headerlink" title="3.1.7    Vision Transformer"></a>3.1.7    <a href="https://zhuanlan.zhihu.com/p/266311690" target="_blank" rel="noopener">Vision Transformer</a></h3><p>ViT(Vision Transformer)是第一项展示Transformer如何在大规模计算机视觉数据集上“完全”替代深度神经网络中的标准卷积的研究。他们在图像数据上应用了原始的Transformer模型（与用于NLP任务的版本相比变化极小）。Transformer在谷歌收集的大量的图像数据集上进行了预训练，然后对下游识别基准（如ImageNet）进行微调。这是一个重要的步骤，因为使用中等大小数据集对ViT进行预训练得到最先进的结果。这是因为CNN对图像域中的先验知识进行编码，减少了对数据的需求；而<strong>Transformer必须从大规模的数据集中发现这些知识</strong>。为此作者使用了包含3亿的图像的JFT数据集对模型进行预训练，使其性能提高到最先进的CNN模型的水准。值得注意的是，iGPT模型也将Transformer应用于完整尺寸的图像，但将训练作为生成任务执行，而ViT使用监督分类任务对模型进行预训练（尽管也探索了自监督变体，导但效果较差）。</p><p>模型的主要架构（如下图）非常类似于语言Transformer。与一维语言嵌入序列不同，二维图像分割为多个子图，每个子图都平铺成一个向量，以序列的形式输入Transformer。然后利用线性映射将这些向量化的子图映射为子图嵌入(patch embedding)，并附加位置嵌入(position embedding)用来编码位置信息。重要的是，一个[cls]符号（类标记）附加在Transformer输入的开头，相对应的第一个位置的输出用作图像的全局表示，反应整张图的信息。这个类别表示最终会被MLP被分为一个具体类别。</p><p><img src="https://i.loli.net/2021/01/27/2QrSRUmwdalpjix.png" alt="image.png"></p><h3 id="3-1-8-Data-efficient-Image-Transformers"><a href="#3-1-8-Data-efficient-Image-Transformers" class="headerlink" title="3.1.8    Data-efficient Image Transformers"></a>3.1.8    <a href="https://mp.weixin.qq.com/s?__biz=MzU4OTg3Nzc3MA==&amp;mid=2247484771&amp;idx=1&amp;sn=d8e3fa7a86b9ae1316ae831bc3270b12&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">Data-efficient Image Transformers</a></h3><p>DeiT(Data-efficient Image Transformers)没有使用任何外部的大规模数据集，这证明了与精心调整的CNN设计相比，Transformer更具潜力。由于与CNN设计不同，Transformer体系结构并不采取图像结构的先验知识，这通常会导致更长的训练时间，并且需要更大的数据集。然而，DeiT演示了如何在中等规模的数据集。除了使用CNN中常见的数据增强和正则化以外，最主要的是一种新颖的Transformer自我蒸馏(native distillation)方法。</p><p>蒸馏过程使用RegNetY-16GF作为教师模型，其输出用于训练Transformer。CNN的输出有助于Transformer有效地为输入图像找出有用的表示。子图嵌入前后分别被附加类标记（见3.1.7）和蒸馏标记。Self-Attention层对这些标记进行操作，获取它们的相互依赖性，并输出所学到的类、子图和蒸馏标记。模型使用定义在输出类标记上的交叉熵损失和匹配蒸馏标记与教师输出的蒸馏损失来训练网络。作者对蒸馏的软标签和硬标签的选择进行了探索，发现硬标签的性能更好。有趣的是，所学习的类和蒸馏标记并没有表现出高相关性，这表明它们具有互补性。其比性能最好的CNN架构（如EfficientNet）学到了更好的表示，并且对于一些下游识别任务也有很好的泛化。</p><blockquote><p>ViT中的类标记只是个初始值，而DeiT带的类标记和蒸馏标记都通过训练学习得到。</p><p>类标记受正确类别监督，蒸馏标记受教师模型的硬标签监督。</p></blockquote><h2 id="3-2-Transformers-for-Object-Detection"><a href="#3-2-Transformers-for-Object-Detection" class="headerlink" title="3.2    Transformers for Object Detection"></a>3.2    Transformers for Object Detection</h2><p>与图像分类类似，Transformer应用于一组从CNN backbone获得的图像特征，以预测精确的目标边框和相应的类标签。下面将要介绍的第一种方法首次将Transformer用于检测问题，第二种方法主要将第一种方法扩展到多尺度体系结构，注重提高计算效率。</p><h3 id="3-2-1-Detection-Transformer-DETR"><a href="#3-2-1-Detection-Transformer-DETR" class="headerlink" title="3.2.1    Detection Transformer - DETR"></a>3.2.1    Detection Transformer - DETR</h3><p>DETR将目标检测视为一个使用Transformer预测问题的集合和一个损失函数集合。第一部分（transformer模型）对一组对象进行预测(一次性完成)，并对其关系进行建模。第二部分（集合损失）对预测结果和GT边框进行匹配。DETR的主要优点是它消除了对人工设计模块和运算的依赖，例如在目标检测中常用的RPN (region proposal network)和NMS (nonmaximal suppression)。这样，对于目标检测等复杂的结构化任务，就不需要依赖先验知识和精心的工程设计。</p><p><img src="https://i.loli.net/2021/01/27/x27bXc4Qk9WtFmi.png" alt="image.png"></p><p>首先通过CNN backbone的空间特征映射得到一个特征集合，将其平铺，如上图所示。然后，这些特征被多头self-attention模块进行编码和解码。解码阶段的主要区别是，所有边框都是并行预测的，而不是使用RNN逐个预测序列元素。DETR获得了与Faster RCNN相当的性能，这是一个令人印象深刻的壮举，因为它的设计十分简洁。</p><h3 id="3-2-2-Deformable-DETR"><a href="#3-2-2-Deformable-DETR" class="headerlink" title="3.2.2    Deformable - DETR"></a>3.2.2    Deformable - DETR</h3><p>上述DETR 成功地将CNN与Transformer相结合，消除了人工设计的需求。然而，它很难检测到小的物体，并且收敛速度慢，计算成本高。在使用Transformer进行关系建模之前，DETR将图像映射到特征空间。因此，Self-attention的计算代价随feature map的空间大小呈二次增长。这限制了DETR对多尺度特征的使用，这对小目标检测至关重要。此外，在训练开始时，Attention模块简单地将统一的Attention权重投射到特征图的所有位置，这需要大量的时间才能使Attention权重收敛到有意义的稀疏位置，使得该方法收敛速度较慢。为了缓解上述问题，作者提出了可形变注意模块(deformable attention module)来处理特征图。<strong>受可形变卷积的启发，可形变注意模块只关注整个feature map中元素的稀疏集，而不考虑其空间大小。</strong>能够在不显著增加计算成本的情况下，利用多尺度注意模块实现跨尺度的特征图聚合。Deformable DETR不仅性能更好，训练时间也仅为DETR的十分之一。</p><h2 id="3-3-Transformers-for-Segmentation"><a href="#3-3-Transformers-for-Segmentation" class="headerlink" title="3.3    Transformers for Segmentation"></a>3.3    Transformers for Segmentation</h2><p>像图像分割到和实例分割这样的密集型预测任务，需要对像素之间的相互作用进行建模。在这里，我们将讲述一种旨在降低Self-Attention复杂性的轴向自注意(axial self-attention)操作和一种跨模态方法，对给定语义进行分割。</p><h3 id="3-3-1-Axial-attention-for-Panoptic-Segmentation"><a href="#3-3-1-Axial-attention-for-Panoptic-Segmentation" class="headerlink" title="3.3.1    Axial-attention for Panoptic Segmentation"></a>3.3.1    Axial-attention for Panoptic Segmentation</h3><p>全景分割(Panoptic segmentation)旨在通过为图像的每个像素分配语义标签和实例id来解决语义分割和实例分割这两种截然不同的任务。全局上下文可以为处理这种复杂的视觉理解任务提供有用的线索。Self-Attention十分擅长获取长程上下文信息，但将它应用于密集预测任务（如全景分割）的大型输入需要非常大的运算量。一个简单的解决方案是将Self-Attention应用于下采样输入或每个像素周围的有限区域。即使在引入这些约束后，Self-Attention仍然具有二次方复杂度，并且牺牲了全局信息。</p><p>为缓解上述问题，Wang等人提出了位敏axial-attention。该方法<strong>将二维self-attention转化为两个一维axial-attention层，分别应用于宽和高两个轴</strong>。axial-attention具有较高的计算效率，并使模型能够捕捉整张图的上下文信息。该方法在多个数据集表现SOTA。</p><p><img src="https://i.loli.net/2021/01/31/39RgIYcZDxGtrbn.png" alt="image.png"></p><h3 id="3-3-2-CMSA-Cross-modal-Self-Attention"><a href="#3-3-2-CMSA-Cross-modal-Self-Attention" class="headerlink" title="3.3.2    CMSA: Cross-modal Self-Attention"></a>3.3.2    CMSA: Cross-modal Self-Attention</h3><p>跨模态自我注意(Cross-modal Self-attention, CMSA)编码语言和视觉域特征之间的长程多模态依赖关系，用于<strong>参考图像分割(referring image segmentation task)</strong>任务。参考图像分割问题的目的是对语言表达式所引用的图像实体进行分割。为此，将图像特征与每个词嵌入和空间坐标特征相连接，得到一组跨模态特征。Self-Attention在这个丰富的特征上运作，并对句子中每个单词对应的图像区域产生注意力。分割网络在多个空间层次上进行Self-Attention，并使用门控多级融合模块对多个分辨率的特征信息进行交换来细化分割掩码。使用BCE损失来训练整体模型，在UNC、G-Ref和Referlt数据集上取得了较大的提升。</p><h2 id="3-4-Transformers-for-Image-Generation"><a href="#3-4-Transformers-for-Image-Generation" class="headerlink" title="3.4    Transformers for Image Generation"></a>3.4    Transformers for Image Generation</h2><p>从生成建模的角度来看，图像生成任务很有趣，因为以无监督方式学习的表示可以用于下游任务。在这里，我们总结了一些基于Transformer的架构，用于图像生成(image generation)、条件生成(conditional generation)和高分辨率图像生成(high-resolution image generation)任务。我们还概述了一个结构化的生成任务，使用场景对象对给定的房间布局进行填充。</p><h3 id="3-4-1-Image-GPT"><a href="#3-4-1-Image-GPT" class="headerlink" title="3.4.1    Image GPT"></a>3.4.1    Image GPT</h3><p>Image GPT (iGPT)证明了Transformer也可以用于图像生成任务，并为下游视觉任务学习强特征。具体来说，iGPT使用GPT v2模型在平铺图像序列（一维像素数组）上进行训练，并表明它可以在没有任何外部监督的情况下生成可信的图像输出。生成的样本表现出了该模型具有理解像素和高级属性（如对象类别、纹理和比例）之间的空间关系的能力。</p><h3 id="3-4-2-Image-Transformer"><a href="#3-4-2-Image-Transformer" class="headerlink" title="3.4.2    Image Transformer"></a>3.4.2    Image Transformer</h3><p>Parmar等人开发了一个图像生成模型，该模型可以根据之前生成的像素顺序预测输出图像的每个像素。他们的方法通过将图像像素分解为像素条件分布(pixel-wise conditional distributions)的乘积来连接其像素分布。先前开发的用于这项任务的自回归模型（如PixelCNN）受到了感受野的局限，这妨碍了在图像中建模长程关系，如部分关系或遮挡。通过使用self-attention，Image Transformer增强了神经网络的感受野，而不会产生很高的计算成本。在条件生成任务如图像超分辨率、图像补全、去噪等方面进行了测试。</p><p>核心方法有两个主要亮点（见下图）:</p><ul><li>(a)图展示了key、query和value三元组在图像中的使用。使用之前生成的像素的特征表示来生成value和key嵌入，而当前像素的特征嵌入被用作query。第一层采用位置嵌入技术对位置信息进行编码。</li><li>(b)图表示该模型比起自然语言任务中使用了更多的self-attention。局部注意（1D和2D变量）仅在query位置周围的局部邻域使用。出于实际原因，我们为每个query定义了一个固定的内存块，而不是为每个像素动态计算不同的内存邻域。采用最大似然损失对生成模型进行训练。</li></ul><p><img src="https://i.loli.net/2021/02/11/IT5ucxoKOQR3Ukf.png" alt="image.png"></p><p>看不下去了看不下去了→_→，等实际使用巩固、对Transformer有更深的了解之后再来看吧。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/alawaka2018/article/details/80570537" target="_blank" rel="noopener">high-level vision和low-lever vision的解释</a></p><p>[2]<a href="https://www.zhihu.com/question/264264203/answer/492568154" target="_blank" rel="noopener">如何理解Inductive bias？</a></p><p>[3]<a href="https://zhuanlan.zhihu.com/p/53010734" target="_blank" rel="noopener">Non-local Neural Networks及自注意力机制思考</a></p><p>[4]<a href="https://www.sohu.com/a/226611009_633698" target="_blank" rel="noopener">看完这篇，别说你还不懂Hinton大神的胶囊网络</a></p><p>[5]<a href="https://zhuanlan.zhihu.com/p/266311690" target="_blank" rel="noopener">用Transformer完全替代CNN</a></p><p>[6]<a href="https://mp.weixin.qq.com/s?__biz=MzU4OTg3Nzc3MA==&amp;mid=2247484771&amp;idx=1&amp;sn=d8e3fa7a86b9ae1316ae831bc3270b12&amp;scene=21#wechat_redirect" target="_blank" rel="noopener">媲美CNN！Facebook提出DeiT</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;http://xxx.itp.ac.cn/pdf/2101.01169v1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Transformers in Vision: A Survey&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;Abst
      
    
    </summary>
    
    
      <category term="Transformer" scheme="http://a-kali.github.io/tags/Transformer/"/>
    
      <category term="论文翻译" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91/"/>
    
      <category term="综述" scheme="http://a-kali.github.io/tags/%E7%BB%BC%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>经济心理学</title>
    <link href="http://a-kali.github.io/2021/02/09/%E7%BB%8F%E6%B5%8E%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    <id>http://a-kali.github.io/2021/02/09/经济心理学/</id>
    <published>2021-02-09T00:38:44.000Z</published>
    <updated>2021-02-09T00:40:18.999Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-随机性的判断误区"><a href="#1-随机性的判断误区" class="headerlink" title="1    随机性的判断误区"></a>1    随机性的判断误区</h1><p>我们生活在一个充满随机性和偶然性的世界之中。随机性的表现不仅包括抛硬币的正反，还包括一些股票的涨跌等。随机性是我们生活中不可分割的一部分，而我们对随机性的理解却往往出现偏差。<strong>对于随机性的认识往往存在这样四种误区：</strong></p><ol><li><strong>过度解释，无中生有；</strong></li><li><strong>错误模拟，过度求变；</strong></li><li><strong>赌徒谬误，高估反转；</strong></li><li><strong>手热错觉，高估惯性。</strong></li></ol><h2 id="（一）过度解释"><a href="#（一）过度解释" class="headerlink" title="（一）过度解释"></a>（一）过度解释</h2><p><strong>从没有结构的地方找结构：人们总把模棱两可的序列事件知觉为更有结构的事件。</strong>比如将一朵云的形状描述成某种小动物，根据星星的排列命名星座，从彩票走势图中找规律等。证券市场上也如此。</p><p><strong>虚假相关：对没有关联的两件事物进行因果关联</strong>。比如“雨神萧敬腾”事件和“丁蟹效应”。</p><p><strong>解释巧合</strong>：人们常常对一些巧合进行解释，而巧合往往不需要偶然性以外的特别的解释。</p><p><strong>公平世界信念</strong>：人们认为这个世界是公正的，人们得其所应得，所得即应得。但其实很多所得到的和损失的都是随机产生的，并不具有需要解释的原因。</p><h2 id="（二）错误模拟"><a href="#（二）错误模拟" class="headerlink" title="（二）错误模拟"></a>（二）错误模拟</h2><p>错误地理解了随机性，认为随机就是每次都不相同。比如对于音乐播放器中的随机播放，当播放器连续播放了同一首歌或者同一歌手的歌时，用户会觉得这不是随机播放；而当程序修改为随机播放时不会连续播放同一首歌或同一歌手的歌时，用户反而认为这是随机播放。</p><h2 id="（三）赌徒谬误"><a href="#（三）赌徒谬误" class="headerlink" title="（三）赌徒谬误"></a>（三）赌徒谬误</h2><p>抛硬币每次都是独立事件，第六次的结果并不会因为前面五次都是正面而出现反面。</p><h2 id="（四）手热错觉"><a href="#（四）手热错觉" class="headerlink" title="（四）手热错觉"></a>（四）手热错觉</h2><p>来源于篮球场上，人们通常会将球交给状态好、手热的球员来投球，认为其此时命中率高。在球场上或许有道理，但在市场上认为股市连续下跌多日后会继续下跌则并没有依据。</p><h1 id="2-控制幻觉"><a href="#2-控制幻觉" class="headerlink" title="2    控制幻觉"></a>2    控制幻觉</h1><p>首先假设一个情景：某一天你被歹徒绑架，歹徒告诉你可以通过玩一个俄罗斯轮盘来决定你的生死。你有两种选择：选择A是一把容量为6的放了1颗子弹的左轮手枪，选择B是在5把空手枪和1把满子弹手枪里选一把。虽然两种选择的死亡概率相同，但大多数人都会选择B，因为人们会对这个选择产生控制幻觉，即认为A是完全随机，而B可以通过自己选择手枪来控制概率。</p><p><strong>控制幻觉的定义</strong>：在完全不可控或部分不可控的情境下，个体由于不合理地高估自己对环境或事件结果的控制力而产生的一种判断偏差。</p><ul><li>彩票实验：自己填写的彩票在转让时往往会要价更高，而由他人帮忙购买的彩票在转让时要价低于前者；</li><li>扑克实验：在随机扑克游戏中，面对表现自信的对手时，受试者往往感到可控力低，押注往往更少。</li><li>掷骰子时，人们想要更大的数字时往往投掷力度会更大；</li><li>自选彩票的销量远高于随机彩票。</li></ul><p>在金融领域，交易者们认为自己的行为能够增加投资收益。于是在国内市场上经常会出现频繁交易的现象，而事实上交易次数整体是与收益成反比的。</p><p>以下三种因素能够影响到控制幻觉：</p><ol><li>个体陷入的程度；</li><li>是否具有选择权；</li><li>事物对个体的特殊意义。</li></ol><h1 id="3-启发式偏差"><a href="#3-启发式偏差" class="headerlink" title="3    启发式偏差"></a>3    启发式偏差</h1><p>人类在做判断时，往往会利用直觉，采取走捷径的方式，这种方式称之为<strong>启发式</strong>。启发式分为代表性启发式和易得性启发式。</p><h2 id="（一）代表性启发式"><a href="#（一）代表性启发式" class="headerlink" title="（一）代表性启发式"></a>（一）代表性启发式</h2><p><strong>代表性启发式</strong>：人们通常会根据A在多大程度上能代表B，或者A在多大程度上与B相似来判断事件发生的可能性。其原因如下：</p><ol><li><strong>对先验概率不敏感</strong>：比如说人们不知道人群中工程师的比例是多少，但当遇到一个符合他们对工程师刻板印象的描述时，却很可能将其归类为对工程师的描述，无视了工程师在人群中的比例（即先验概率）。</li><li><strong>对样本大小不敏感</strong>：样本越大才越具有代表性，小样本通常具有偶然性。而人们往往会在一个小样本上做出对于规律的判断，认为小样本能反映整体现象。</li><li><strong>对随机性的误解</strong>：同第一章随机性的四种误区。</li><li><strong>对向均值回归的误解</strong>：一些连续性的数据往往会有向平均值回归的趋势，而人们却忽略了这种趋势。比如某位选手在一段时间内表现较佳，人们会认为其将一直表现优异下去，但实际上这段时间只是该选手高于自身平均的一段时间。</li></ol><p>在投资领域，人们通常将好公司与好股票对等，而事实并非如此，这就是代表性启发式的例子。</p><h2 id="（二）易得性启发式"><a href="#（二）易得性启发式" class="headerlink" title="（二）易得性启发式"></a>（二）易得性启发式</h2><p><strong>易得性启发式</strong>：人们倾向于根据一个客体或事件在知觉或记忆中的易得性程度来评估其相对概率，认为容易知觉到的或回想起的被判定为更常出现。其与以下两点相关联：</p><ol><li><strong>信息的生动性</strong>：亲生经历的、越容易回想起的例子越生动，越容易被判断为容易发生的事件；</li><li><strong>时间接近性</strong>：越是最近发生的事，人们越容易认为它会再发生一次。</li></ol><p>在投资领域，人们偏爱自己熟悉了解的事物。比如人们更愿意去参与自己更熟悉的游戏（哪怕获胜概率更低）、更愿意买自己公司的股票。（从这种角度来看，茅台下跌的那天是不是就是它失去热度的那天？）</p><h1 id="4-锚定效应"><a href="#4-锚定效应" class="headerlink" title="4    锚定效应"></a>4    锚定效应</h1><p><strong>锚定效应（Anchoring Effect）</strong>：当人们对某个事件做定量估测时，会将某些特定数值作为起始值，进行上下不充分的调整。锚定效应可以根据锚是来自于内还是来自于外可以分为外部锚和内部锚。</p><h2 id="（一）外部锚"><a href="#（一）外部锚" class="headerlink" title="（一）外部锚"></a>（一）外部锚</h2><p>外部锚包括传统锚定效应和基本锚定效应。</p><p><strong>传统锚定效应</strong>：比较判断问题+绝对判断问题。</p><p><strong>基本锚定效应</strong>：单纯数字也会影响绝对的判断，使最终的估计值趋向无关信息的值。人们对数值最终的估计会偏向于那个最近出现的无关数值。</p><h2 id="（二）内部锚"><a href="#（二）内部锚" class="headerlink" title="（二）内部锚"></a>（二）内部锚</h2><p>人们会根据自身的一些经验来推算估计最终的数值。</p><h2 id="（三）锚定效应的应用"><a href="#（三）锚定效应的应用" class="headerlink" title="（三）锚定效应的应用"></a>（三）锚定效应的应用</h2><p>某些手表公司可能会生产上百万美元的手表，但这种手表通常不是用来售卖的，而是用来给顾客一个锚点，让顾客能接受该公司手表价格高于其他公司。</p><p>超市里相同类别的两种商品，贵的商品会衬托出便宜的商品的实惠，增加便宜商品的销量。但事实上可能两种商品都是高于其合理价格的。</p><h1 id="5-过度自信与后悔"><a href="#5-过度自信与后悔" class="headerlink" title="5    过度自信与后悔"></a>5    过度自信与后悔</h1><p><strong>过度自信与后悔</strong>：人们在做出判断时往往会出现一种高估自己能力、判断准确性以及信息准确性的现象，即过度自信（over-confidence）。</p><p><strong>过度自信</strong>分为两种类型：过高估计与过高定位。</p><ul><li><strong>过高估计</strong>：个体高估自身实际能力、表现、对事件控制水平以及成功几率的一种认知差。大多数公司会高估自己公司的存活率和成功率，而低估其它公司的成功率和存活率。</li><li><strong>过高定位</strong>：个体认为自身能力要高于其他人的一种倾向，又称为高于平均效应。95%的美国大学生认为自己的社交能力高于平均。</li></ul><p>通常男性比女性更容易出现过度自信。在投资领域，投资者往往会高估私人信息产生信号的准确性和自身对证券价值的估价能力，导致过度交易和低估风险。而男性投资者的年交易量通常高于女性，而投资收益低于女性。单身男性的投资组合风险最大，其后依次是已婚男性、已婚女性和单身女性。</p><p><strong>后悔</strong>：后悔是一种基于认知的消极情感，主要发生在个体意识到或者想象出如果先前采取其他的行为，将产生更好的结果时。后悔会导致两种结果：</p><ul><li><strong>后悔规避</strong>：人们当做出错误决策时，会对自己的行为感到痛苦，并对这种痛苦采取回避的方式。</li><li><strong>不作为惯性</strong>：当人失去一个行为机会时，会导致对后来类似的机会的不作为。</li><li><strong>处置效应</strong>：是指避免后悔、寻求自豪的心理导致投资者过早套现盈利股票，而过久持有亏损股票。</li></ul><h1 id="6-前景理论"><a href="#6-前景理论" class="headerlink" title="6    前景理论"></a>6    前景理论</h1><p><img src="https://i.loli.net/2021/02/07/v3YmDLFnMbKhiel.png" alt="image.png"></p><p><strong>前景理论</strong>的核心是<strong>价值函数</strong>（如上图），值得注意的是改图并不是中心对称的。前景理论可以概括为五个方面：</p><ol><li><strong>确定效应</strong>：大多数人处于收益状态时往往小心翼翼、厌恶风险，喜欢见好就收，害怕失去已有的李润。</li><li><strong>反射效应</strong>：处于损失预期时，大多数人会变得甘冒风险。反射效应与确定效应结合起来就是处置效应。一个典型的例子是交通事故中的肇事司机往往会选择逃逸而不是及时抢救止损，而股市中人们处于亏损状态时也更倾向于长期持有，不愿意及时止损来接受损失的事实。</li><li><strong>损失厌恶</strong>： 对于相同金额的收益和损失，损失带给人的痛是大于收益带来的快乐。一个典型的例子是<strong>沉没成本效应</strong>。</li><li><strong>迷恋小概率事件</strong>：人们对小概率事件往往非常重视，而对大概率事件没有给到应有的权重。对于小概率的盈利，多数人是风险喜好者；对于小概率的损失，多数人是风险厌恶者。</li><li><strong>参照依赖</strong>：人们对得失的判断往往根据参照点决定。“在一群赚6万的人中赚7万元”比“在一群赚12万元的人中赚10万元”更快乐。</li></ol><h1 id="7-禀赋效应"><a href="#7-禀赋效应" class="headerlink" title="7    禀赋效应"></a>7    禀赋效应</h1><p><strong>禀赋效应</strong>：当个人一旦拥有某样物品，那么他对该物品价值的评价比未拥有前大大提高。与得到某物品所愿意支付的金钱相比，个体出让该物品所要求的道德金钱通常更多。</p><p>禀赋效应最高的物品为公共物品或非市场化的商品，比如空气、水，其次是私人物品；而禀赋效应最低的物品是货币和代币券。 </p><p>禀赋效应有三种解释：</p><ol><li>第一种解释认为禀赋效应源于损失厌恶，人们将已拥有的物品转让出去视为损失； </li><li>第二种解释认为禀赋效应源于<strong>参照点转换理论</strong>， 损失规避具有参照依赖性，损失和收益是相对于某一参照点而言的。</li><li>第三种解释认为禀赋效应源于<strong>程数理论</strong>。</li></ol><p><strong>伪禀赋效应</strong>：人们对可能拥有但并未实质拥有的物品也会产生禀赋效应。（如果我未曾见过光明，我本可以忍受黑暗.jpg）</p><h1 id="8-偏好反转与框架效应"><a href="#8-偏好反转与框架效应" class="headerlink" title="8    偏好反转与框架效应"></a>8    偏好反转与框架效应</h1><p><strong>偏好反转</strong>：决策者在两个相同评价条件但不同的引导模式下，对方案的偏好有所差异，甚至出现逆转的现象。在期望值大体相等的一对博弈中，人们往往选择概率高而损益值小的博弈（安全博弈），却对概率低而损益值大的博弈（风险博弈）定高价。</p><p>在两个期望收益相当的赌局中，人们往往会选择那个风险小收益低的，但在转售这两场赌局的门票时，人们往往会对风险大收益高的赌局定高价。</p><p><strong>框架效应</strong>：对相同客观信息的不同表述能够显著地改变决策模型，即使框架差异程度不应对理性决策产生影响。框架效应分为三种：</p><ol><li><strong>特征框架效应</strong>：以积极的语言对事物的关键属性进行描述，比起以消极的语言描述，人们对事物或事件会更满意。瘦肉占80%比肥肉占20%的描述更能让消费者满意。</li><li><strong>目标框架效应</strong>：指当一个信息强调为不做出某种行为导致的消极后果时，比强调为做出这种行为的积极后果更有说服力（损失厌恶+后悔）。</li><li><strong>风险决策框架效应</strong>：当信息以积极的形式呈现时，个体表现出风险规避的倾向；当信息以消极的形式呈现，个体表现为风险偏好的倾向（确定效应+反射效应）。</li></ol><h1 id="9-心理账户"><a href="#9-心理账户" class="headerlink" title="9    心理账户"></a>9    心理账户</h1><p><strong>心理账户（mental accounting, MA）</strong>是个人或家庭用来管理、评估、跟踪金融活动的一种操作定式。</p><p>如果今天晚上你打算去听一场音乐会，票价是200元，在你马上要出发的时候，你发现你把最近买的价值200元的电话卡弄丢了。你是否还会去听这场音乐会？实验表明，大部分的回答者仍旧去听。可是如果情况变一下，假设你昨天花了200元钱买了一张今天晚上音乐会门票。在你马上要出发的时候，突然发现你把门票弄丢了。如果你想要听音乐会，就必须再花200元钱买张门票，你是否还会去听？结果却是，大部分人回答说不去了。</p><p>可仔细想一想，上面这两个回答其实是自相矛盾的。不管丢的是电话卡还是音乐会门票，总之是丢失了价值200元的东西，从损失的金钱上看，并没有区别。之所以出现上面两种不同的结果，其原因就是大多数人的心理账户的问题。</p><p>人们在脑海中，把电话卡和音乐会门票归到了不同的账户中，所以丢失了电话卡不会影响音乐会所在的账户的预算和支出，大部分人仍旧选择去听音乐会。但是丢了的音乐会门票和后来需要再买的门票都被归入了同一个账户，所以看上去就好像要花400元听一场音乐会了。人们当然觉得这样不划算了。</p><p><img src="https://i.loli.net/2021/02/08/erSMxkhqwPH2fUX.png" alt="image.png"></p><p>MA的原因有四种类型：</p><ol><li><strong>财富来源不同</strong>：人们对意外之财的消费倾向较高，常用于享乐；而对辛苦所得的钱消费倾向低，常用于日常开支。</li><li><strong>消费类型不同</strong>：人们在日常消费中很可能不会买很贵的衣服，但在节日送礼时可能会消费这件衣服，因为日常消费和节假日消费不在同一个心理账户中。</li><li><strong>存储方式不同</strong>：存在银行的钱、在证券市场里的钱、以及手上的现金有不同的心理账户。</li><li><strong>时间划分不同</strong>：出租车司机每天都要交份子钱，所以通常每天都会工作到挣够预期的钱才下班，其心理账户是按天计算的；而如果心理账户按月计算的话，司机可以选择在身体不适时选择休息，而在平时身体健康时多拉点活。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-随机性的判断误区&quot;&gt;&lt;a href=&quot;#1-随机性的判断误区&quot; class=&quot;headerlink&quot; title=&quot;1    随机性的判断误区&quot;&gt;&lt;/a&gt;1    随机性的判断误区&lt;/h1&gt;&lt;p&gt;我们生活在一个充满随机性和偶然性的世界之中。随机性的表现不仅包括
      
    
    </summary>
    
      <category term="经济与金融" scheme="http://a-kali.github.io/categories/%E7%BB%8F%E6%B5%8E%E4%B8%8E%E9%87%91%E8%9E%8D/"/>
    
    
      <category term="经济" scheme="http://a-kali.github.io/tags/%E7%BB%8F%E6%B5%8E/"/>
    
      <category term="心理学" scheme="http://a-kali.github.io/tags/%E5%BF%83%E7%90%86%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>Self-Attention and Transformer</title>
    <link href="http://a-kali.github.io/2021/02/09/Self-Attention-and-Transformer/"/>
    <id>http://a-kali.github.io/2021/02/09/Self-Attention-and-Transformer/</id>
    <published>2021-02-09T00:37:24.000Z</published>
    <updated>2021-02-09T00:57:17.027Z</updated>
    
    <content type="html"><![CDATA[<p>关于attention的详细介绍在<a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">Attention机制详解（一）——Seq2Seq中的Attention</a>有详细介绍，在此不做多赘述。</p><p>添加了Attention的LSTM依然存在一些问题：</p><ul><li>梯度消失，无法捕捉到长程的依赖关系；</li><li>运算量大，且无法并行。</li></ul><p>以上问题在时序模型中普遍存在，于是一种能够拟合时序数据的非时序模型应运而生，那就是Transformer，而Transformer中的关键结构就是Self-attention。</p><h1 id="Transformer的结构"><a href="#Transformer的结构" class="headerlink" title="Transformer的结构"></a>Transformer的结构</h1><p><img src="https://i.loli.net/2021/01/21/xubkHCYoVmiX7Dv.png" alt="image.png"></p><p>上图是一个Transformer的结构，其中Multi-Head Attention就是多个Self-Attention的结合。</p><h1 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h1><h2 id="Self-Attention的作用"><a href="#Self-Attention的作用" class="headerlink" title="Self-Attention的作用"></a>Self-Attention的作用</h2><p>Transformer模型的左半部分称为Encoder，由多个Encoder模块组成，单个模块的结构可以看作下图。其输入为字符串中的多个字符经过Embedding后的<strong>相互无关的多个词向量</strong>，经过Self-Attention后得到一个新<strong>相互关联的向量</strong>，这就是Self-Attention的作用。</p><p><img src="https://i.loli.net/2021/01/21/HaduK5krWozRsMO.png" alt="image.png"></p><h2 id="Self-Attention的原理"><a href="#Self-Attention的原理" class="headerlink" title="Self-Attention的原理"></a>Self-Attention的原理</h2><p>Self-Attention模块维护了三个可训练的矩阵，分别为$W_Q, W_K, W_V$，使用这三个矩阵与词向量相乘得到$q, k, v$三个向量。</p><p><img src="https://i.loli.net/2021/01/21/wX3fTFsq6NDvd8H.png" alt="image.png"></p><p>使$q_i$和$k_j$相乘得到attention score，该值表示翻译第i个词时，第j个词与其的相关度。显然，当前单词与其自身的attention score一般最大。然后除以8对该词进行缩放（这个除数由k向量的长度开根号得到，原论文中向量长度为64），使用softmax进行归一化。</p><p><img src="https://i.loli.net/2021/01/21/X5jcazOdS3irEgY.png" alt="image.png"></p><p>最后使用softmax得到的结果对多个v向量进行加权求和，得到当前词的结果z，并将得到的所有z输入到下一个Encoder模块。</p><p><img src="https://i.loli.net/2021/01/21/QvorCjHTumKJOy4.png" alt="image.png"></p><p>值得一提的是在Decoder中无法并行预测所有词，需要循环地一个个预测单词，因为要用上一个位置的输入当作attention的query。</p><h1 id="什么是Multi-head？"><a href="#什么是Multi-head？" class="headerlink" title="什么是Multi-head？"></a>什么是Multi-head？</h1><p>Multi-head就是我们可以有不同的Q,K,V表示的Self Attention，最后再将其结果结合起来，如下图所示：</p><p><img src="https://i.loli.net/2021/01/21/iL3yU8FOH9WmCMB.png" alt="image.png"></p><h1 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h1><p>#copy</p><p>由于Transformer中没有循环以及卷积结构，为了使模型能够利用序列的顺序，作者们需要插入一些关于tokens在序列中相对或绝对位置的信息。因此，作者们提出了<strong>位置编码(Positional Encoding)</strong>的概念。Positional Encoding和token embedding相加，作为encoder和decoder栈的底部输入。Positional Encoding和embedding具有同样的维度，因此这两者可以直接相加。</p><p>在位置编码的论文中，作者使用了不同频率的正弦函数和余弦函数作为位置编码用来表示每个词在句子中的位置，具体可参考<a href="https://zhuanlan.zhihu.com/p/98641990" target="_blank" rel="noopener">博客</a>。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://zhuanlan.zhihu.com/p/47282410" target="_blank" rel="noopener">Attention机制详解（二）——Self-Attention与Transformer</a><br>[2]<a href="https://blog.csdn.net/weixin_40871455/article/details/86084560" target="_blank" rel="noopener">transformer 模型（self-attention自注意力）</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/98641990" target="_blank" rel="noopener">对Transformer中的Positional Encoding一点解释和理解</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/44121378" target="_blank" rel="noopener">【NLP】Transformer模型原理详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于attention的详细介绍在&lt;a href=&quot;https://zhuanlan.zhihu.com/p/47063917&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Attention机制详解（一）——Seq2Seq中的Attention&lt;/a&gt;有
      
    
    </summary>
    
    
      <category term="self-attention" scheme="http://a-kali.github.io/tags/self-attention/"/>
    
      <category term="transformer" scheme="http://a-kali.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>EfficientNet简述</title>
    <link href="http://a-kali.github.io/2021/01/18/EfficientNet%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2021/01/18/EfficientNet简述/</id>
    <published>2021-01-18T01:01:09.000Z</published>
    <updated>2021-01-18T01:41:23.718Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p><p>EfficientNet作者系统地研究了模型缩放，并发现对网络<strong>深度、宽度和分辨率</strong>进行平衡可以带来更好的性能，而前人的文章多是放大其中的一个以达到更高的准确率。EfficientNet仅使用了很小的参数量就超越了当时的SOTA模型。</p><p><img src="https://i.loli.net/2021/01/18/zeOAPEBmIoHFCyi.png" alt="image.png"></p><p>作者发现只对模型的深度、宽度和分辨率其中一个维度进行扩张能得到性能提升，但是会有较大的局限性。作者认为<strong>各个维度之间的扩张不应该是相互独立的</strong>，比如说，对于更大分辨率的图像，应该使用更深、更宽的网络，这就意味着需要平衡各个扩张维度，而不是在单一维度张扩张。</p><p>对此作者提出了<strong>复合缩放法(compound scaling method)</strong>，方程式如下，其中约束(s.t.)限制了模型的复杂度。在约束式中宽度和分辨率都有一个平方项，这是因为如果增加宽度或分辨率两倍，其计算量是增加四倍，但是增加深度两倍，其计算量只会增加两倍。</p><p><img src="https://i.loli.net/2021/01/18/WZAmwraFBPnHfiq.png" alt="image.png"></p><p>求解方式：</p><ol><li>固定公式中的φ=1，然后通过网格搜索（grid search）得出最优的α、β、γ，得出最基本的模型EfficientNet-B0.</li><li>固定α、β、γ的值，使用不同的φ，得到EfficientNet-B1, …, EfficientNet-B7</li></ol><p>φ的大小对应着消耗资源的大小，相当于：</p><ol><li>当φ=1时，得出了一个最小的最优基础模型；</li><li>增大φ时，相当于对基模型三个维度同时扩展，模型变大，性能也会提升，资源消耗也变大。</li></ol><p>值得一提的是EfficientNet中使用了<strong>移动翻转瓶颈卷积(mobile inverted bottleneck convolution，MBConv)模块</strong>，该模块引入了深度分离卷积和SENet的思想。</p><p>参考文献：</p><p>[1]<a href="https://zhuanlan.zhihu.com/p/96773680" target="_blank" rel="noopener">令人拍案叫绝的EfficientNet和EfficientDet</a><br>[2]<a href="https://www.jianshu.com/p/2ac06d97a830" target="_blank" rel="noopener">速度与精度的结合 - EfficientNet 详解</a><br>[3]<a href="https://zhuanlan.zhihu.com/p/111115509" target="_blank" rel="noopener">EfficientNet-B0解读</a><br>[4]<a href="https://zhuanlan.zhihu.com/p/258386372" target="_blank" rel="noopener">EfficentNet详解之MBConvBlock</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;EfficientNet: Rethinking Model Scaling for Convolutional N
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
  </entry>
  
  <entry>
    <title>RANSAC算法概述</title>
    <link href="http://a-kali.github.io/2020/12/30/RANSAC%E7%AE%97%E6%B3%95%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/30/RANSAC算法概述/</id>
    <published>2020-12-30T09:14:17.000Z</published>
    <updated>2020-12-30T09:15:36.758Z</updated>
    
    <content type="html"><![CDATA[<p><strong>RANSAC（Random Sample Consensus，随机取样一致）</strong>是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。</p><p>一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图）。最小二乘法会使用包括异常样本的所有的数据样本进行计算，得到拟合参数，但其会受到异常点的影响。而RANSAC就是为了解决这个问题。</p><p><img src="https://i.loli.net/2020/12/29/r9S51AEdvij6N3O.png" alt="image.png"></p><p>RANSAC算法通过反复选择数据中的以组随机子集进行验证，概述如下：</p><ol><li>随机选择一组点作为<strong>局内点</strong>，用这部分局内点拟合一个模型；</li><li>将所有满足该模型的点加入一个新的局内点集合；</li><li>使用新的局内点集合去拟合一个新的模型，并测试模型的准确度（即满足模型的点                                                                                          占所有点的比例）；</li><li>重复步骤2、3，最终得到一个准确度达到预期的模型。</li></ol><p>RANSAC常用于删除特征匹配过程中的异常点。</p><p><img src="https://i.loli.net/2020/12/29/eVio9WpMPXntUIA.png" alt="image.png"></p><p>参考博客：<a href="https://blog.csdn.net/robinhjwy/article/details/79174914" target="_blank" rel="noopener">RANSAC算法理解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;RANSAC（Random Sample Consensus，随机取样一致）&lt;/strong&gt;是根据一组包含异常数据的样本数据集，计算出数据的数学模型参数，得到有效样本数据的算法。&lt;/p&gt;
&lt;p&gt;一个直观的例子就是使用一条直线去拟合平面上的多个离散点（如下图
      
    
    </summary>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="RANSAC" scheme="http://a-kali.github.io/tags/RANSAC/"/>
    
  </entry>
  
  <entry>
    <title>SIFT特征提取和描述</title>
    <link href="http://a-kali.github.io/2020/12/28/SIFT%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%92%8C%E6%8F%8F%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/28/SIFT特征提取和描述/</id>
    <published>2020-12-28T10:26:10.000Z</published>
    <updated>2021-02-09T00:58:41.521Z</updated>
    
    <content type="html"><![CDATA[<p>SIFT，即<strong>尺度不变特征变换（Scale-invariant feature transform，SIFT）</strong>，是用于图像处理领域对局部特征的一种描述方法。主要原理是<strong>通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点的值生成一个用于稳定描述该特征点的向量</strong>。该算法常用于特征匹配领域。</p><p>SIFT算法具有如下一些特点：</p><ol><li>SIFT特征是图像的局部特征，其对旋转、尺度缩放、亮度变化保持不变性，对视角变化、仿射变换、噪声也保持一定程度的稳定性；</li><li>区分性（Distinctiveness）好，信息量丰富，适用于在海量特征数据库中进行快速、准确的匹配；</li><li>多量性，即使少数的几个物体也可以产生大量的SIFT特征向量；</li><li>高速性，经优化的SIFT匹配算法甚至可以达到实时的要求；</li><li>可扩展性，可以很方便的与其他形式的特征向量进行联合。</li></ol><h1 id="一、图像尺度空间"><a href="#一、图像尺度空间" class="headerlink" title="一、图像尺度空间"></a>一、图像尺度空间</h1><p>在一定范围内，无论物体是大还是小，人眼都能分辨出来；但计算机对不同尺度下的物体分辨能力却很低。所以要让机器能够对物体在不同尺度下有一个统一的认知，就需要考虑图像在不同尺度下都存在的特点。</p><p>不同尺度的图像可以通过<strong>高斯模糊</strong>（或称为<a href="https://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/">高斯滤波</a>）来模拟：</p><script type="math/tex; mode=display">L(x, y, \sigma)=G(x, y, \sigma)*I(x, y)</script><p>其中I为图像，G为高斯函数：</p><script type="math/tex; mode=display">G(x, y, \sigma)=\frac{1}{2\pi\sigma^2}e^{\frac{x^2+y^2}{2\sigma^2}}</script><p>其中σ用于控制滤波器权重，σ越大，权重数值分布越均匀，即周边值更大、中心值更小，得到的图像越模糊。</p><p><img src="https://i.loli.net/2020/12/27/LU4sBYMQXWIb8Fx.png" alt="image.png"></p><p>通过这个方法可以得到不同尺度空间的图像，图像越模糊，就相当于得到了更远距离/更小尺度空间的图像。</p><h1 id="二、高斯差分金字塔"><a href="#二、高斯差分金字塔" class="headerlink" title="二、高斯差分金字塔"></a>二、高斯差分金字塔</h1><p>对于一张图像进行n-1次上/下采样操作，得到的n个不同分辨率的结果可以组合成一个<strong>图像金字塔</strong>。对金字塔的每一层进行m次不同程度的高斯模糊，最终得到n×m张图像。</p><p><img src="https://i.loli.net/2020/12/27/HerEtTBQoWiIXD2.png" alt="image.png"></p><p>之后对金字塔每一层得到的模糊结果分别求差，得到n×(m-1)个二维数组，得到的这个结果就是<strong>高斯差分金字塔（Difference of Gaussian, DOG）</strong>。DOG中较大的值表示该点对于不同的模糊程度变化更大，该点更有可能是特征的关键点/边缘。找出<strong>局部极值点</strong>需要对每个点与其周围的26个点（同平面8个，上下各9个）点进行比较。</p><p><img src="https://i.loli.net/2020/12/28/9HfCAV4gOTuz57l.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/28/QX2VGIU7OmFjH98.png" alt="image.png"></p><p>DOG用数学公式表示如下：</p><script type="math/tex; mode=display">D(x,y,\sigma)=L(x, y, k\sigma)-L(x, y, \sigma)</script><h1 id="三、特征描述"><a href="#三、特征描述" class="headerlink" title="三、特征描述"></a>三、特征描述</h1><h2 id="1-特征点的方向"><a href="#1-特征点的方向" class="headerlink" title="1    特征点的方向"></a>1    特征点的方向</h2><p>每个像素点L(x, y)的梯度的模m(x, y)以及方向θ(x, y)计算如下：</p><script type="math/tex; mode=display">m(x,y)=\sqrt{[L(x+1,y)-L(x-1,y)]^2+[L(x,y+1)-L{(x,y-1)}]^2}\\\theta(x,y)=arctan \frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}</script><p>每个像素点可以得到<strong>位置(x, y)、尺度m和方向θ</strong>三个信息。对于每个特征点，统计其邻域所有像素点在8个方向上的尺度和，尺度和最大的方向视为该特征点的<strong>主方向</strong>，占比超过主方向80%的视为<strong>辅方向</strong>。具有多个方向的关键点可以被复制成多份，然后将方向值分别赋给复制后的特征点，于是一个特征点就产生了多个坐标、尺度相同，但方向不同的特征点。</p><p><img src="https://i.loli.net/2020/12/28/qjAZ2HrOoeJ8law.png" alt="image.png"></p><h2 id="2-生成特征描述"><a href="#2-生成特征描述" class="headerlink" title="2    生成特征描述"></a>2    生成特征描述</h2><p>本部分将使用邻域像素的方向和尺度为该特征点生成一个唯一的指纹，称为<strong>描述符</strong>。首先在关键点周围采用16×16的邻域，将该16×16区域进一步划分为4×4子块。由于子块中的每一个像素都具有8个方向中的一个，并且具有尺度。于是对于每一个子块都能用一个长度为8的向量来表示该子块所有像素在8个方向上的尺度和。</p><p>最终对于每一个特征点，我们得到了一个总长度为4×4×8=128的特征描述符。</p><p><img src="https://i.loli.net/2020/12/28/26wiWJ8cZzmL9aV.png" alt="image.png"></p><p>计算两个特征点描述符之间的欧氏距离即可进行匹配。原算法中使用的是kd树进行搜索匹配，这里不作详细描述。</p><p><img src="https://i.loli.net/2020/12/28/rKe1gRlLtJNPi4k.png" alt="image.png"></p><h1 id="四、OpenCV中的SIFT算法"><a href="#四、OpenCV中的SIFT算法" class="headerlink" title="四、OpenCV中的SIFT算法"></a>四、OpenCV中的SIFT算法</h1><p>由于算法版权问题，SIFT算法只能在OpenCV3.4及以下版本才能使用。</p><p>特征描述：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sift = cv2.xfeatures2d.SIFT_create()  <span class="comment"># 创建一个SIFT对象</span></span><br><span class="line">kp, des = sift.detectAndCompute(img_gray, <span class="literal">None</span>) <span class="comment"># 返回关键点对象和以及每个关键点的特征向量</span></span><br><span class="line">show_kp_img = cv2.drawKeypoints(img_gray, kp, img)  <span class="comment"># 在图像中标出关键点</span></span><br></pre></td></tr></table></figure><p>特征匹配：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一对一匹配</span></span><br><span class="line">bf = cv2.BFMatcher(crossCheck=<span class="literal">True</span>)  </span><br><span class="line">matchs = bf.match(des1, des2)</span><br><span class="line">matches.sort(key=<span class="keyword">lambda</span> x: x.distance)</span><br><span class="line"><span class="comment"># 可视化匹配结果</span></span><br><span class="line">matched_img = cv2.drawMatches(img1, kp1, img2, kp2, matches[:<span class="number">10</span>], <span class="literal">None</span>, flags=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;SIFT，即&lt;strong&gt;尺度不变特征变换（Scale-invariant feature transform，SIFT）&lt;/strong&gt;，是用于图像处理领域对局部特征的一种描述方法。主要原理是&lt;strong&gt;通过一些数学计算得到图像中特征点的坐标，并根据该点周围像素点
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="SIFT" scheme="http://a-kali.github.io/tags/SIFT/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV与角点检测</title>
    <link href="http://a-kali.github.io/2020/12/28/OpenCV%E4%B8%8E%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/28/OpenCV与角点检测/</id>
    <published>2020-12-28T10:24:08.000Z</published>
    <updated>2021-02-09T00:54:56.899Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Harris角点检测原理"><a href="#Harris角点检测原理" class="headerlink" title="Harris角点检测原理"></a>Harris角点检测原理</h1><p>Harris角点检测的基本原理：类似于边缘检测，只不过边缘检测只判断单个方向的梯度，而角点检测判断多个方向的梯度/相似性。</p><p>对于一张灰度图，计算在点(x, y)处平移(Δx, Δy)后的自相似性c：</p><script type="math/tex; mode=display">c(x, y;Δx, Δy)=\sum_{(u, v)\in W(x,y)}w(u,v)(P(u,v)-P(u+Δx, v+Δy))^2</script><p>W(x, y)表示以点(x, y)为中心的窗口；而w(x, y)是每个像素的权重，既可以是常数，也可以是高斯加权函数；P(u, v)表示坐标为(u, v)的像素值。上述公式表示计算窗口滑动前后，窗口中的每个像素及其对应像素的差值加权总和。</p><p>对平移后的像素点进行泰勒一阶展开，可得：</p><script type="math/tex; mode=display">P(u+Δx, v+Δy)\approx P(u,v)+P_x(u,v)Δx+P_y(u,v)Δy</script><p>于是相似性c简化后如下，此时可以看出这是一个椭圆函数：</p><script type="math/tex; mode=display">c(x, y;Δx, Δy)\approx\ AΔx^2+2CΔxΔy+BΔy^2</script><p>其中</p><script type="math/tex; mode=display">A=I_x(x,y)^2\\B=I_y(x,y)^2\\C=I_x(x,y)I_y(x,y)</script><p>经过对角化消除C后得：</p><script type="math/tex; mode=display">c(x, y;Δx, Δy)\approx\ \lambda_1Δx^2+\lambda_2Δy^2</script><p>可以看出λ越大，表示平移前后的滑窗灰度值差别越大。λ1大表示在横坐标上相差较大，λ2大表示在纵坐标上相差较大。当λ1和λ2的值都很大且值相近时，判断(x, y)为物体角点。</p><p><img src="https://i.loli.net/2020/12/24/ZGvlFgDmr7i1yOT.png" alt="image.png"></p><p>于是可以通过计算<strong>角点响应值</strong>R来判断该点是否为角点。</p><script type="math/tex; mode=display">R=\lambda_1\lambda_2-k (\lambda_1+\lambda_2)^2</script><p>其中k为系数，通常取值0.04 ~ 0.06。R&gt;&gt;0表示该点为角点，R≈0表示该点为平坦区，R&lt;&lt;0表示该点为边界。</p><h1 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.cornerHarris(img, blockSize, ksize, k)</span><br></pre></td></tr></table></figure><ul><li>img：输入图像；</li><li>blockSize：滑动窗口的大小；</li><li>ksize：Sobel求导中使用的矩阵大小；</li><li>k：计算角点响应值的系数。</li></ul><p>检测结果：</p><p><img src="https://i.loli.net/2020/12/24/8YvEOeaNGot7qSz.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Harris角点检测原理&quot;&gt;&lt;a href=&quot;#Harris角点检测原理&quot; class=&quot;headerlink&quot; title=&quot;Harris角点检测原理&quot;&gt;&lt;/a&gt;Harris角点检测原理&lt;/h1&gt;&lt;p&gt;Harris角点检测的基本原理：类似于边缘检测，只不过边缘检
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="角点检测" scheme="http://a-kali.github.io/tags/%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>图像直方图与均衡化</title>
    <link href="http://a-kali.github.io/2020/12/28/%E5%9B%BE%E5%83%8F%E7%9B%B4%E6%96%B9%E5%9B%BE%E4%B8%8E%E5%9D%87%E8%A1%A1%E5%8C%96/"/>
    <id>http://a-kali.github.io/2020/12/28/图像直方图与均衡化/</id>
    <published>2020-12-28T10:21:39.000Z</published>
    <updated>2021-02-09T00:57:44.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像直方图"><a href="#图像直方图" class="headerlink" title="图像直方图"></a>图像直方图</h1><p><strong>图像直方图</strong>即对图像中每个像素值出现次数的统计直方图。</p><p><img src="https://i.loli.net/2020/12/21/xt3kON27wSyeZ85.png" alt="image.png"></p><p>OpenCV：<code>cv2.calcHist(images, channels, mask, histSize, ranges)</code></p><ul><li>images：输入图像；</li><li>channels：输入一个包含[0, 1, 2]的列表选择通道，全选表示BGR，[0]表示灰度图；</li><li>mask：与images对齐的掩码，用于标记需要统计的像素，全选则输入None；</li><li>histSize：直方图横坐标的个数；</li><li>ranges：像素值范围，默认为[0, 256]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'cat.jpg'</span>)</span><br><span class="line">color = (<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">for</span> i, col <span class="keyword">in</span> enumerate(color):</span><br><span class="line">    hist = cv2.calcHist([img], [i], <span class="literal">None</span>, [<span class="number">256</span>], [<span class="number">0</span>, <span class="number">256</span>])</span><br><span class="line">    plt.plot(hist, color=col)</span><br><span class="line">    plt.xlim([<span class="number">0</span>, <span class="number">256</span>])</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/21/c4CFThHt6OUIVZr.png" alt="image.png"></p><h1 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h1><p><strong>直方图均衡化</strong>是将原图像通过某种变换，得到一幅灰度直方图为均匀分布的新图像的方法。其基本思想是对在图像中像素个数多的灰度级进行展宽，而对像素个数少的灰度级进行缩减。从而达到清晰图像的目的。</p><p><img src="https://i.loli.net/2020/12/21/DfW9auiAH875oBJ.png" alt="image.png"></p><p><img src="https://i.loli.net/2020/12/21/KpfIYFsvorSGyRT.png" alt="image.png"></p><p>步骤：</p><ol><li>确定图像的灰度级（通常情况下，如果我们的图像是彩色。需要将其转换为灰度图像，其灰度级一般是0-255）；</li><li>统计每一个灰度在原始图像上的像素所占总体的比例，即每个灰度的概率；</li><li>计算累加概率（将低灰度级的概率累加到高灰度级的概率上）；</li><li>根据公式求映射结果：</li></ol><script type="math/tex; mode=display">SS(i)=int((max(pix)-min(pix))*S(i)+0.5)</script><p><img src="https://i.loli.net/2020/12/21/k1VlTa9vjCKhszf.png" alt="image.png"></p><p>OpenCV代码：<code>cv2.equalizeHist(img)</code></p><p>虽然直方图均衡化后能让图像整体更加清晰，但有时候会使得原本突出的局部特征变得模糊（如图中的人脸）。</p><p><img src="https://i.loli.net/2020/12/21/Bep6arx71ok8FLb.png" alt="image.png"></p><p>可以使用<strong>自适应直方图均衡化</strong>解决这个问题。将图像划分为多个小区域（在opencv中默认为8×8），对每一个划分子域分别进行直方图均衡化。最后使用双线性插值对每一个子域的边界进行拼接。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a CLAHE object (Arguments are optional).</span></span><br><span class="line">clahe = cv2.createCLAHE(clipLimit=<span class="number">2.0</span>, tileGridSize=(<span class="number">8</span>,<span class="number">8</span>))</span><br><span class="line">img = clahe.apply(img)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像直方图&quot;&gt;&lt;a href=&quot;#图像直方图&quot; class=&quot;headerlink&quot; title=&quot;图像直方图&quot;&gt;&lt;/a&gt;图像直方图&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;图像直方图&lt;/strong&gt;即对图像中每个像素值出现次数的统计直方图。&lt;/p&gt;
&lt;p&gt;&lt;img sr
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="直方图" scheme="http://a-kali.github.io/tags/%E7%9B%B4%E6%96%B9%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（二）：骨干网络的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E9%AA%A8%E5%B9%B2%E7%BD%91%E7%BB%9C%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（二）：骨干网络的改进/</id>
    <published>2020-12-21T00:17:53.000Z</published>
    <updated>2020-12-21T00:21:32.458Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CSPDarknet53"><a href="#CSPDarknet53" class="headerlink" title="CSPDarknet53"></a>CSPDarknet53</h1><p>在Darknet53的基础上参考CSPNet加入了CSP(Cross Stage Paritial)结构。整个backbone划分为5个模块，每个模块下采样2倍。</p><p>如下图所示，CSP结构仅仅是将特征的一部分直接concat到block的末尾，这个操作使得该block中的每个dense layer的梯度不再直接参与更浅层的梯度计算，而是新计算出一个值（于是可以在反向传播到浅层时，清除深层的dense layer梯度信息），大量减少了内存的消耗和计算瓶颈。</p><p><img src="https://i.loli.net/2020/12/21/hrMEojey9fZ13LR.png" alt="image-20201219060045088.png"></p><h1 id="Mish激活函数"><a href="#Mish激活函数" class="headerlink" title="Mish激活函数"></a>Mish激活函数</h1><script type="math/tex; mode=display">Mish = x*tanh(ln(1+e^x))</script><p><img src="https://i.loli.net/2020/12/19/7AFq31YoTClmSDt.png" alt="image.png"></p><p>可以看出Mish比ReLU更加平滑，在每个点上都是可微的，优化效果更好；同时允许了较小的负梯度流入，更好地保证信息的流动。在YOLOv4中仅backbone使用了Mish，其它地方都用的Leaky ReLU。</p><h1 id="Dropblock"><a href="#Dropblock" class="headerlink" title="Dropblock"></a>Dropblock</h1><p>Dropblock与Dropout类似。Dropout会随机丢弃网络中的一些信息，但卷积层对随机丢弃的信息并不敏感，即使随机丢弃一些像素，卷积也能从相邻的像素学到相同的信息。</p><p>于是Dropblock对整个局部区域进行丢弃，相当于对特征图进行CutOut操作。</p><p><img src="https://i.loli.net/2020/12/19/PiCgEXtkaUsK9cl.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;CSPDarknet53&quot;&gt;&lt;a href=&quot;#CSPDarknet53&quot; class=&quot;headerlink&quot; title=&quot;CSPDarknet53&quot;&gt;&lt;/a&gt;CSPDarknet53&lt;/h1&gt;&lt;p&gt;在Darknet53的基础上参考CSPNet加入了CSP(C
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv4（一）：输入端的改进</title>
    <link href="http://a-kali.github.io/2020/12/21/YOLOv4%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E8%BE%93%E5%85%A5%E7%AB%AF%E7%9A%84%E6%94%B9%E8%BF%9B/"/>
    <id>http://a-kali.github.io/2020/12/21/YOLOv4（一）：输入端的改进/</id>
    <published>2020-12-21T00:12:33.000Z</published>
    <updated>2020-12-21T00:15:33.884Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mosaic"><a href="#Mosaic" class="headerlink" title="Mosaic"></a>Mosaic</h1><p>YOLOv4中提出的<strong>Mosaic数据增强</strong>是参考2019年底提出的<strong>CutMix数据增强</strong>的方式，但CutMix只使用了两张图片进行拼接，而Mosaic则采用了4张图片，<strong>随机缩放</strong>、<strong>随机裁剪</strong>、<strong>随机排布</strong>的方式进行拼接。一张图相当于4张图，可以减少训练所需的batch size。</p><p><img src="https://i.loli.net/2020/08/17/XseLfUlb6SiREVW.png" alt="image.png"></p><h1 id="CmBN"><a href="#CmBN" class="headerlink" title="CmBN"></a>CmBN</h1><p>在梯度累积时，模型不同batch的参数梯度会累积在一起一次性反向传播更新参数，但BN的统计量却是在每个batch迭代更新的，这使得反向传播时会产生偏差。<strong>CmBN(Cross mini-Batch Normalization)</strong>则是在梯度累积的同时保证BN的统计量只累积更新，在反向传播的同时更新统计量。</p><p><img src="https://i.loli.net/2020/12/17/5rEOw9ouQyHj3n8.png" alt="image.png"></p><h1 id="SAT"><a href="#SAT" class="headerlink" title="SAT"></a>SAT</h1><p><strong>SAT(Self Adversarial Training)</strong>即自对抗训练，文中没有详细说明，可以参考论文Adversarial Examples for Semantic Segmentation and Object Detection。</p><p>第一阶段，将权重设置为固定，图片设置为可导，对目标检测正确的区域采用反向label（即错误的label），计算Loss并进行反向传播，对图片产生扰动，叠加到图片上实现攻击。此时图片肉眼看上去变化不大，但对于神经网络来说，图片已经更接近于反向label。在第二阶段就是正常训练，把被攻击了的图片输入网络进行训练。作者将其归纳为一种数据增强操作。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mosaic&quot;&gt;&lt;a href=&quot;#Mosaic&quot; class=&quot;headerlink&quot; title=&quot;Mosaic&quot;&gt;&lt;/a&gt;Mosaic&lt;/h1&gt;&lt;p&gt;YOLOv4中提出的&lt;strong&gt;Mosaic数据增强&lt;/strong&gt;是参考2019年底提出的&lt;stro
      
    
    </summary>
    
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
      <category term="YOLOv4" scheme="http://a-kali.github.io/tags/YOLOv4/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】MaxPooling和AveragePooling</title>
    <link href="http://a-kali.github.io/2020/12/21/MaxPooling%E5%92%8CAveragePooling/"/>
    <id>http://a-kali.github.io/2020/12/21/MaxPooling和AveragePooling/</id>
    <published>2020-12-21T00:10:42.000Z</published>
    <updated>2020-12-30T09:32:08.922Z</updated>
    
    <content type="html"><![CDATA[<p>在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了<strong>辨识度更高的特征</strong>，并且提供了<strong>非线性</strong>。</p><p>而Average Pooling通常用来做特征对齐，在<strong>减小特征图尺寸</strong>的同时，保证特征的<strong>完整性</strong>。比如说DenseNet的模块之间大多采用Average Pooling进行连接。或者是用于网络的最后一层，<strong>代替平铺操作</strong>，将三维特征图转化为一维向量。</p><p><strong>Q：MaxPooling的反向传播是如何进行的？</strong></p><p>梯度正常传播给最大值，并将其余非最大值的损失/梯度置零。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在CNN中，通常用来下采样的池化操作都是使用Max Pooling。因为Max Pooling选择了&lt;strong&gt;辨识度更高的特征&lt;/strong&gt;，并且提供了&lt;strong&gt;非线性&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;而Average Pooling通常用来做特征对齐，在
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="池化" scheme="http://a-kali.github.io/tags/%E6%B1%A0%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】Dropout</title>
    <link href="http://a-kali.github.io/2020/12/13/Dropout/"/>
    <id>http://a-kali.github.io/2020/12/13/Dropout/</id>
    <published>2020-12-13T00:13:47.000Z</published>
    <updated>2020-12-13T00:15:00.739Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout有什么作用？</strong></p><p>解决过拟合的问题。</p><p><strong>Q：Dropout如何实现？</strong></p><p>使该层每个神经元的激活层都有一定概率输出0。</p><p><strong>Q：Dropout 反向传播的处理</strong></p><p>反向传播时忽略被Dropout的神经元。</p><p><strong>Q：Dropout是失活神经元还是失活连接？</strong></p><p>失活神经元并清除其周围连接。</p><p><strong>Q：Dropout为什么能防止过拟合？</strong></p><ol><li>使模型泛化能力更强，不依赖于某些局部的特征；</li><li>Dropout可以看作多个共享部分参数的模型的集成（其实这条跟上面那条是共通的）；</li></ol><p><strong>Q：Dropout在训练和测试时有何不同？</strong></p><p>Dropout在测试时不会失活神经元。并且在训练时根据失活率p对神经元权重进行缩放，即除以1-p；或者在测试时乘以p。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout有什么作用？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决过拟合的问题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：Dropout如何实现？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;使该层每个神经元的激活层都有一定概率输出0。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Q：D
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="Dropout" scheme="http://a-kali.github.io/tags/Dropout/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】FLOPs与参数量的计算</title>
    <link href="http://a-kali.github.io/2020/12/13/FLOPs%E4%B8%8E%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E8%AE%A1%E7%AE%97/"/>
    <id>http://a-kali.github.io/2020/12/13/FLOPs与参数量的计算/</id>
    <published>2020-12-12T20:36:07.000Z</published>
    <updated>2021-01-20T01:17:19.603Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FLOPs"><a href="#FLOPs" class="headerlink" title="FLOPs"></a>FLOPs</h1><p><strong>FLOPs(floating point operations)</strong>，意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。注意不要和FLOPS(floating point operations per second)搞混。</p><p>拿卷积操作举例。这里需要注意，卷积的计算量如果从output map size考虑的话，是可以与input map size无关的。因为卷积的每一次矩阵相乘都一定输出output map的一个像素点，但如果从input map下手的话，还需要考虑stride和padding。</p><p>所以<strong>卷积层</strong>的FLOPs计算公式为：</p><script type="math/tex; mode=display">FLOPs=(2×C_i×K^2)×(H×W×C_o)</script><p>其中Ci和Co分别为输入输出的通道数，K为卷积核大小，H和W分别为output map的高和宽。</p><p>这个公式前半部分的运算就是每计算一个输出元素所需要的计算量，就是output map的元素数量。之所以要×2是因为一次卷积运算需要进行乘法和加法两种操作（相乘后求和），比如说两个3×3大小的矩阵相乘求和得到一个元素值，需要进行3×3次乘法操作和3×3-1次加法操作，再加上神经网络中的bias，运算量一共为2×3×3。</p><p>同理可得<strong>全连接层</strong>的FLOPs计算公式为：</p><script type="math/tex; mode=display">FLOPs=2×I×O</script><h1 id="参数量"><a href="#参数量" class="headerlink" title="参数量"></a>参数量</h1><p>卷积参数量的计算比较简单，这里带过一下：</p><script type="math/tex; mode=display">params=(K^2×C_i+1)×C_o</script><p>实际工程上要计算这两个值的话的话，感觉github上有挺多包的……</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FLOPs&quot;&gt;&lt;a href=&quot;#FLOPs&quot; class=&quot;headerlink&quot; title=&quot;FLOPs&quot;&gt;&lt;/a&gt;FLOPs&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;FLOPs(floating point operations)&lt;/strong&gt;，意指浮点运算数，
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="FLOPs" scheme="http://a-kali.github.io/tags/FLOPs/"/>
    
      <category term="参数量" scheme="http://a-kali.github.io/tags/%E5%8F%82%E6%95%B0%E9%87%8F/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>OpenCV中的平滑、腐蚀膨胀和边缘检测</title>
    <link href="http://a-kali.github.io/2020/12/13/OpenCV%E4%B8%AD%E7%9A%84%E5%B9%B3%E6%BB%91%E3%80%81%E8%85%90%E8%9A%80%E8%86%A8%E8%83%80%E5%92%8C%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/12/13/OpenCV中的平滑、腐蚀膨胀和边缘检测/</id>
    <published>2020-12-12T20:32:16.000Z</published>
    <updated>2020-12-30T09:13:33.947Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图像平滑与滤波"><a href="#图像平滑与滤波" class="headerlink" title="图像平滑与滤波"></a>图像平滑与滤波</h1><p>图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，这些图像滤波都是一些简单的卷积操作。</p><ul><li>均值滤波：即在一个卷积核的感受野内的平均值作为该像素点的值，opencv代码为<code>cv2.blur(img, (3, 3))</code>。</li><li>高斯滤波：高斯滤波会根据距离取周围的像素点加权和，越近的像素权重越大。opencv代码为<code>cv2.GaussianBlur</code>。由于高斯滤波对中心点权重最高，使得其对椒盐类的噪声处理效果较差。</li><li>中值滤波：即在一个卷积核的感受野内的中值作为该像素点的值，opencv代码为<code>cv2.midianBlur(img, (3, 3))</code>。中值滤波对中心点的取值较为绝对化，故对椒盐类的噪声处理能力较强。</li></ul><p>下面三张图分别为原图、高斯滤波结果、中值滤波结果。</p><p><img src="https://i.loli.net/2020/12/05/tHxuK4iXhCP85cz.png" alt="image.png"></p><h1 id="图像的形态学操作"><a href="#图像的形态学操作" class="headerlink" title="图像的形态学操作"></a>图像的形态学操作</h1><h2 id="1-腐蚀与膨胀"><a href="#1-腐蚀与膨胀" class="headerlink" title="1    腐蚀与膨胀"></a>1    腐蚀与膨胀</h2><p>进行膨胀(dilation)操作时，使用卷积核遍历图像，使用内核覆盖区域的最大相素值代替锚点位置的相素。这一最大化操作将会导致图像中的亮区扩大，因此名为膨胀 。腐蚀(erosion)则与膨胀相反。在OpenCV中代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kernel = np.ones((<span class="number">3</span>, <span class="number">3</span>), np.uint8)</span><br><span class="line">cv2.erode(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 腐蚀</span></span><br><span class="line">cv2.dilate(img, kernel, iterations=<span class="number">1</span>)  <span class="comment"># 膨胀</span></span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2020/12/05/XHDSuCR7yxOYIin.png" alt="image.png"></p><h2 id="2-开运算与闭运算"><a href="#2-开运算与闭运算" class="headerlink" title="2    开运算与闭运算"></a>2    开运算与闭运算</h2><p>开运算指的是先腐蚀后膨胀的操作，闭运算则是先膨胀后腐蚀的操作。通常用于消除噪点、沟壑，平滑物体轮廓。在OpenCV中代码如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)  # 开运算</span><br><span class="line">cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)  # 闭运算</span><br></pre></td></tr></table></figure><h2 id="3-梯度计算"><a href="#3-梯度计算" class="headerlink" title="3    梯度计算"></a>3    梯度计算</h2><p>使用膨胀的结果减去腐蚀的结果可以计算得到图像的梯度，但OpenCV中提供了更便捷的操作：<code>cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</code>。</p><h1 id="Sobel算子"><a href="#Sobel算子" class="headerlink" title="Sobel算子"></a>Sobel算子</h1><p>Sobel算子是计算机视觉领域的一种重要处理方法。主要用于获得数字图像的<strong>一阶梯度</strong>，常用于<strong>边缘检测</strong>（注意与轮廓检测相区分）。Sobel算子是把图像中每个像素的上下左右四领域的<strong>灰度值加权差</strong>，在边缘处达到极值从而检测边缘。</p><p>该算子包含两组3x3的矩阵，分别为横向及纵向，将之与图像作平面<strong>卷积</strong>，即可分别得出横向及纵向的差值。如果以A代表原始图像，Gx及Gy分别代表经横向及纵向边缘检测的图像，其公式如下：</p><p><img src="https://i.loli.net/2020/12/04/rIq3OVLCfDnWvpu.png" alt="image.png"></p><p>在OpenCV中，可通过<code>cv2.Sobel(src, ddepth, dx, dy, ksize)</code>调用Sobel算子。具体操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'pic.png'</span>, cv2.IMREAD_GRAYSCALE)  <span class="comment"># 读取灰度图</span></span><br><span class="line">sobelx = cv2.Sobel(img, cv2.CV_64F, <span class="number">1</span>, <span class="number">0</span>, ksize=<span class="number">3</span>)  <span class="comment"># x方向的梯度</span></span><br><span class="line">sobelx = cv2.convertScaleAbs(sobelx)  <span class="comment"># 取绝对值将复数转为正数</span></span><br><span class="line">sobely = cv2.Sobel(img, cv2.CV_64F, <span class="number">0</span>, <span class="number">1</span>, ksize=<span class="number">3</span>)  <span class="comment"># y方向的梯度</span></span><br><span class="line">sobely = cv2.convertScaleAbs(sobely)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将x和y方向分别求到的梯度加权求和</span></span><br><span class="line">sobelxy = cv2.addWeighted(sobelx, <span class="number">0.5</span>, sobely, <span class="number">0.5</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>求得的结果如下所示，其中左边是原图，右边是边缘检测结果：</p><p><img src="https://i.loli.net/2020/12/04/CgwknPWiDm12b7x.png" alt="image.png"></p><p>因为直接求两个方向的梯度效果较差，所以此处是对两个方向分别求梯度再加权平均，而不是直接求两个方向的梯度。</p><h1 id="Canny边缘检测"><a href="#Canny边缘检测" class="headerlink" title="Canny边缘检测"></a>Canny边缘检测</h1><p>Canny边缘检测算法是对图像进行的一系列用于边缘检测的操作步骤：</p><ol><li>使用高斯滤波器对图像进行平滑处理，滤除噪声；</li><li>计算图像中每个像素点的梯度强度和方向；</li><li>使用非极大值抑制消除边缘检测带来的杂散效应；</li><li>应用双阈值检测找出潜在的边缘，抑制孤立弱边缘。</li></ol><p><img src="https://i.loli.net/2020/12/05/f3BTRk6cFPYgqHG.png" alt="image.png"></p><p>OpenCV：<code>cv2.Canny(img, min_threshold, max_threshold)</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;图像平滑与滤波&quot;&gt;&lt;a href=&quot;#图像平滑与滤波&quot; class=&quot;headerlink&quot; title=&quot;图像平滑与滤波&quot;&gt;&lt;/a&gt;图像平滑与滤波&lt;/h1&gt;&lt;p&gt;图像平滑是指为了抑制噪声，使图像亮度趋于平缓的处理方法就是图像平滑。图像平滑可以通过各种滤波来实现，
      
    
    </summary>
    
      <category term="图像处理" scheme="http://a-kali.github.io/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
      <category term="OpenCV" scheme="http://a-kali.github.io/tags/OpenCV/"/>
    
      <category term="边缘检测" scheme="http://a-kali.github.io/tags/%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】手撕卷积</title>
    <link href="http://a-kali.github.io/2020/12/03/%E6%89%8B%E6%92%95%E5%8D%B7%E7%A7%AF/"/>
    <id>http://a-kali.github.io/2020/12/03/手撕卷积/</id>
    <published>2020-12-03T09:18:27.000Z</published>
    <updated>2020-12-03T09:19:44.350Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(z, K, b, padding=<span class="params">(<span class="number">0</span>, <span class="number">0</span>)</span>, strides=<span class="params">(<span class="number">1</span>, <span class="number">1</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    多通道卷积前向过程</span></span><br><span class="line"><span class="string">    :param z: 卷积层矩阵,形状(N,C,H,W)，N为batch_size，C为通道数</span></span><br><span class="line"><span class="string">    :param K: 卷积核,形状(C,D,k1,k2), C为输入通道数，D为输出通道数</span></span><br><span class="line"><span class="string">    :param b: 偏置,形状(D,)</span></span><br><span class="line"><span class="string">    :param padding: padding</span></span><br><span class="line"><span class="string">    :param strides: 步长</span></span><br><span class="line"><span class="string">    :return: 卷积结果</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    padding_z = np.lib.pad(z, ((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), (padding[<span class="number">0</span>], padding[<span class="number">0</span>]), (padding[<span class="number">1</span>], padding[<span class="number">1</span>])), <span class="string">'constant'</span>, constant_values=<span class="number">0</span>)  <span class="comment"># 在H和W维度上进行padding</span></span><br><span class="line">    N, _, height, width = padding_z.shape</span><br><span class="line">    C, D, k1, k2 = K.shape</span><br><span class="line">    conv_z = np.zeros((N, D, <span class="number">1</span> + (height - k1) // strides[<span class="number">0</span>], <span class="number">1</span> + (width - k2) // strides[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> np.arange(N):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> np.arange(D):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> np.arange(height - k1 + <span class="number">1</span>)[::strides[<span class="number">0</span>]]:</span><br><span class="line">                <span class="keyword">for</span> w <span class="keyword">in</span> np.arange(width - k2 + <span class="number">1</span>)[::strides[<span class="number">1</span>]]:</span><br><span class="line">                    conv_z[n, d, h // strides[<span class="number">0</span>], w // strides[<span class="number">1</span>]] = np.sum(padding_z[n, :, h:h + k1, w:w + k2] * K[:, d]) + b[d]</span><br><span class="line">    <span class="keyword">return</span> conv_z</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span clas
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="卷积" scheme="http://a-kali.github.io/tags/%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>CenterLoss简述</title>
    <link href="http://a-kali.github.io/2020/12/03/CenterLoss%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/12/03/CenterLoss简述/</id>
    <published>2020-12-03T09:18:01.000Z</published>
    <updated>2020-12-12T20:43:25.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Center-Loss"><a href="#Center-Loss" class="headerlink" title="Center Loss"></a>Center Loss</h1><p>对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图为神经网络在MNIST数据集上进行手写体数字分类任务得到的特征分布，在类与类之间能看到很明显的界限，能够便于softmax对特征进行分类。但是仍然可以看出部分不同类的样本距离很近，而同类的样本分布距离较远，这有可能对最终的分类结果有影响。</p><p><img src="https://i.loli.net/2020/12/02/4M5jQxUcpW3H6G1.png" alt="image.png"></p><p>而<strong>Center Loss</strong>的提出就是为了<strong>缩小类内(intra-class)距离</strong>。其公式表示如下：</p><script type="math/tex; mode=display">L_c=\frac{1}{2}\sum^m_{i=1}‖x_i-c_{yi}‖^2_2</script><p>其中$c_{yi}$表示第y个类别的特征中心，$x_i$表示全连接层之前的特征，$m$表示mini-batch的大小。该损失函数试图使每个样本的特征向其类别中心靠近。</p><p>至于类中心怎么更新迭代，这里就不写了（公式太复杂了yingyingying）。反正最终得到的特征分布如下：</p><p><img src="https://i.loli.net/2020/12/02/pdn8rZzMEN7OaoA.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Center-Loss&quot;&gt;&lt;a href=&quot;#Center-Loss&quot; class=&quot;headerlink&quot; title=&quot;Center Loss&quot;&gt;&lt;/a&gt;Center Loss&lt;/h1&gt;&lt;p&gt;对于常见的图像分类问题，我们通常使用交叉熵损失函数监督神经网络。下图
      
    
    </summary>
    
    
      <category term="损失函数" scheme="http://a-kali.github.io/tags/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>CycleGAN简述</title>
    <link href="http://a-kali.github.io/2020/11/19/CycleGAN%E7%AE%80%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2020/11/19/CycleGAN简述/</id>
    <published>2020-11-19T04:18:04.000Z</published>
    <updated>2020-11-19T04:19:12.186Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks</a></p><p>PyTorch代码：<a href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank" rel="noopener">https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix</a></p><p>CycleGAN（或许）是首个将GAN用于风格迁移的网络。</p><p>CycleGAN的作用本质是Image2Image的风格迁移，并不是传统意义上的生成。</p><p><img src="https://i.loli.net/2020/11/16/x2lpJWfVCsqhyja.png" alt="image.png"></p><p>相比于其它Image2Image的网络，CycleGAN并不需要成对的样本数据用来训练，而仅仅需要两组无关的图片，其中目标样本通常为一组风格相似的图片集。</p><p><img src="https://i.loli.net/2020/11/16/Af6MzV79s8NCoTE.png" alt="image.png"></p><p>为了避免网络将X中所有样本映射到Y中的某一个样本，CycleGAN同时包含Y→X的映射，要求网络能够对自己生成的图片进行还原。这使得F(X)必须包含X的内容信息，因此更容易学习Y整体的风格信息（大概）。</p><p><img src="https://i.loli.net/2020/11/16/ivueyVzcxMwRrAk.png" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks&quot;&gt;Unpaired Image-to-Image Translation using 
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="GAN" scheme="http://a-kali.github.io/tags/GAN/"/>
    
  </entry>
  
  <entry>
    <title>【面试向】如何进行多标签分类</title>
    <link href="http://a-kali.github.io/2020/11/12/%E5%A6%82%E4%BD%95%E8%BF%9B%E8%A1%8C%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB/"/>
    <id>http://a-kali.github.io/2020/11/12/如何进行多标签分类/</id>
    <published>2020-11-11T16:37:46.000Z</published>
    <updated>2020-11-11T16:38:38.377Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：多标签分类怎么解决？</strong></p><p>解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。</p><p>第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这应该是最简单常见的方法，但该方法忽略了标签与标签之间的相关性。</p><p>第二种是使用CNN+RNN+Embedding的策略（参考文献：<a href="https://arxiv.org/abs/1604.04573" target="_blank" rel="noopener">CNN-RNN: A Unified Framework for Multi-label Image Classification</a>）。该方法考虑到了各个标签之间的关联性，将每个标签映射到高维空间上。每次运行预测一个标签，并根据该标签预测下一个标签。理论上该方法的准确率会比较高，但时间效率很低。</p><p><img src="https://i.loli.net/2020/11/11/S5p13cqzibZQfK2.png" alt="image.png"></p><p><a href="https://www.kaggle.com/c/imet-2019-fgvc6" target="_blank" rel="noopener">Kaggle: iMet Collection 2019 - FGVC6</a> 大都会文物分类竞赛就是多标签分类任务，几乎所有参赛者都使用第一种方法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：多标签分类怎么解决？&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;解决多标签分类问题的方法有很多，个人感觉只有两种比较好用。&lt;/p&gt;
&lt;p&gt;第一种是将n标签问题转化为n个二分类的问题，即模型输出为长度为n的二值向量，每个值对应一个标签，用于表示样本是否包含该标签。这
      
    
    </summary>
    
      <category term="面试" scheme="http://a-kali.github.io/categories/%E9%9D%A2%E8%AF%95/"/>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>DB：基于可微二值化的实时场景文本检测</title>
    <link href="http://a-kali.github.io/2020/11/12/DB%EF%BC%9A%E5%9F%BA%E4%BA%8E%E5%8F%AF%E5%BE%AE%E4%BA%8C%E5%80%BC%E5%8C%96%E7%9A%84%E5%AE%9E%E6%97%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B/"/>
    <id>http://a-kali.github.io/2020/11/12/DB：基于可微二值化的实时场景文本检测/</id>
    <published>2020-11-11T16:36:11.000Z</published>
    <updated>2021-01-15T03:15:22.431Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1911.08947" target="_blank" rel="noopener">Real-time Scene Text Detection with Differentiable Binarization</a></p><p>Github：<a href="https://github.com/MhLiao/DB（代码结构很复杂）" target="_blank" rel="noopener">https://github.com/MhLiao/DB（代码结构很复杂）</a></p><p><strong>Structure</strong></p><p>DB(Differentiable Binarization)是一个轻量级的、基于分割的场景文本检测(Scene Text Detection, STD)模型。该模型的原理简洁易懂，在本文中就简单介绍一下。</p><p><img src="https://i.loli.net/2020/11/04/MF73kDbCcueraNL.png" alt="image.png"></p><p>上图展示了DB的模型结构。前半部分是常见的32倍下采样+8倍上采样+特征融合，以及参考FPN采用了后四层反卷积的输出concat到一起进行预测。图中的pred是3×3卷积，输出两张map分别用来表示概率和阈值。使用阈值map对概率map进行二值化后，就将输出结果分为了前景（文本域）和背景。</p><p>这个自适应阈值能把分布比较密集的文本域给隔开，避免混淆成一个文本域。</p><p>值得一提的是阈值map只在训练的时候使用，在测试时仅使用固定阈值。估计在测试时使用DB并不能得到比较有效的提升，出于运算量的考虑决定去掉这部分。</p><p><strong>Loss</strong></p><p>损失函数分为三部分：概率图损失，阈值损失，二值图损失。其中概率图和二值图都使用交叉熵损失函数，而阈值损失使用的是L1损失函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1911.08947&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Real-time Scene Text Detection with Differentiable Binariz
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="场景文本检查" scheme="http://a-kali.github.io/tags/%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%9F%A5/"/>
    
      <category term="DB" scheme="http://a-kali.github.io/tags/DB/"/>
    
  </entry>
  
</feed>
