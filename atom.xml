<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>某科学のBLOG</title>
  
  <subtitle>与其感慨路难行，不如马上出发</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://a-kali.github.io/"/>
  <updated>2020-02-01T12:07:17.937Z</updated>
  <id>http://a-kali.github.io/</id>
  
  <author>
    <name>Hsaki</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>SCI写作入门（1）——SCI简介及写作顺序</title>
    <link href="http://a-kali.github.io/2020/02/01/SCI%E5%86%99%E4%BD%9C%E5%85%A5%E9%97%A8%EF%BC%881%EF%BC%89%E2%80%94%E2%80%94SCI%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%86%99%E4%BD%9C%E9%A1%BA%E5%BA%8F/"/>
    <id>http://a-kali.github.io/2020/02/01/SCI写作入门（1）——SCI简介及写作顺序/</id>
    <published>2020-02-01T10:30:13.000Z</published>
    <updated>2020-02-01T12:07:17.937Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-SCI论文组成"><a href="#1-SCI论文组成" class="headerlink" title="1    SCI论文组成"></a>1    SCI论文组成</h1><ul><li>Title：文章标题</li><li>Abstract：摘要</li><li>Introduction：引言</li><li>Methods：实验所使用的材料和方法</li><li>Results：实验结果</li><li>Discussion：对实验结果的讨论和分析</li><li>Reference：参考文献</li><li>Figures and Tables：图片、图表</li><li>Figure legends：图片、图表说明</li><li>Acknowledgements：致谢</li></ul><h2 id="1-1-Title"><a href="#1-1-Title" class="headerlink" title="1.1    Title"></a>1.1    Title</h2><p>简明、准确地总结文章内容，包含关键词，使文章更容易被所需要的人检索到。</p><h2 id="1-2-Abstract"><a href="#1-2-Abstract" class="headerlink" title="1.2    Abstract"></a>1.2    Abstract</h2><p>通过一段话将你的<strong>研究背景、内容、目的、结果以及研究意义</strong>告诉读者，使读者仅仅通过读Abstract就能明白这篇文献是否是其感兴趣的（或者使读者通过读Abstract就能对你的研究产生兴趣）。是用最少的词表述最重要的内容。</p><h2 id="1-3-Introduction"><a href="#1-3-Introduction" class="headerlink" title="1.3    Introduction"></a>1.3    Introduction</h2><p>Introduction是比较难写的一部分。在Introduction中，作者需要比Abstract更为详细地回答以下五个问题：</p><ol><li><p>为什么——为什么要做这个研究？</p></li><li><p>是什么——你的<strong>科学假说</strong>是什么？</p></li><li><p>做什么——你的<strong>研究方法</strong>是什么？</p></li><li><p>什么结果——你的<strong>研究发现</strong>是什么？</p></li><li><p>什么意义——为什么你的研究很重要？</p></li></ol><h2 id="1-4-Methods"><a href="#1-4-Methods" class="headerlink" title="1.4    Methods"></a>1.4    Methods</h2><p>详细叙述研究采用的方法，能够让读者参考该文献复现出实验研究。</p><h2 id="1-5-Results"><a href="#1-5-Results" class="headerlink" title="1.5     Results"></a>1.5     Results</h2><p>开门见山地告诉读者你发现了什么现象、得出了什么数据和结论。同时在这一部分，作者需要灵活地使用图片、表格等方式更加形象地展示研究成果。</p><h2 id="1-6-Discussion"><a href="#1-6-Discussion" class="headerlink" title="1.6    Discussion"></a>1.6    Discussion</h2><p>Discussion同样是比较难写的一部分。作者需要对自己的研究进行总结和归纳；同时需要讨论该研究与相关研究的关系，其结果是否具有一致性，如何去解释不一致性的产生；最后还要说明自己研究的局限性以及如何去改进这种局限性。提出研究和价值。<strong>Discussion直接决定了整篇文章的质量，对文章能否顺利发表有很大的影响</strong>。</p><h2 id="1-7-Reference"><a href="#1-7-Reference" class="headerlink" title="1.7    Reference"></a>1.7    Reference</h2><ul><li>列举出参考文献，为问题提供背景，为方法提供出处，为结论提供证据。 </li><li>对前人工作和观点的一种尊重和赞同。</li><li>为读者提供更多的信息来源。</li></ul><h1 id="2-SCI论文合理写作顺序"><a href="#2-SCI论文合理写作顺序" class="headerlink" title="2    SCI论文合理写作顺序"></a>2    SCI论文合理写作顺序</h1><ol><li>建议首先制作论文图片和表格，表格和图片就是论文的骨架！完成了图片和表格后，整个文章的基本框架便在心里有数。</li><li>完成表格和图片的说明（Figure Legend），趁热打铁。</li><li>完成论文的Results。一个图片/表格对应一个小结，合起来就是整个Results。</li><li>完成论文的Introduction。对于一件你已经完成的事情，写Introduction来解释你为什么要做这件事情。</li><li>完成Discussion。相当于Introduction的后续，对提出的工作进行总结。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-SCI论文组成&quot;&gt;&lt;a href=&quot;#1-SCI论文组成&quot; class=&quot;headerlink&quot; title=&quot;1    SCI论文组成&quot;&gt;&lt;/a&gt;1    SCI论文组成&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Title：文章标题&lt;/li&gt;
&lt;li&gt;Abstract：摘
      
    
    </summary>
    
    
      <category term="论文写作" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E5%86%99%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>【论文解读】Detecting Depression with AI (AVEC2019)</title>
    <link href="http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/"/>
    <id>http://a-kali.github.io/2020/02/01/Detecting-Depression-with-AI-AVEC2019/</id>
    <published>2020-02-01T02:45:56.000Z</published>
    <updated>2020-02-01T08:17:25.251Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1907.11510v1" target="_blank" rel="noopener">AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression with AI, and Cross-Cultural Affect Recognition</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>视听情感挑战与工作坊(AVEC 2019)“心智状态、人工智能检测抑郁、跨文化情感识别”是第九届比赛，旨在将多媒体处理和机器学习方法用于自动视听健康和情感分析，所有参与者在相同的条件下严格竞争。该挑战的目标是为多模态信息处理提供一个通用的基准测试集，并将健康和情绪识别社区以及视听处理社区集合在一起，以比较现实生活数据中各种健康和情绪识别方法的相对优点。本文介绍了今年的主要创新点、挑战指南、使用的数据和基线系统在三个拟议任务上的表现:心理状态识别、人工智能抑郁评估和跨文化情感感知。</p><h1 id="Depression-Detection-with-AI"><a href="#Depression-Detection-with-AI" class="headerlink" title="Depression Detection with AI"></a>Depression Detection with AI</h1><p>抑郁症，尤其是重度抑郁症( major depressive disorder, MDD)，是一种常见的心理健康问题，对人的思维、感觉和行为方式有负面影响。它会导致各种情绪和身体问题，影响工作和个人生活的许多方面。世界卫生组织(WHO)在2015年宣布抑郁症是全球范围内导致疾病和残疾的主要原因:超过3亿人患有抑郁症。鉴于抑郁症的高患病率及其自杀风险，寻找新的诊断和治疗方法变得越来越重要。由于有令人信服的证据表明抑郁症和相关的精神健康障碍与行为模式的改变有关，人们越来越有兴趣使用自动人类行为分析来基于行为线索(如面部表情和说话韵律)进行计算机辅助抑郁症诊断。面部活动、手势、头部运动和表达能力等行为信号都与抑郁症密切相关。</p><p>计算机视觉可以追踪的面部表情和头部动作也是预测抑郁的好方法。据报道，更向下的凝视角度、不那么强烈的微笑、更短的平均微笑持续时间是抑郁症最显著的面部特征。此外，身体表情、手势、头部动作和语言线索也被报道为抑郁检测提供相关线索。综合所有这些证据，有人提议将情感计算技术集成到一个计算机代理中，该代理可以访问人们并识别精神疾病的语言和非语言指标。对创伤后应激障碍患者收集的数据表明，当代理人由充当WoZ的人驱动时，对其抑郁严重程度的自动评估(PHQ-8问卷)可以实现RMSE小于5；PHQ-8 range∈[0,24] 的cutpoint分别定义为轻度、中度、中度和重度抑郁症。这些结果需要进一步研究，因为代理完全由人工智能驱动，因为向导可能会将虚拟代理驱动到一种情况，从而减轻与抑郁症相关的模式的观察，或者自主代理可能在适当地进行访谈方面存在问题。</p><h1 id="Distress-Analysis-Interview-Corpus"><a href="#Distress-Analysis-Interview-Corpus" class="headerlink" title="Distress Analysis Interview Corpus"></a>Distress Analysis Interview Corpus</h1><p>扩展遇险分析访谈语料库(E-DAIC)是WOZ-DAIC的扩展版本，包含半临床访谈，旨在支持诊断焦虑、抑郁和创伤后应激障碍等心理困扰状况。收集这些访谈是为了创建一个计算机代理来采访人们，并识别精神疾病的语言和非语言指标。收集的数据包括音频和视频记录、使用谷歌云语音识别服务自动转录的文本以及广泛的问卷回答。这些面试是由一个叫做Ellie的动画虚拟面试官进行的。在theWoZ的面试中，虚拟代理由另一个房间的人类面试官(巫师)控制，而在Al的面试中，代理以完全自主的方式使用不同的自动感知和行为生成模块。</p><p>为了达到挑战的目的，E-DAIC数据集被划分为培训、开发和测试集，同时保留了演讲者的整体多样性——在年龄、性别分布和8项患者健康问卷(pho8)评分方面——在这些划分内。训练和开发集包括WoZ和人工智能场景的混合，而测试集仅由自主人工智能收集的数据构成。关于扬声器在分区上的分布的详细信息见表2。</p><p><img src="https://i.loli.net/2020/02/01/teQ2Za9kod7L4M1.png" alt="K_UZ`K8@_ADCD0_THOI_~35.png"></p><h1 id="Baseline-System"><a href="#Baseline-System" class="headerlink" title="Baseline System"></a>Baseline System</h1><p>对于抑郁检测基线，我们使用单层64-d GRU作为我们的递归网络，其失步正规化率为20%，然后使用64-d全连通层获得单值回归评分。为了处理偏差，我们将PHQ-8分数标签转换为浮点数，方法是在培训之前按25的倍数缩小比例。使用CCC损失函数和评价分数对网络进行训练和评价，使用原始的PHQ量表报告RMSE结果。批处理大小为15的方法得到了一致的使用，并且在不同的特性集之间优化了学习率。为了使数据适合GPU内存，为会话分配了最大的序列长度。对于MFCCs和eGeMAPS LLDs，以及诸如DeepSpectrum、ResNet和VGG等高维深表示，使用的最大序列长度为20分钟。另外，对于ResNet、VGG和DEEP SPECTRUM表示帧，根据维数的不同，将保留两帧中的一帧或四帧中的一帧，以便将数据加载到内存中。融合不同的视听表现是通过平均他们的分数来实现的。</p><p>DDS的基线结果见表6。结果表明，在开发集上，利用深度谱(DS-VGG)特征获取音频特征的最佳CCC评分，利用ResNet特征获取视觉特征的最佳CCC评分。这些结果表明表达的力量深层神经网络学习的大量数据时在不同的上下文中使用他们最初的设计,这是证实与ResNet视觉模型在测试集上实现最好的结果,尽管相对较低的CCC。</p><p>不同表现形式的融合在开发集上获得了最好的结果，测试集上返回的RMSE比使用AVEC 2017基线系统在DAIC-WoZ数据集上获得的RMSE稍好一些;AVEC2019年的RMSE=6.37，而AVEC 2017年的RMSE=6.97。然而，为今年的挑战开发的基线系统更加复杂，与今年的GRU-RNNs相比，它是一个简单的线性回归模型，因此，根据2017年AVEC抑郁亚挑战的最佳结果(RMSE=4.99)，应该最好地考虑相应的分数。</p><p>根据从与虚拟代理的交互中获得的抑郁程度自动感知的结果，当代理仅由人工智能驱动时，识别似乎比由人作为WoZ驱动时更具挑战性。这一观察结果为设计抑郁症诱因的设计带来了有趣的研究问题。，通过强化学习，根据agent的交互方式。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1907.11510v1&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;AVEC 2019 Workshop and Challenge: State-of-Mind, Detecti
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="抑郁评估" scheme="http://a-kali.github.io/tags/%E6%8A%91%E9%83%81%E8%AF%84%E4%BC%B0/"/>
    
  </entry>
  
  <entry>
    <title>胶囊网络（Capsule Network）</title>
    <link href="http://a-kali.github.io/2020/01/11/%E8%83%B6%E5%9B%8A%E7%BD%91%E7%BB%9C%EF%BC%88Capsule-Network%EF%BC%89/"/>
    <id>http://a-kali.github.io/2020/01/11/胶囊网络（Capsule-Network）/</id>
    <published>2020-01-11T12:03:27.000Z</published>
    <updated>2020-01-11T12:03:27.803Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>随机权值平均(Stochastic Weight Averaging,  SWA)</title>
    <link href="http://a-kali.github.io/2020/01/11/%E9%9A%8F%E6%9C%BA%E6%9D%83%E5%80%BC%E5%B9%B3%E5%9D%87-Stochastic-Weight-Averaging-SWA/"/>
    <id>http://a-kali.github.io/2020/01/11/随机权值平均-Stochastic-Weight-Averaging-SWA/</id>
    <published>2020-01-11T01:15:59.000Z</published>
    <updated>2020-01-23T04:57:31.951Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1803.05407" target="_blank" rel="noopener">Averaging Weights Leads to Wider Optima and Better Generalization</a></p><p>SWA是一种较为先进的模型融合方法。传统的模型融合通常使用多个模型进行预测，再使用某种方法来对预测结果取平均值得到最终的预测值。而SWA仅需要训练单个模型即可进行融合。</p><p>SWA一定程度上参考了传统的<strong>快照集成（snapshot ensembling）</strong>，即每训练 20-40 epochs，对局部最优模型保存一个权重快照，然后利用<strong>余弦退火</strong>的特性跳出局部最优，寻找另一个局部最优解；最后使用多个保存的模型权重进行预测并对预测结果进行融合。这种集成方法仅需要训练单个模型就能得出接近多模型融合的效果。</p><p>但快照集成也保留了传统模型融合的部分缺点：需要保存多个模型，较为占用存储空间；训练周期过长，每收敛一个新模型需要经过较多epochs。</p><p>而SWA的作者发现 2-4 epochs <strong>循环学习率</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1803.05407&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Averaging Weights Leads to Wider Optima and Better General
      
    
    </summary>
    
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模型融合" scheme="http://a-kali.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    
  </entry>
  
  <entry>
    <title>2020年上半年论文阅读计划</title>
    <link href="http://a-kali.github.io/2020/01/06/2020%E5%B9%B4%E4%B8%8A%E5%8D%8A%E5%B9%B4%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E8%AE%A1%E5%88%92/"/>
    <id>http://a-kali.github.io/2020/01/06/2020年上半年论文阅读计划/</id>
    <published>2020-01-06T10:58:49.000Z</published>
    <updated>2020-01-19T06:13:01.323Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。</p><p>目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标跟踪、实例分割、全景分割等），顺便再补一下去年没填的坑（比如LSTM、FPN、YOLOv3、知识蒸馏等）。</p><p><a href="https://arxiv.org/abs/1811.00116" target="_blank" rel="noopener">Face Recognition: From Traditional to Deep Learning Methods</a>：人脸识别综述</p><p><a href="https://arxiv.org/abs/1912.04977" target="_blank" rel="noopener">Advances and Open Problems in Federated Learning</a>：联邦学习综述</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近一直在打比赛，临近年关总算是抽出了点时间写一篇计划类的 blog 了。原本放假前计划了很多事情，但一开始比赛就打乱了所有的计划。&lt;/p&gt;
&lt;p&gt;目前计划是打算系统地学习一下人脸识别、视频处理和NAS方面的内容，大概了解下一些新兴技术的基本原理（比如联邦学习、GAN、目标
      
    
    </summary>
    
      <category term="计划" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E5%88%92/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="计划" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Bengali.AI Handwritten Grapheme Classification 比赛记录</title>
    <link href="http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/"/>
    <id>http://a-kali.github.io/2020/01/03/Bengali-AI-Handwritten-Grapheme-Classification/</id>
    <published>2020-01-03T14:33:32.000Z</published>
    <updated>2020-01-16T07:02:22.977Z</updated>
    
    <content type="html"><![CDATA[<div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Please enter the password to read the blog." />    <label for="pass">Please enter the password to read the blog.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">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</div><script src="/lib/crypto-js.js"></script><script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
    
    <summary type="html">
    
      The article has been encrypted, please enter your password to view.&lt;br&gt;
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="Kaggle" scheme="http://a-kali.github.io/tags/Kaggle/"/>
    
      <category term="比赛记录" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>DeepLab全家桶（From v1 to v3+）</title>
    <link href="http://a-kali.github.io/2019/12/13/DeepLab%E5%85%A8%E5%AE%B6%E6%A1%B6/"/>
    <id>http://a-kali.github.io/2019/12/13/DeepLab全家桶/</id>
    <published>2019-12-13T15:19:07.000Z</published>
    <updated>2019-12-17T08:59:29.078Z</updated>
    
    <content type="html"><![CDATA[<h1 id="DeepLabv1"><a href="#DeepLabv1" class="headerlink" title="DeepLabv1"></a>DeepLabv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1412.7062v3" target="_blank" rel="noopener">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</a></p><p>其实挺烦看这种远古论文的，引用的算法现在都不太常见，使用的措辞也和现在不太一样。该论文主要引入了<strong>空洞卷积(Astrous/Dilated Convolution)</strong>和<strong>条件随机场(Conditional Random Field, CRF)</strong>。</p><p>空洞卷积，顾名思义，即是在卷积核权重之间注入空洞，<strong>使用小卷积核的计算量获得大卷积核的感受野</strong>。（如理解有误请邮件指正）</p><p>空洞卷积比传统卷积多一个参数为<strong>采样率(dilation rate)</strong>，表示一个卷积核中采样的间隔。</p><p><img src="https://s2.ax1x.com/2019/12/14/Q2DWex.gif" alt="Q2DWex.gif"></p><p>条件随机场涉及到很多机器学习的知识，学起来比较耗时间，而且在后来的DeepLab版本中被取代，所以此处暂略，有机会再补上。</p><h1 id="DeepLabv2"><a href="#DeepLabv2" class="headerlink" title="DeepLabv2"></a>DeepLabv2</h1><p>论文地址：<a href="https://arxiv.org/abs/1606.00915" target="_blank" rel="noopener">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs</a></p><p>比起v1，v2的主要改动是增加了<strong>带孔空间金字塔池化(ASPP)</strong>模块，其思想来源于SPPnet。但是文中对ASPP的阐述非常少，完全没有讲清楚ASPP的机制，只能通过论文中的图片和网上的博客来猜。</p><p><img src="https://i.loli.net/2019/12/15/MgRpErQ4utse9ND.png" alt="YM_ISX2ECM53ZW3_4T7HNYJ.png"></p><p><img src="https://i.loli.net/2019/12/15/6S7hpAo3ZiefQBN.png" alt="_H7RQ3P@__TYL_4_87Z0H05.png"></p><p>可以看出，ASPP使用了几种不同采样率的空洞卷积，对一张特征图得出多个分支后，最终concat到一起。我到现在也没搞明白为啥叫“空间金字塔池化”而不是“空间金字塔卷积”。</p><h1 id="DeepLabv3"><a href="#DeepLabv3" class="headerlink" title="DeepLabv3"></a>DeepLabv3</h1><p>论文地址：<a href="https://arxiv.org/abs/1706.05587" target="_blank" rel="noopener">Rethinking Atrous Convolution for Semantic Image Segmentation</a></p><p>提出了串联(cascade)和并联(parallel)两种格式，并指出并联效果更好。</p><p><img src="https://i.loli.net/2019/12/16/3PZxML24biRdpGj.png" alt="YF@__L_0M@_0RUKBG_N_O6C.png"></p><p><img src="https://i.loli.net/2019/12/15/tqhbdGpKZIwgf8A.png" alt="_C1PITC8~_S3@U_48_2_L5M.png"></p><p>网络去除了CRF，修改了一些参数，应用了一些新技术（比如批归一化）使模型更加精简。</p><p>虽然从文中看不出做了多少修改，但作者说性能得到了很大的提升。科科。</p><h1 id="DeepLabv3-1"><a href="#DeepLabv3-1" class="headerlink" title="DeepLabv3+"></a>DeepLabv3+</h1><p>论文地址：<a href="https://arxiv.org/abs/1802.02611v1" target="_blank" rel="noopener">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</a></p><p>8102年，deeplab终于将Encoder-Decoder结构加进网络里了，之前一直用的双线性插值做上采样。</p><p><img src="https://i.loli.net/2019/12/16/bHvnI59LjUo3JcQ.png" alt="V7U32G_QEI_GOMGI97N6LAG.png"></p><p><img src="https://i.loli.net/2019/12/16/1UiclraR5NuOVvz.png" alt="__ZDD_GOD~3NX9_P_L@HSWA.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://blog.csdn.net/qq_31622015/article/details/90551107" target="_blank" rel="noopener">【语义分割系列：一】DeepLab v1 / v2 论文阅读翻译笔记</a></li><li><a href="https://blog.csdn.net/qq_21997625/article/details/87080576" target="_blank" rel="noopener">语义分割(semantic segmentation)–DeepLabV3之ASPP(Atrous Spatial Pyramid Pooling)代码详解</a></li><li><a href="https://blog.csdn.net/guo_rongxin/article/details/79842895" target="_blank" rel="noopener">deeplab v3论文翻译 Rethinking Atrous Convolution for Semantic Image Segmentation</a></li><li><a href="https://blog.csdn.net/fish_like_apple/article/details/82787705" target="_blank" rel="noopener">Deeplab相关改进的阅读记录（Deeplab V3和Deeplab V3+）</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;DeepLabv1&quot;&gt;&lt;a href=&quot;#DeepLabv1&quot; class=&quot;headerlink&quot; title=&quot;DeepLabv1&quot;&gt;&lt;/a&gt;DeepLabv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1412.7
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="DeepLab" scheme="http://a-kali.github.io/tags/DeepLab/"/>
    
  </entry>
  
  <entry>
    <title>SENet: Squeeze-and-Excitation Networks</title>
    <link href="http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/"/>
    <id>http://a-kali.github.io/2019/12/08/SENet-Squeeze-and-Excitation-Networks/</id>
    <published>2019-12-08T11:24:08.000Z</published>
    <updated>2019-12-17T09:00:21.295Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1709.01507" target="_blank" rel="noopener">Squeeze-and-Excitation Networks</a></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SENet 是北京 Momenta 公司研发团队提出的网络结构，该团队凭借SENet以极大的优势获得了 ImageNet 2017 竞赛的图像分类任务冠军。该网络至今(2019.12.12)仍然是最强力的分类网络之一。</p><p>我们从卷积网络开始说起。近些年来，卷积神经网络在很多领域上都取得了巨大的突破。而卷积核作为卷积神经网络的核心，通常被看做是在局部感受野上，将<strong>空间上（spatial）的信息和特征维度上（channel-wise）的信息</strong>进行聚合的信息聚合体。卷积神经网络由一系列卷积层、非线性层和下采样层构成，这样它们能够从全局感受野上去捕获图像的特征来进行图像的描述。</p><p>Inception 结构即是从空间上提取特征的典型案例，其使用不同大小的卷积核，聚合多种不同感受野的特征来提升性能。而 SENet 则是从通道特征之间的关系来考虑，采用了一种<strong>特征重标定(feature recalibration)</strong>的策略，即<strong>通过学习的方式来自动获取到每个特征通道的重要程度，然后依照这个重要程度去提升有用的特征并抑制对当前任务用处不大的特征。</strong></p><h2 id="Squeeze-and-Excitation-Blocks"><a href="#Squeeze-and-Excitation-Blocks" class="headerlink" title="Squeeze-and-Excitation Blocks"></a>Squeeze-and-Excitation Blocks</h2><p><img src="https://i.loli.net/2019/12/12/SmgX8AJWzBUvR9s.png" alt="I8_M4J____T1XB_FTW_P_H5.png"></p><p>上图是一个 SE 模块的示意图。</p><ul><li><p><strong>Squeeze</strong>：$F_{sq}(·)$ 将特征图进行<strong>全局平均池化(global average pooling)</strong>，使每个二维的特征通道对应生成一个实数，这个实数某种程度上具有全局的感受野。这意味着当 SE 模块用于靠近输入的层时，也能获得全局的感受野，这一点在很多任务中都是非常有用的。</p></li><li><p><strong>Excitation</strong>：$F_{ex}(·,W)$ 是一些<strong>全连接层和激活函数</strong>，其输入为squeeze的结果，<strong>输出为每个特征通道的重要性</strong>。具体分为4层网络：</p><ul><li>FC1：输入长度为 C 的向量，输出长度为 C/16 的向量。</li><li>ReLU：增加非线性</li><li>FC2：输入长度为 C/16 的向量，输出长度为 C 的向量。</li><li>Sigmoid：将输出限制到(0,1)之间，作为每个通道的权重，表示通道的重要性。</li></ul><p>使用两个而不是一个全连接层的好处在于：</p><ol><li>多加了一个ReLU层，具有更多的非线性，可以更好地拟合通道间复杂的相关性</li><li>极大地减少了参数量和计算量（约为原计算量的1/8）</li></ol></li><li><p><strong>Reweight</strong>：通过乘法将 Excitation 得到的通道权重加权到先前的特征上，完成在通道维度上对原始特征的重标定。</p></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>作者将 SE 模块拟合到了 Inception 和 ResNet 中，对网络性能产生了较大的增益。</p><p><img src="https://i.loli.net/2019/12/12/myFb9gUAhtNZSxV.png" alt="_LW`_DHP74CK_LED0COG_1P.png"></p><p>在理论上 SE 模块仅增加了网络 1% 的计算量；在实验中，由于 GPU 的架构原因，在 GPU 上的运算时间增加了 10%，而在 CPU 上仅增加了 2%。</p><p><img src="https://i.loli.net/2019/12/12/lzBcfFuq27vDGjw.png" alt="28R__T_T_RPEF6@_EIL4_8F.png"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a href="https://blog.csdn.net/wfei101/article/details/79672944" target="_blank" rel="noopener">Face Paper：SeNet论文详解</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1709.01507&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Squeeze-and-Excitation Networks&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SENet" scheme="http://a-kali.github.io/tags/SENet/"/>
    
  </entry>
  
  <entry>
    <title>轻量级卷积神经网络综述：从SqueezeNet到MixNet</title>
    <link href="http://a-kali.github.io/2019/12/05/%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0%EF%BC%9A%E4%BB%8ESqueezeNet%E5%88%B0EfficientNet/"/>
    <id>http://a-kali.github.io/2019/12/05/轻量级卷积神经网络综述：从SqueezeNet到EfficientNet/</id>
    <published>2019-12-05T10:40:16.000Z</published>
    <updated>2020-01-06T11:02:30.316Z</updated>
    
    <content type="html"><![CDATA[<p>在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗电量高则会导致汽车、手机等移动端续航能力变差，而只有轻量级的神经网络能解决这个问题。下面我将介绍近年来轻量级卷积神经网络的发展。</p><h1 id="1-SqueezeNet"><a href="#1-SqueezeNet" class="headerlink" title="1    SqueezeNet"></a>1    SqueezeNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1602.07360v2" target="_blank" rel="noopener">SqueezeNet:AlexNet-level accuracy with 50x fewer parameters and &lt;1MB model size</a></p><p>SqueezeNet网络的主要亮点在于提出了<strong>Fire Module</strong>来减少参数量。Fire Module 分为两部分：<strong>Squeeze 和 Expand</strong>。Squeeze层通过 1×1 卷积对特征图进行降维，减少参数量，Expand层分别使用 1×1 和 3×3 卷积对降维后的特征图进行处理后concat到一起。比起直接用3×3卷积，这种方法减少了一定的运算量。</p><p><img src="https://i.loli.net/2019/12/05/8TYQwPWMnCU3ok4.png" alt="(R7V7PD.png"></p><p>整个网络由多个Fire Module堆叠而成，很像GoogLeNet。右边两个网络结构参考了ResNet。</p><p><img src="https://i.loli.net/2019/12/05/rW4ugsYPjLUxXVh.png" alt="`YX7Opng"></p><h1 id="2-MobileNet-v1"><a href="#2-MobileNet-v1" class="headerlink" title="2    MobileNet v1"></a>2    MobileNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</a></p><p>MobileNet由谷歌公司提出，主要用于移动和嵌入式视觉应用，其亮点在于采用<strong>深度可分离卷积(Depth-wise Separable Convolution)</strong> 代替传统卷积。</p><h2 id="深度可分离卷积"><a href="#深度可分离卷积" class="headerlink" title="深度可分离卷积"></a>深度可分离卷积</h2><p>深度可分离卷积分为两步：</p><ol><li><strong>Depthwise convolution</strong>：对特征图各个通道进行卷积，每个卷积核只有一个通道且只负责特征图的一个通道。</li><li><strong>Pointwise convolution</strong>：使用1×1卷积将特征图串起来，得到和普通卷积一样的输出。</li></ol><p><img src="https://i.loli.net/2019/12/05/9bdIvHnKpY3XOG8.png" alt="W8I.png"></p><h2 id="运算量对比"><a href="#运算量对比" class="headerlink" title="运算量对比"></a>运算量对比</h2><p>假设输入图像为12×12×3，输出图像为8×8×256。</p><ul><li>Convolution：<ul><li>卷积核大小 5×5×3，卷积核数量 256</li><li>数据量：5×5×3×256 = 19200</li><li>计算量：仅考虑乘法运算，每产生一个输出值就要进行5×5×3次运算，一共要产生8×8×256个输出值，故 5×5×3×256×8×8 = 1228800。</li></ul></li><li>Depthwise Separable Convolution：<ul><li>Depthwise convolution：卷积核大小 5×5×1，卷积核数量 3</li><li>Pointwise convolution：卷积核大小 1×1×3，卷积核数量 256</li><li>数据量：5×5×1×3+1×1×3×256 = 843</li><li>计算量：5×5×1×3×8×8+1×1×3×8×8×256 = 53952</li></ul></li></ul><h2 id="网络模块"><a href="#网络模块" class="headerlink" title="网络模块"></a>网络模块</h2><p>左边是传统卷积，右边是深度可分离卷积。</p><p><img src="https://i.loli.net/2019/12/05/K2pm9axhdokJOiR.png" alt="FT.png"></p><h2 id="实验结果对比"><a href="#实验结果对比" class="headerlink" title="实验结果对比"></a>实验结果对比</h2><p>可以看到MobileNet在只牺牲了少量精确度的情况下节约了大量的运算量和网络参数。</p><p><img src="https://i.loli.net/2019/12/05/YwsgB2EQ7cxT1XC.png" alt="O.png"></p><h1 id="3-Xception"><a href="#3-Xception" class="headerlink" title="3    Xception"></a>3    Xception</h1><p>论文地址：<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise Separable Convolutions</a></p><p>Xception 借鉴了深度可分离卷积的思想并以此改进了Inception V3。</p><p><img src="https://i.loli.net/2019/12/05/Ht6YxQXpTvRbVA2.png" alt="Fpng"></p><p>图中是一个Xception模块，先用 1×1 卷积改变特征图的通道数，再对输出的每个通道分别进行 3×3 卷积，最后将 3×3 卷积的输出concat到一起。</p><h1 id="4-ShuffleNet-v1"><a href="#4-ShuffleNet-v1" class="headerlink" title="4    ShuffleNet v1"></a>4    ShuffleNet v1</h1><p>论文地址：<a href="https://arxiv.org/abs/1707.01083" target="_blank" rel="noopener">ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices</a></p><p>ShuffleNet是由旷视公司提出的轻量级网络，该网络结构主要使用了<strong>分组卷积(group convolution)</strong>和<strong>通道洗牌(channel shuffle)</strong>。</p><p><img src="https://i.loli.net/2019/12/05/tsFGvMo17V6QuIg.png" alt="MBng"></p><p>图a展示了分组卷积，即将通道均等分为多组，分别进行卷积操作（类似于深度可分离卷积）。但这样会导致组之间的信息不流通，对精度造成影响。于是使用通道洗牌的方式，对各组的通道进行交换。</p><p>下图是两种ShuffleNet单元：</p><p><img src="https://i.loli.net/2019/12/05/iNdkjWleXIDqsFK.png" alt="7EYQpng"></p><h1 id="5-MobileNet-v2"><a href="#5-MobileNet-v2" class="headerlink" title="5    MobileNet v2"></a>5    MobileNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a></p><h2 id="MobileNet-v1-存在问题"><a href="#MobileNet-v1-存在问题" class="headerlink" title="MobileNet v1 存在问题"></a>MobileNet v1 存在问题</h2><p>MobileNet v1 虽然很好地降低了模型运算量，但依然存在如下两个问题：</p><ol><li>MobileNet v1 的结构是类似于 VGG 的堆叠结构，而这种结构比起后来的 ResNet、GoogLeNet 来说性能不高。</li><li>Depthwise Convolution 的潜在问题：论文作者发现，由于<strong>深度残差卷积产生的特征图通道数较少，在 ReLU 的影响下很容易产生较大的信息损耗</strong>（这个故事告诉我们不要在压缩通道后用ReLU）。</li></ol><h2 id="MobileNet-v2-的创新点"><a href="#MobileNet-v2-的创新点" class="headerlink" title="MobileNet v2 的创新点"></a>MobileNet v2 的创新点</h2><p>为了解决 v1 存在的问题，v2 提出了以下改进方法：</p><ol><li><p><strong>Inverted Residual Block</strong>：首先从名字可以看出，这是从传统残差块演化而来的<strong>逆残差</strong>，两者主要的不同在于对 1×1 卷积的运用方式不同。传统的残差块使用 1×1 卷积降低特征图的通道数，减少 3×3 卷积的运算量；而逆残差则是用 1×1 卷积来提升维度，以便提升网络的准确度。可能作者觉得反正 Depthwise Convolution 运算量也不大，不如就牺牲一丢丢速度来提高一下精度吧。</p><p><img src="https://i.loli.net/2019/12/08/cBeWJVdTSU9t5Eb.png" alt="Q_P`KE8N_V3JY_L``4O6CXS.png"></p></li><li><p><strong>Linear Bottlenecks</strong>：对比 v1 和 v2 的结构可以看出，v2 使用线性函数替换了 v1 模块最后的ReLU6：</p></li></ol><p><img src="https://i.loli.net/2019/12/11/aSftLEykhVcC19I.png" alt="H_T83__0YR5Q6K_Y~V8_M_S.png"></p><h1 id="6-ShuffleNet-v2"><a href="#6-ShuffleNet-v2" class="headerlink" title="6    ShuffleNet v2"></a>6    ShuffleNet v2</h1><p>论文地址：<a href="https://arxiv.org/abs/1807.11164" target="_blank" rel="noopener">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</a></p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>通常用于神经网络的设计指导指标使用的是计算复杂度衡量指标：<strong>FLOPs</strong>，而不是更直接的评价指标：<strong>运行速度</strong>(speed)。而作者发现相同FLOPs的网络速度可能差别很大，认为FLOPs并不能作为网络性能的唯一衡量指标。</p><p>造成FLOPs和速度不成比例的原因：</p><ol><li>部分影响速度的原因没有被FLOPs包含在内：<ul><li><strong>内存访问成本</strong>(memory access cost, <strong>MAC</strong>)：这会使得强大的GPU算力受到限制。</li><li><strong>并行度</strong>(degree of parallelism)：在相同FLOPs的情况下，并行度高的网络模型速度远高于低并行度模型。</li></ul></li><li>不同的运行平台会影响FLOPs。比如说新版的CUDNN专门对 3 × 3 卷积运算进行了优化。</li></ol><p>出于这点考虑，作者提出了两点高效结构设计的指导性原则：</p><ol><li>应当使用直接的评价指标（e.g., 速度）而不是间接的（e.g., FLOPs）。</li><li>应当在规定的平台上进行评估。</li></ol><h2 id="高效卷积网络设计准测"><a href="#高效卷积网络设计准测" class="headerlink" title="高效卷积网络设计准测"></a>高效卷积网络设计准测</h2><ul><li><strong>G1: 当输入、输出channels数目相同时，conv计算所需的MAC最低。</strong>以深度可分离卷积(Depth-wise Separable Convolution)为例，其 pointwise convlution (i.e., 1×1 conv) 部分占用了其大部分复杂度。设$c_1$，$c_2$为 1 × 1 卷积的输入、输出通道数，$h$和$w$为特征图的高和宽，则 FLOPs 计算为 $B=hwc_1c_2$。内存访问操作次数为 $MAC=hw(c_1+c_2)+c_1c_2$。得出下面的不等式，仅当输入输出通道数相同时，MAC最小：<br>$$<br>MAC\geq 2\sqrt{hwB}+\frac{B}{hw}<br>$$</li><li><strong>G2: 过多的分组卷积(Group Convolution)会增大 MAC 开销。</strong>设分组数量为 $g$，从下面公式可以看出随着 $g$ 增加，MAC增加。  ：</li></ul><p>$$<br>MAC=hwc_1+\frac{Bg}{c_1}+\frac{B}{hx}<br>$$</p><ul><li><strong>G3: 网络碎片化(fragmentation)会减少并发度。</strong>这里的碎片化大概指的是模型的分支数量。比如说 NASNET-A 的分支数就高达13，而 ResNet 的分支数为2或3。作者通过实验证明，分支数量的提升会提高网络的准确率，但也会因降低GPU并行计算能力而影响效率。</li><li><strong>G4: Element-wise 操作的计算量不容忽视。</strong>element-wise包括激活、张量相加、添加偏置等，它们的共同特征就是FLOPS较小但是MAC相对较大。同时作者将 depthwise convolution 操作也算入了element-wise，因为其有着同样高的 MAC/FLOP 比率。</li></ul><p>目前的轻量级网络结构主要是是以FLOPS作为度量标准设计的，而没有考虑以上的几点属性。比如说，ShuffleNet v1使用了过多的分组卷积(与G2违背)、bottleneck-like块(与G1违背)；MobileNet v2使用倒置的bottleneck结构(与G1违背)，同时使用了深度卷积和ReLU在”thick”特征图上(与G4违背)；自动生成结构过多的使用了碎片化结构(与G3违背)</p><h2 id="ShuffleNet-V2-网络结构"><a href="#ShuffleNet-V2-网络结构" class="headerlink" title="ShuffleNet V2 网络结构"></a>ShuffleNet V2 网络结构</h2><p>为了使ShuffleNet更加高效，关键在于保持等宽的出入输出通道，以及使用密集卷积操作而不是过多的分组卷积。</p><p><img src="https://i.loli.net/2019/12/11/3YuOe1yB8zalwjL.png" alt="S9YD9ZB_OHNUS`GHFVS@9D0.png"></p><p>如图，左边两个是 ShuffleNet v1 的模块，右边两个是 ShuffleNet v2 的模块。</p><p>图c是 ShuffleNet v2 的基本模块，其首先将输入的通道随机split成两部分（这是一种变相的分组卷积，不过只分了两个组，遵守了G2和G3），一部分恒等映射到模块尾部，另一部分通过三个输入输出通道数相同的卷积前向传播（遵守了G1），之后使用concat操作（而不是add操作，遵守了G4）将两个分支结合在一起，最后进行通道洗牌(channel shuffle)。</p><p>图d为下采样模块，原理类似，stride=2缩小特征图，没有使用channel split操作，最后两个分支concat到一起使通道数翻倍。</p><p>恒等映射后concat到模块尾部，能使特征得到复用，提高准确度。这种思想来源于DenseNet。</p><h1 id="7-MnasNet（待更新）"><a href="#7-MnasNet（待更新）" class="headerlink" title="7    MnasNet（待更新）"></a>7    MnasNet（待更新）</h1><p>太复杂了，回头再看</p><h1 id="8-MobileNet-v3（待更新）"><a href="#8-MobileNet-v3（待更新）" class="headerlink" title="8    MobileNet v3（待更新）"></a>8    MobileNet v3（待更新）</h1><p>基于MnasNet</p><h1 id="9-MixNet（待更新）"><a href="#9-MixNet（待更新）" class="headerlink" title="9    MixNet（待更新）"></a>9    MixNet（待更新）</h1><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><ol><li><a href="https://baijiahao.baidu.com/s?id=1589005428414488177&wfr=spider" target="_blank" rel="noopener">纵览轻量化卷积神经网络：SqueezeNet、MobileNet、ShuffleNet、Xception</a></li><li><a href="https://www.jianshu.com/p/fdd7d7353c55" target="_blank" rel="noopener">SqueezeNet | 轻量级深层神经网络</a></li><li><a href="https://www.greedyai.com/" target="_blank" rel="noopener">贪心学院</a></li><li><a href="https://blog.csdn.net/lk3030/article/details/84847879" target="_blank" rel="noopener">Xception</a></li><li><a href="https://blog.csdn.net/kangdi7547/article/details/81431572" target="_blank" rel="noopener">轻量级模型：MobileNet V2</a></li><li><a href="https://www.jianshu.com/p/71e32918ea0a?utm_source=oschina-app" target="_blank" rel="noopener">精简CNN模型系列之六：ShuffleNet v2</a></li><li><a href="https://blog.csdn.net/u014380165/article/details/81322175" target="_blank" rel="noopener">ShuffleNet v2算法笔记</a></li><li><a href="https://blog.csdn.net/h__ang/article/details/88618089" target="_blank" rel="noopener">ShuffleNet_v2模型解读</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在先前的神经网络发展中，神经网络通常是通过加深、加宽网络结构或重复使用特征图来提高网络的性能。但在网络性能提升的同时，也导致了其对内存、CPU/GPU需求大，推演速度慢，耗电量高等问题，难以运用在实时场景和移动设备中。如自动驾驶车载神经网络运行速度慢就有可能导致事故发生，耗
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="轻量级网络" scheme="http://a-kali.github.io/tags/%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%BD%91%E7%BB%9C/"/>
    
      <category term="SqueezeNet" scheme="http://a-kali.github.io/tags/SqueezeNet/"/>
    
      <category term="MobileNet" scheme="http://a-kali.github.io/tags/MobileNet/"/>
    
      <category term="Xception" scheme="http://a-kali.github.io/tags/Xception/"/>
    
      <category term="ShuffleNet" scheme="http://a-kali.github.io/tags/ShuffleNet/"/>
    
      <category term="EfficientNet" scheme="http://a-kali.github.io/tags/EfficientNet/"/>
    
      <category term="深度可分离卷积" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%8F%AF%E5%88%86%E7%A6%BB%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>SSD: Single-shot detectors</title>
    <link href="http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/"/>
    <id>http://a-kali.github.io/2019/12/04/SSD-Single-shot-detectors/</id>
    <published>2019-12-04T08:49:35.000Z</published>
    <updated>2019-12-17T09:01:49.867Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/abs/1512.02325" target="_blank" rel="noopener">SSD: Single Shot MultiBox Detector</a></p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>本文提出了一个基于深度神经网络的<strong>单步(single shot)目标检测器SSD</strong>，其在继承了YOLO单步预测高检测速度的同时，拥有不弱于Faster R-CNN的准确度。</p><h1 id="SSD-网络结构"><a href="#SSD-网络结构" class="headerlink" title="SSD 网络结构"></a>SSD 网络结构</h1><p><img src="https://i.loli.net/2019/12/04/qIhrwjU6uRMV4Nn.png" alt="png"></p><p>从图中可以看出：</p><ul><li>不同于YOLOv1和Faster R-CNN，SSD是一个全卷积网络。</li><li>SSD的预测结果并不完全由最后一层输出，而是由其5个<strong>额外特征层(Extra Feature Layers)</strong>和 VGG16中的一层的输出综合而来。</li><li>由于SSD是个全卷积网络，所以其分类操作也由卷积层进行。上图中横向的直线即是<strong>卷积分类器</strong>，卷积核大小为3×3，channel数量为anchors×(Classes+4)。此处anchors指anchor的数量；classes为类别数，预测值为每个类置信度，这点应该会给后面的NMS作为评判标准；+4就是(x,y,w,h)。</li><li>SSD的输出特征图平均每个像素都有一组anchor，整个网络共生成8732个anchor，远多于YOLO和Faster R-CNN。（这里有个问题，根据上面一条，使用3×3卷积核作为滑动窗口是没法做到每个像素都有anchor的，所以此处应该有padding）</li></ul><p><img src="https://i.loli.net/2019/12/04/To4hNmlKez1CrqV.png" alt="2Bng"></p><h1 id="Hard-negative-mining"><a href="#Hard-negative-mining" class="headerlink" title="Hard negative mining"></a>Hard negative mining</h1><p>检测结果中，未被选为最终结果的样本都是负样本。这导致负样本数量远大于正样本，样本不均衡。作者采用<strong>Hard negative mining</strong>的方式，仅选用被误认为是正样本可能性更大的负样本。</p><h1 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h1><p>论文中还提到了损失函数和anchor的选择，但跟其它的目标检测网络差不多，就不再赘述。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1512.02325&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SSD: Single Shot MultiBox Detector&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;概述&quot;&gt;&lt;a 
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="SSD" scheme="http://a-kali.github.io/tags/SSD/"/>
    
  </entry>
  
  <entry>
    <title>盘点那些在github上找到的宝藏</title>
    <link href="http://a-kali.github.io/2019/12/04/%E7%9B%98%E7%82%B9%E9%82%A3%E4%BA%9B%E5%9C%A8github%E4%B8%8A%E6%89%BE%E5%88%B0%E7%9A%84%E5%AE%9D%E8%97%8F/"/>
    <id>http://a-kali.github.io/2019/12/04/盘点那些在github上找到的宝藏/</id>
    <published>2019-12-03T16:14:40.000Z</published>
    <updated>2020-01-06T10:33:28.524Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p><strong>计算机视觉&amp;深度学习超级Road Map！</strong>：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没有SLAM、自动驾驶、行人检测、对抗样本）的学习路线和各个模型的概述。CVer必备！项目地址：<a href="https://github.com/weslynn/AlphaTree-graphic-deep-neural-network" target="_blank" rel="noopener">https://github.com/weslynn/AlphaTree-graphic-deep-neural-network</a></p></li><li><p><strong>使用Python实现各种算法</strong>：想学算法又不想学C++？重度Python患者的福音，项目地址：<a href="https://github.com/TheAlgorithms/Python" target="_blank" rel="noopener">https://github.com/TheAlgorithms/Python</a></p></li><li><p><strong>深度学习500问</strong>：以问答形式对常用的概率知识、线性代数、机器学习、深度学习、计算机视觉等热点问题进行阐述，面试党必备。项目地址：<a href="https://github.com/scutan90/DeepLearning-500-questions" target="_blank" rel="noopener">https://github.com/scutan90/DeepLearning-500-questions</a></p></li><li><p><strong>3DCNN-PyTorch</strong>：PyTorch 3D卷积预训练模型。项目地址：<a href="https://github.com/kenshohara/3D-ResNets-PyTorch" target="_blank" rel="noopener">https://github.com/kenshohara/3D-ResNets-PyTorch</a> 。附腾讯优图的3D医疗影像预训练模型MedicalNet（未来会出2D），亲测效果不错，项目地址：<a href="https://github.com/Tencent/MedicalNet" target="_blank" rel="noopener">https://github.com/Tencent/MedicalNet</a></p></li><li><p><strong>南瓜书</strong>：西瓜书公式推导解析，节约2w根头发。项目地址：<a href="https://github.com/datawhalechina/pumpkin-book" target="_blank" rel="noopener">https://github.com/datawhalechina/pumpkin-book</a></p></li><li><p><strong>深度学习面试宝典</strong>：涵盖各大公司ML、CV、NLP、数学、算法、强化学习、SLAM等方向的面试题&amp;解答集合！外加面试技巧和经验！面试党吐血墙裂推荐！项目地址：<a href="https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs" target="_blank" rel="noopener">https://github.com/amusi/Deep-Learning-Interview-Book/tree/master/docs</a></p></li><li><p><strong>PyTorch预训练模型</strong>：</p><ul><li>主要backbone汇总：<a href="https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet" target="_blank" rel="noopener">https://github.com/Cadene/pretrained-models.pytorch，美中不足的是没有EfficientNet</a> 。</li><li>EfficientNet：<a href="https://github.com/lukemelas/EfficientNet-PyTorch" target="_blank" rel="noopener">https://github.com/lukemelas/EfficientNet-PyTorch</a> 。这个预训练模型有个缺点，只能单GPU运行，但这不妨碍EfficientNet牛逼。</li><li>语义分割模型：<a href="https://github.com/qubvel/segmentation_models.pytorch" target="_blank" rel="noopener">https://github.com/qubvel/segmentation_models.pytorch</a> 。主流语义分割模型，可惜没有DeepLab系列。</li><li>DeepLab：<a href="https://github.com/jfzhang95/pytorch-deeplab-xception" target="_blank" rel="noopener">https://github.com/jfzhang95/pytorch-deeplab-xception</a></li></ul></li><li><p><strong>数据增广大全</strong>：涵盖了CV、自然语言、音频方向的各种数据增广图例、调用库和论文。项目地址：<a href="https://github.com/AgaMiko/data-augmentation-review" target="_blank" rel="noopener">https://github.com/AgaMiko/data-augmentation-review</a></p></li><li><p><strong>Chrome实用插件大全</strong>：Chrome插件英雄榜, 为优秀的Chrome插件写一本中文说明书, 让Chrome插件英雄们造福人类。项目地址：<a href="https://github.com/zhaoolee/ChromeAppHeroes" target="_blank" rel="noopener">https://github.com/zhaoolee/ChromeAppHeroes</a></p></li><li><p><strong>绘制炫酷的神经网络图</strong>：</p><ul><li><p>使用图形界面绘制：<a href="https://github.com/zfrenchee" target="_blank" rel="noopener">https://github.com/zfrenchee</a></p><p>画图工具体验地址：<a href="http://alexlenail.me/NN-SVG/" target="_blank" rel="noopener">http://alexlenail.me/NN-SVG/</a></p></li><li><p>使用LaTeX绘制：<a href="https://github.com/HarisIqbal88/PlotNeuralNet" target="_blank" rel="noopener">https://github.com/HarisIqbal88/PlotNeuralNet</a></p></li><li><p>使用参数绘制：<a href="https://cbovar.github.io/ConvNetDraw/" target="_blank" rel="noopener">https://cbovar.github.io/ConvNetDraw/</a></p><p><img src="https://i.loli.net/2019/12/15/lY4JZtPcwC6fvUu.png" alt="6KJ9MJ75ZKFMJCB9_Z8_DPQ.png"></p></li><li><p>使用Python绘制：<a href="https://github.com/gwding/draw_convnet" target="_blank" rel="noopener">https://github.com/gwding/draw_convnet</a></p></li><li><p>方便&amp;好看&amp;著名的NetScope：<a href="https://github.com/ethereon/netscope" target="_blank" rel="noopener">https://github.com/ethereon/netscope</a></p></li></ul></li></ul><p>找到其它宝藏的小伙伴可以通过邮箱发给我鸭！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;计算机视觉&amp;amp;深度学习超级Road Map！&lt;/strong&gt;：盘点了计算机视觉相关的深度学习技术，涵盖了目标分类、目标检测、目标分割、GAN、轻量级模型、人脸检测、人脸识别、人脸对齐、3DCNN、风格迁移、OCR、姿态检测等方向（没
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="PyTorch" scheme="http://a-kali.github.io/tags/PyTorch/"/>
    
      <category term="github" scheme="http://a-kali.github.io/tags/github/"/>
    
  </entry>
  
  <entry>
    <title>常见的聚类算法(K-Means\GMM\DBSCAN)</title>
    <link href="http://a-kali.github.io/2019/12/02/k-means-%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>http://a-kali.github.io/2019/12/02/k-means-聚类算法/</id>
    <published>2019-12-02T15:59:48.000Z</published>
    <updated>2019-12-17T09:00:44.515Z</updated>
    
    <content type="html"><![CDATA[<h1 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h1><p>在<strong>无监督学习(unsupervised learning)</strong>中，训练样本的标记信息是未知的，需要·通过一些算法来揭示这些样本数据中的内在性质和规律。无监督学习通常解决的是<strong>聚类(clustering)</strong>问题。</p><p>聚类算法通常将数据集中的样本划分为若干个通常是不相交的子集，每个子集称为一个”簇(cluster)”。而每个簇都包含一定的内在关系或者性质。</p><p>如下图是一个为做标记的样本集，通过它们的分布，我们很容易对上图中的样本做出以下几种划分：</p><p>当需要将其划分为两个簇时，即 k=2 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKF1eO.png" alt="QKF1eO.png"></p><p>当需要将其划分为四个簇时，即 k=4 时：</p><p><img src="https://s2.ax1x.com/2019/12/03/QKFalt.png" alt="QKFalt.png"></p><p>而对这些样本进行划分的就是聚类算法。下面我们将介绍几种常见的聚类算法中的算法。</p><h1 id="K-means-算法"><a href="#K-means-算法" class="headerlink" title="K-means 算法"></a>K-means 算法</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>k-means算法又名 k 均值算法。其算法思想大致为：<strong>先从样本集中随机选取 k 个样本作为簇中心，并计算所有样本与这 k 个簇中心的距离，对于每一个样本，将其划分到与其距离最近的簇中心所在的簇中，对于新的簇计算各个簇的新的簇中心，根据新的簇中心来重新划分簇。重复上述过程，直到所有样本到其簇中心距离之和达到最小。</strong>在普通K-means算法中，k 值通常凭经验和需求、通过多次尝试来选择；度量距离的方法通常采用<strong>欧氏距离</strong>，其它距离测量方法有曼哈顿距离、切比雪夫距离等，在这里不一一赘述。</p><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>（待更新）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;聚类&quot;&gt;&lt;a href=&quot;#聚类&quot; class=&quot;headerlink&quot; title=&quot;聚类&quot;&gt;&lt;/a&gt;聚类&lt;/h1&gt;&lt;p&gt;在&lt;strong&gt;无监督学习(unsupervised learning)&lt;/strong&gt;中，训练样本的标记信息是未知的，需要·通过一些算
      
    
    </summary>
    
      <category term="机器学习" scheme="http://a-kali.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="k-means" scheme="http://a-kali.github.io/tags/k-means/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="无监督学习" scheme="http://a-kali.github.io/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="聚类" scheme="http://a-kali.github.io/tags/%E8%81%9A%E7%B1%BB/"/>
    
  </entry>
  
  <entry>
    <title>部分计算机视觉算法面试题解答</title>
    <link href="http://a-kali.github.io/2019/12/02/%E9%83%A8%E5%88%86%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98%E8%A7%A3%E7%AD%94/"/>
    <id>http://a-kali.github.io/2019/12/02/部分计算机视觉算法面试题解答/</id>
    <published>2019-12-01T16:59:03.000Z</published>
    <updated>2019-12-01T17:15:59.062Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Q：Dropout是失活神经元还是失活连接</strong></p><p>A：失活神经元并清除失活神经元周围的连接</p><p><a href="https://imgse.com/i/Qm4Aht" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qm4Aht.md.png" alt="Qm4Aht.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Q：Dropout是失活神经元还是失活连接&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A：失活神经元并清除失活神经元周围的连接&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://imgse.com/i/Qm4Aht&quot; target=&quot;_blank&quot; rel=&quot;noope
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>常见激活函数汇总</title>
    <link href="http://a-kali.github.io/2019/12/01/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%B1%87%E6%80%BB/"/>
    <id>http://a-kali.github.io/2019/12/01/激活函数汇总/</id>
    <published>2019-12-01T15:05:09.000Z</published>
    <updated>2019-12-01T16:38:44.814Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h1><p>Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。</p><p>函数式：<br>$$<br>σ(x) = \frac{1}{1+e^{-x}}<br>$$<br>导数式：<br>$$<br>σ’(x) = σ(x)×(1-σ(x))<br>$$<br>函数图像：</p><p><a href="https://imgse.com/i/QmcKiT" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcKiT.md.png" alt="QmcKiT.md.png"></a></p><h1 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h1><p>ReLU是神经网络激活层最常用的一种函数，因为其运算简单、易于求导，能用最简单的方式实现非线性运算的性质。</p><p>函数式：<br>$$<br>f(x)=max(0,x)<br>$$<br>函数图像：</p><p><a href="https://imgse.com/i/QmcLkV" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/01/QmcLkV.md.png" alt="QmcLkV.md.png"></a></p><h1 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h1><p>常见于递归神经网络。</p><p>函数式：<br>$$<br>f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}<br>$$<br>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/QmgaBn.png" alt="QmgaBn.png"></p><h1 id="Leaky-ReLU"><a href="#Leaky-ReLU" class="headerlink" title="Leaky ReLU"></a>Leaky ReLU</h1><p>函数式：<br>$$<br>f(x)=\left{<br>\begin{aligned}<br>x, \quad x\geq0\<br>ax, \quad x&lt;0<br>\end{aligned}<br>\right.<br>$$<br>函数图像：</p><p><img src="https://s2.ax1x.com/2019/12/01/Qm21bR.png" alt="Qm21bR.png"></p><h1 id="PReLU"><a href="#PReLU" class="headerlink" title="PReLU"></a>PReLU</h1><p>PReLU(Parametric Rectified Linear Unit) 参数化修正线性单元。其参数随着网络训练而改变。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmRH0A.png" alt="QmRH0A.png"></p><p>参数更新：</p><p><img src="https://s2.ax1x.com/2019/12/02/QmROtP.png" alt="QmROtP.png"></p><h1 id="RReLU"><a href="#RReLU" class="headerlink" title="RReLU"></a>RReLU</h1><p>Random Leaky ReLU，其参数是随机生成的在[0, 1)之间的值。</p><p><img src="https://s2.ax1x.com/2019/12/02/QmWjER.png" alt="QmWjER.png"></p><h1 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h1><p>指数线性单元。右侧线性部分使得ELU能够缓解梯度消失，而左侧软饱能够让ELU对输入变化或噪声更鲁棒。ELU的输出均值接近于零，所以收敛速度更快。α为常数。</p><p><a href="https://imgse.com/i/Qmf1bj" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/12/02/Qmf1bj.md.png" alt="Qmf1bj.md.png"></a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Sigmoid&quot;&gt;&lt;a href=&quot;#Sigmoid&quot; class=&quot;headerlink&quot; title=&quot;Sigmoid&quot;&gt;&lt;/a&gt;Sigmoid&lt;/h1&gt;&lt;p&gt;Sigmoid 常用于二分类和多标签分类的最后一层，能将实数值映射到0-1之间。&lt;/p&gt;
&lt;p&gt;函数
      
    
    </summary>
    
      <category term="神经网络" scheme="http://a-kali.github.io/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="激活函数" scheme="http://a-kali.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>YOLOv1 - YOLOv3</title>
    <link href="http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/"/>
    <id>http://a-kali.github.io/2019/11/27/YOLOv1-YOLOv3/</id>
    <published>2019-11-27T09:00:31.000Z</published>
    <updated>2020-01-06T10:57:54.994Z</updated>
    
    <content type="html"><![CDATA[<h1 id="YOLOv1"><a href="#YOLOv1" class="headerlink" title="YOLOv1"></a>YOLOv1</h1><p>论文地址：<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>众所周知，在深度学习目标检测领域有着两个流派，分别是<strong>基于候选区域的R-CNN流派</strong>和<strong>直接回归输出边框的YOLO流派</strong>。R-CNN系列的准确率较高，但即便发展到Faster R-CNN，运算速度也才只有7fps。为了使检测工作更接近实时，作者提出了YOLO结构。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCYCPs.png" alt="QCYCPs.png"></p><h2 id="YOLO-的实现"><a href="#YOLO-的实现" class="headerlink" title="YOLO 的实现"></a>YOLO 的实现</h2><p>（第一次看可能有点复杂，建议拿笔出来边梳理边看）</p><p>YOLO 将输入图像划分为 <strong>S × S 个网格</strong>。如果一个物体的中心点在这个网格中，则该网格负责检测这个物体。每个网格预测 <strong>B 个边框(bounding box)</strong>及其<strong>置信度(confidence)</strong>。其中置信度为<strong>该网格包含目标物体的概率</strong>乘以预测边界框与真实边界框(ground truth)的<strong>交并比(IOU)</strong>，即：<br>$$<br>Confidence=Pr(Object)×IOU^{truth}_{pred}<br>$$<br>也就是说，<strong>当置信度为0时，边框内不含有任何目标物，除此之外置信度都等于交并比</strong>。该置信度只是个预测值，受真实的置信度监督。这点可以从后面的损失函数看出来。</p><p>于是我们得知，每个边框由5个预测值组成，分别为$x,y,w,h,confidence$。</p><p>同时每个网格预测一组 <strong>C 个类别的概率</strong> $Pr(Class_i|Object)$，即输出一组长度为 C 的概率向量。这个概率表示网格含有物体的情况下，各个类别属于该网格的概率。</p><p>经过上述步骤，最终在神经网络末端输出一个$S×S×(B×5+C)$的张量。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCIFr6.png" alt="QCIFr6.png"></p><p>测试时，将置信度和每一类概率相乘</p><p>$$<br>Score=Pr(Class_i|Object)×Pr(Object)×IOU^{truth}<em>{pred}=Pr(Class_i)×IOU^{truth}</em>{pred}<br>$$</p><p>得到的Score表示<strong>每一类在每个边框中的置信度(class-specific confidence for each box)</strong>。通过设置阈值筛选出得分高的box，再以的分最高的box为基准进行NMS选出最优结果。</p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p>如图，在 PASCAL VOC 数据集中，图像输入为 448×448，取 S=7，B=2，一共有20 个类别（C=20），则输出就是 7x7x30 的一个 tensor。</p><p><img src="https://s2.ax1x.com/2019/11/27/QCHg76.png" alt="QCHg76.png"></p><p>可以看出这是一个<strong>彻头彻尾的端到端网络</strong>。看到这里可能会有点震惊，上面讲了那么多复杂的设定到头来居然只是个这么朴素的端到端网络？事实上，上面那么多设定大多都是来源于其巧妙设计的损失函数。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="https://s2.ax1x.com/2019/11/28/QCq2WD.png" alt="QCq2WD.png"></p><p>由于坐标、长宽、置信度的重要性不同，作者给予了他们不同的损失函数和权重。</p><ul><li>重视坐标预测，给这些损失前面赋予更大的权重，取 5。</li><li>对没有 object 的 box 的 confidence loss，赋予较小的损失权重，取 0.5。</li><li>有 object 的 box 的 confidence loss 和类别的 loss 的损失权重取 1。</li><li>对不同大小的边框预测中，相比于大边框，小边框预测偏一点造成的影响更大。而均方误差中对同样的偏移 loss 是一样。为了缓和这个问题，作者用了一个比较取巧的办法，就是将 box 的 width 和 height 取平方根代替原本的 height 和 width。</li></ul><h2 id="YOLO-的缺点"><a href="#YOLO-的缺点" class="headerlink" title="YOLO 的缺点"></a>YOLO 的缺点</h2><ul><li>YOLO对比较密集的、小型的物体（如鸟群）检测效果不佳。因为会有两个同类物体出现在同一个网格中的情况。</li><li>同一类物体出现的新的不常见的长宽比和其他情况时，泛化能力偏弱。</li><li>由于损失函数的问题，定位误差是影响检测效果的主要原因。尤其是大小物体的处理上，还有待加强。</li><li>召回率远低于RCNN系列。</li></ul><h1 id="YOLOv2-amp-YOLO9000"><a href="#YOLOv2-amp-YOLO9000" class="headerlink" title="YOLOv2 &amp; YOLO9000"></a>YOLOv2 &amp; YOLO9000</h1><p>论文地址：<a href="http://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO9000: Better, Faster, Stronger</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>YOLOv2：在多方面进行改进，在mAP上超过了使用resnet作为backbone的Faster R-CNN 和 SSD，而且速度更快。</li><li>YOLO9000：使用大量分类数据集和检测数据集进行联合训练，能够对9000+类别进行检测。</li></ul><h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><p>YOLOv2 和 YOLOv1 对比：</p><p><img src="https://s2.ax1x.com/2019/11/29/QA2ryq.png" alt="QA2ryq.png"></p><ul><li><strong>增加了 batch norm</strong>。</li><li><strong>使用高分辨率微调模型</strong>：YOLOv1 采用224×224大小的图片进行预训练，但训练检测模型时使用的是448×448，这一变动对模型性能会产生一定影响。而YOLOv2在常规预训练和进行正式训练之间使用了448×448的分类图像样本进行了微调，缓解了分辨率突然切换造成的影响。</li><li><strong>采用了 Anchor Boxes</strong>：借鉴Faster R-CNN的做法使用了锚框，大幅提高了召回率但mAP轻微下降。</li><li>将图片输入尺寸改为416×416，grid改为13×13，<strong>使grid长宽为奇数</strong>，这样能更有效地预测图片中央的目标物（根据经验，目标物在图片中央的可能性较大）。</li><li><strong>使用 k-means 聚类算法来选择锚框</strong>：手工选择的锚框可能对性能产生影响性能。作者使用k-means对训练集目标框进行聚类，以IOU为距离计算指标，即 $d = 1 - IOU$。在对性能和准确率进行衡量之后，选择了 $k = 5$，得出聚类结果的5个聚类中心作为锚框的最终选择（只取锚框的大小和形状，不取锚框的位置）。</li><li><strong>Direct location prediction</strong>：在RPN中的锚框非常不稳定，其公式如下：</li></ul><p>$$<br>x=(t_x<em>w_a)−x_a\y=(t_y</em>h_a)−y_a<br>$$</p><p>$t_x$和$t_y$为预测值，当$t_x=1$时，预测框相比于原本的锚框将右移一整个锚框的宽度！YOLOv2对这种方法进行了改进：<br>$$<br>b_x=σ(t_x)+c_x\<br>b_y=σ(t_y)+c_y\<br>b_w=p_we^{t_w}\<br>b_h=p_he^{t_h}\<br>Pr(object)<em>IOU(b,object)=σ(t_o)<br>$$<br>$t_x,t_y,t_w,t_h,t_o$为预测值，被Sigmoid函数限制在(0,1)之间。之后再通过下图的一些运算得到最终box。在*</em>限制了预测值大小**的情况下，模型参数会更容易学习。</p><p><img src="https://i.loli.net/2019/12/03/Pb7ZzNEjQhDtS4c.png" alt="RH.png"></p><ul><li><strong>Fine-Grained Features</strong>：为了能在小型目标上获得更好的效果，作者把浅层高分辨率的特征图叠加到深层低分辨率的图上了，但没详细说明，估计跟U-Net差不多吧。</li><li><strong>多尺度训练</strong>：因为YOLOv2是全卷积，所以能用任意大小的图像作为输入。作者使用了{320,352，…，608}大小的图像进行训练以提高模型的泛化性能。</li></ul><h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><p>大部分网络使用VGG16作为backbone，但是VGG有点臃肿。作者自定义了<strong>Darknet-19</strong>作为网络的backbone。</p><p><img src="https://i.loli.net/2019/12/03/MAWNrVztIbf3CjJ.png" alt="BPV.png"></p><h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><p>这部分讲了下YOLO9000和WordTree，但没有看懂而且好像不是很重要的亚子，跳了。</p><h1 id="YOLOv3"><a href="#YOLOv3" class="headerlink" title="YOLOv3"></a>YOLOv3</h1><p>待更新</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/woduoxiangfeiya/article/details/80866155" target="_blank" rel="noopener">YOLOv1论文翻译</a></p><p>[2]<a href="https://blog.csdn.net/guleileo/article/details/80581858" target="_blank" rel="noopener">从YOLOv1到YOLOv3，目标检测的进化之路</a></p><p>[3]<a href="https://www.jianshu.com/p/517a1b344a88" target="_blank" rel="noopener">YOLOv2 / YOLO9000 深入理解</a></p><p>[4]<a href="https://www.jianshu.com/p/b02f64e0d44b" target="_blank" rel="noopener">Yolo系列其二：Yolo_v2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;YOLOv1&quot;&gt;&lt;a href=&quot;#YOLOv1&quot; class=&quot;headerlink&quot; title=&quot;YOLOv1&quot;&gt;&lt;/a&gt;YOLOv1&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1506.02640&quot; target
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="YOLO" scheme="http://a-kali.github.io/tags/YOLO/"/>
    
  </entry>
  
  <entry>
    <title>深度学习自动驾驶概述</title>
    <link href="http://a-kali.github.io/2019/11/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6%E6%A6%82%E8%BF%B0/"/>
    <id>http://a-kali.github.io/2019/11/05/深度学习自动驾驶概述/</id>
    <published>2019-11-05T09:45:08.000Z</published>
    <updated>2019-11-05T09:46:18.720Z</updated>
    
    <content type="html"><![CDATA[<p><strong>目标：</strong>使用端到端的深度学习方法，根据车载摄像头的画面来判断如何<strong>打方向盘和踩油门</strong>。</p><p><img src="https://s2.ax1x.com/2019/11/05/MprAb9.png" alt="MprAb9.png"></p><p>参考论文：End to End Learning for Self-Driving Cars</p><p><strong>收集数据：</strong></p><p><img src="https://s2.ax1x.com/2019/11/05/MpBqT1.png" alt="MpBqT1.png"></p><p>汽车人为行驶时，其左中右三个摄像头、方向盘转向、油门、转向灯等数据都会通过其 CAN bus 传入处理器。而如今的汽车中基本都带有上述传感器帮忙训练神经网络；当汽车自动驾驶时，汽车根据中间摄像头传入的数据来操控方向盘等设备。</p><p><img src="https://s2.ax1x.com/2019/11/05/Mpye1K.png" alt="Mpye1K.png"></p><p><strong>自动驾驶模拟器：</strong></p><p><a href="https://imgchr.com/i/Mp6LR0" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/05/Mp6LR0.md.png" alt="Mp6LR0.md.png"></a></p><p>看起来很好玩的样子，有空看看源码了解下神经网络输出如何操控这些游戏。</p><p><strong>图像处理：</strong></p><ul><li>亮度调整（适应白天、晚上、阴天、晴天等情景）</li><li>归一化</li><li>图像切割（去除地平线以上和车头部分的无关紧要的数据）</li><li>水平翻转（左转右转）</li><li>数据平衡（欠采样、过采样、给样本少的数据更大权重、合成新数据）</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;目标：&lt;/strong&gt;使用端到端的深度学习方法，根据车载摄像头的画面来判断如何&lt;strong&gt;打方向盘和踩油门&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/11/05/MprAb9.png&quot; al
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自动驾驶" scheme="http://a-kali.github.io/tags/%E8%87%AA%E5%8A%A8%E9%A9%BE%E9%A9%B6/"/>
    
      <category term="概述" scheme="http://a-kali.github.io/tags/%E6%A6%82%E8%BF%B0/"/>
    
  </entry>
  
  <entry>
    <title>Kaggle days China, Oct 2019</title>
    <link href="http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/"/>
    <id>http://a-kali.github.io/2019/10/30/Kaggle-days-China-Oct-2019/</id>
    <published>2019-10-30T12:17:30.000Z</published>
    <updated>2019-11-09T15:52:47.745Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Winning-competitions-with-deep-learning-skills-–-SeuTao"><a href="#Winning-competitions-with-deep-learning-skills-–-SeuTao" class="headerlink" title="Winning competitions with deep learning skills – SeuTao"></a>Winning competitions with deep learning skills – SeuTao</h1><p><img src="https://s2.ax1x.com/2019/11/09/Mnu7rQ.png" alt="Mnu7rQ.png"></p><h2 id="Prepare-for-a-DL-competition"><a href="#Prepare-for-a-DL-competition" class="headerlink" title="Prepare for a DL competition"></a>Prepare for a DL competition</h2><ul><li>GPUs 是基础&amp;必要条件，但不是获得金牌的决定性条件。有着9块金牌的涛神在2019年也才只有2块1080ti而已。</li><li>多读 paper 是获得 idea 的关键，在很多 paper 中能找到相似问题的解决方案。</li><li>多读别人的代码。</li></ul><h2 id="Five-steps-to-Win-a-DL-competition"><a href="#Five-steps-to-Win-a-DL-competition" class="headerlink" title="Five steps to Win a DL competition"></a>Five steps to Win a DL competition</h2><ul><li>Understand the data</li><li>Build a strong baseline</li><li>Find the tricks</li><li>Ensemble</li><li>Pseudo-labels</li></ul><h3 id="Build-a-strong-baseline"><a href="#Build-a-strong-baseline" class="headerlink" title="Build a strong baseline"></a>Build a strong baseline</h3><ul><li>据涛神的看法，建立一个 <strong>strong baseline</strong> 是整个比赛中最重要的一环。一个高质量的 baseline 可以直接让你拿到<strong>银牌</strong>甚至 top15。可以建立一个高质量的 pipeline 并重复利用。</li><li>不要使用花里胡哨的神经网络架构和损失函数。这里大概可以理解为，baseline应使用简单轻量的神经网络，便于快速训练、调参、尝试 tricks。</li><li><strong>优化器</strong>：动量梯度下降或者 lr(3e-4) Adam优化器。优化器的改变对网络性能提升不大。</li><li><strong>学习率</strong>：可以尝试 warm up 和 余弦退火/cyclic lr</li><li>找到对数据合适的<strong>数据增强</strong>。</li><li>可靠的<strong>本地验证</strong>。在kaggle上提交验证相对麻烦而且有次数限制，而有一个可靠的本地验证就能快速地尝试验证各种 tricks。</li><li><strong>BatchNorm</strong>问题，基线很难高分的一个原因，涉及到神经网络细节。这里没看懂先挂张图：<img src="https://s2.ax1x.com/2019/11/09/Mn3W5j.png" alt="Mn3W5j.png"></li></ul><h3 id="Find-the-tricks"><a href="#Find-the-tricks" class="headerlink" title="Find the tricks"></a>Find the tricks</h3><ul><li>任务型 trick：图片分类trick、目标检测trick等。这些trick需要大量相关论文的积累。</li><li>数据型 trick：这需要你对数据敏锐的分析。数据相关的trick往往是制胜的关键。</li></ul><h3 id="Ensemble"><a href="#Ensemble" class="headerlink" title="Ensemble"></a>Ensemble</h3><p>融合技巧很重要，比如stacking、blending等</p><p><img src="https://s2.ax1x.com/2019/11/09/Mn82y6.png" alt="Mn82y6.png"></p><h3 id="Pseudo-labels"><a href="#Pseudo-labels" class="headerlink" title="Pseudo labels"></a>Pseudo labels</h3><ul><li>易于使用而且几乎在所有的深度学习竞赛中都奏效。</li><li>可以通过测试集或者外部数据来生成伪标签。</li><li>在比赛的最后stage使用——Overfit the LB then create pseudo labels（这个有点难理解）</li><li>注意不要 overfit 伪标签</li></ul><h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><ul><li>实验效率很重要，总结每一次实验经验，不管是成功还是失败。</li><li>在 kernel only 这种限制测试时间的比赛上，可以使用模型蒸馏、加速。</li><li>找到任务实质相关的论文。</li><li>熟读计算机视觉各个分支的论文，很可能会在之前读过的相关的论文上找到thick。</li></ul><h1 id="Tricks-of-image-classification-–-Jun-Lan"><a href="#Tricks-of-image-classification-–-Jun-Lan" class="headerlink" title="Tricks of image classification – Jun Lan"></a>Tricks of image classification – Jun Lan</h1><ul><li><p>图像分类大致可以分为两种：多类别分类（一个样本属于一个类别） vs 多标签分类（一个样本属于多个类别）</p></li><li><p>找到之前相似的比赛，观察高分solution</p></li><li><p>将数据增强后的图片可视化查看效果，根据任务选择增强方法</p></li><li><p>医疗影像预训练数据：MedicalNet。目前没开源2d数据</p></li><li><p>cycle learning rate：减少调参，更快收敛</p></li><li><p>多类别：交叉熵损失；多标签：二值交叉熵损失</p></li><li><p>mixup：一种数据增强的方法。将两张图片及其标签按一定比例进行融合</p></li><li><p>apex：基于pytorch的低精度运算（32位或16位）。减少显存占用，增加训练速度。pure float可能会导致精度损失和溢出。解决方案：混合精度训练。（设成O1就行了）</p></li><li><p>梯度累加（batch accumulation）：增大batch的方法，（多累积几步再更新梯度？）</p></li><li><p>伪标签：数据少或有大量额外数据且没有标签的情况下</p><ol><li>训练集训练模型</li><li>测试数据</li><li>将置信度较高的数据放入训练集（0.95、0，98）</li><li>再训练</li></ol></li><li><p>数据蒸馏（knowledge distillation）：使用小模型（student）来获取大模型（teacher）中的核心知识</p><ol><li>将数据集分为k折</li><li>k折交叉验证训练teacher model</li><li>预测out-of-fold的标签</li><li>在out-of-fold训练student model</li></ol></li></ul><hr><h1 id="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"><a href="#半年5战5金：Kaggle史上最快GrandMaster是如何炼成的" class="headerlink" title="半年5战5金：Kaggle史上最快GrandMaster是如何炼成的"></a>半年5战5金：Kaggle史上最快GrandMaster是如何炼成的</h1><p>下面内容跟 kaggle days 没什么关系，是一些很有用的 tricks。整理自网络，有删改，原文地址：<a href="https://zhuanlan.zhihu.com/p/89476481" target="_blank" rel="noopener">Kaggle你问我答【1】——SeuTao</a></p><p>这是 Kaggle 你问我答 (AMA) 的第一期活动，本期请到的嘉宾是 SueTao，他研究生毕业于东南大学，目前是腾讯的一名算法工程师。SueTao 擅长计算机视觉（Computer Vision），半年 5 战 5 金，也许是史上最快的 GrandMaster。截至目前共斩获 9 金 3 银，kaggle 最高排名全球第 10。</p><p>以下是本期活动的问答集锦：</p><p><strong>Q1：如何搭建kaggle data pipeline?</strong></p><p>A1：我目前的比赛还是集中在cv，也做过语音，还有前段时候的PMP，都是DL相关的竞赛。 数据的pipeline其实是可以积累并且优化的。我觉得可以参考一些前人的代码，尤其是蛙神的code。 可以在蛙神的code基础上，慢慢优化跟积累出自己的数据pipeline。 DL数据pipeline中还有个很重要的部分就是数据增强，这块针对不同比赛可能有不同的做法。</p><p><strong>Q2：自己曾经努力拿过银牌，但是觉得金牌好难，特别是solo的情况，请问金牌和银牌的差距在哪里，如何突破？</strong></p><p>A2：我还是从我参与比较多的cv竞赛角度出发哈。首先，如果你是cv新人，在kaggle竞赛上觉得拿金牌很困难，其实是很正常的。目前cv赛基本被cv高手霸榜了。 如果你是已经比较熟悉cv各个方向的模型，那你可能需要一个竞赛好手来给你带路。毕竟竞赛还是有很多套路的。 如果是新人，我的建议是坚持，通过几个cv竞赛来积累对这个方向的认识。了解不同模型不同任务。 我觉得可以参考padue，大家如果看他竞赛的成绩的话，开始他也只是银牌水平，但是从前段时间的protein开始，他现在在cv赛的水平基本就是solo gold了。 deep learning实践的积累还是很重要，一口吃不成胖子。</p><p><strong>Q4：新出的3d object比赛是不是一种趋势，请问涛神对computer vision的发展有什么观察和展望？</strong></p><p>A4：cv的话3d绝对是一个趋势，包括学术界和工业界； sensor的成本越来越低，性能也越来越好；就人脸识别来说，用3d来说安全性和可靠性就更高了。 其实我目前也算是退坑computer vision了，也谈不上对cv有深入的认识。大家从kaggle上cv赛的数量上可以发现，cv对企业的价值还是非常高的。前景是非常好，例如工业检测之类的。</p><p><strong>Q5：怎么判断该改进网络结构还是调学习率？</strong></p><p>A5：学习率和学习策略可能是搭建baseline里面最重要的部分。这块需要在比赛的前期优化到最好，建议使用简单的网络作为baseline，然后仔细优化学习策略。没有提升空间之后再考虑别的方向的优化。</p><p><strong>Q6：是否应该从分类错误的sample中提取灵感继续改进？如果是该怎么做？</strong></p><p>A6：cv最好的一点是可以看图，非常直观。举个例子：比如之前的鲸鱼竞赛，baseline模型的bad case大多是一些姿态较大，分辨率较差的图像。那么我们就可以考虑增加对应的数据增强。效果也很显著。 再举个反面例子：刚刚结束的nips的cellsignal竞赛，是细胞的荧光成像。整个比赛我完全没有看bad case。 因为没有domain知识，图像非自然，很难观察。 但是也不妨碍比赛能拿名次，只看log来调参。</p><p><strong>Q7：请评价cv 各项任务中 state of the art 模型的实用性，有何推荐？</strong></p><p>A7：“试过才有发言权”，这是我做kaggle之后的一个经验。没做kaggle之前，我工作集中在轻量级的模型，对于sota的大模型几乎没有尝试。所以我在竞赛中会尽量去尝试各种sota，最终会有很多有意思的结论。 会发现kaiming的resnet为什么强，unet为什么就是好用。 有些很fancy的模型真的只是过拟合特定的数据集。 我也没有尝试过所有的sota，但是我觉得paper里的内容看看就好，去伪存真，实践出真知。</p><p><strong>Q8：作为一个新人从头开始拿到金牌的最佳策略？比如选择比赛的类型？</strong></p><p>A8：哈哈 因为我cv一把梭，只能给到cv的经验。如果新人想拿金牌的话，最好就是找一个蛙神all in的比赛，step by step follow蛙神！只要比所有人都肝，有足够计算资源，对齐discussion report出来的模型精度，solo gold就有希望！ 其实我第一个比赛TGS就是这么做的。</p><p><strong>Q9：在kaggle学到的东西是否有应用到别的地方？能否举例说明？</strong></p><p>A9：非常多。举个例子：模型集成（ensemble）。可能有些人说模型集成在实际工作中用不了；工作中的场景有效率的要求；在计算资源受限的情况下，3个小模型集成的效果可能远好于1个大模型的效果。 我之前的参与的人脸项目，其实就用了这样的策略，很好用。但是如何去集成，怎么增大模型间diversity，这些技巧大家可以从kaggle上学习。</p><p><strong>Q10：回头看自己的经历，对刚入坑的新人，有什么想提醒的经验和教训？</strong></p><p>A10：教训到没有，做比赛一年感触还是蛮多的，投入越多收获越大吧。希望大家坚持。 真的只有投入去做了，才会有收获。</p><p><strong>Q11：CV比赛假如遇到瓶颈会往哪些方向尝试？</strong></p><p>A11：数据层面绝对是提分收益最大的方向；还是要多看数据，多分析bad case；不看数据就调网络结构是不可取的。 数据层面有些线索之后，可以指导你对模型结构本身做一些改进。另外最重要的：多看paper，paper是idea的来源。</p><p><strong>Q12：一般会用哪种方式平时积累知识？</strong></p><p>A12：过去很长一段时间内，我积累的方式还是来自比赛 通过一个比赛，我可以验证很多paper的方法，实践在工作中无法使用的模型；帮助我深入理解一些数据上和模型上的问题 感觉从我个人而言，比赛和工作相辅相成，给我工作提供了非常好的积累和储备。</p><p><strong>Q13：想知道打比赛的节奏是什么， 比如比赛结束前一个月， 一周， 几天主要干什么？</strong></p><p>A13：基本上最后一周前，最终方案就要定了。考虑最终的集成。</p><p><strong>Q14：有复现比赛top solution的习惯吗？ 有的话是一种怎样的方式呢？</strong></p><p>A14：会看，但是很少会跑。因为一直忙着做新的比赛。其实应该仔细去研究下的。</p><p><strong>Q15：分类比赛中的最后的sub的阈值应该根据什么来选取呢，有什么选取技巧呢？</strong></p><p>A15：我只能说可靠的local validation是最重要的，所有涉及模型选择，调参；其实都需要一个依据，local validation就是这个依据。这样问题就变成如何建立可靠的local validation了。</p><p><strong>Q16：分类比赛中最后的两个sub一般会怎么样选择呢，不同的方案的模型，还是其他？</strong></p><p>A16：这个问题比较好。前期几个比赛的sub一般都是我选的，有幸抽中过金牌。我个人的建议是，差异一定要大，一个激进一个保守。 就dl比赛来说，集成最稳的是weight ave，简单有效，一般来说我会选一个这个； 然后一些存在过拟合风险的方法，但是lb和cv都很可观的方案，我也会选择一个。</p><p><strong>Q17：请问经常看到各位大佬同时参加好几个比赛，还能拿到很好的名次，这是怎么做到的？</strong></p><p>A17：其实kaggle上的top CVer都会有自己积累下来的pipeline。竞赛任务无非是这几种，迅速搭建一个可靠的baseline，对top选手很容易； 看似在做多个竞赛，可能跑的是一套代码。真的要最终比赛冲刺了，会有针对性地去理解数据和优化。</p><p><strong>Q18：图像比赛有什么通用的技巧吗？厉害的选手一次提交就可以进到绿圈，细节处理上有什么独到之处？</strong></p><p>A18：DL调参的细节太多了，需要很长时间的积累。同样的数据+网络，不同人的训练结果可能相差巨大。这是top CVer的核心竞争力 通用技巧的话，paper上带着“bag of tricks”的都需要仔细阅读 bag of tricks for image classification， bag of tricks for object detection。</p><p><strong>Q19：想问下之前说没法做bad case的时候通过log调参是怎么调的， 另外一般bad case怎么样比较好的分析？</strong></p><p>A19：其实很简单: bias-variance trade off，只看log的话，拿捏好这个。 比如nips cellsignal比赛，baseline效果是，training拟合的非常好，test却非常差。其实是一种train test consistency。从1）数据层面；2）网络层面，去分析可能的情况。1）数据层面:数据分布的问题，2）网络层面：batchnorm。针对性地去做实验，确定问题所在，继续观察bias-variance，要得出可靠结论，再进行下一步。</p><p><strong>Q20：我这边自己写了个基于 pytorch 的轮子, 每次基本上能跟上 public kernel 的步伐, 但是就是很难超越. 我估计是训练资源和调参问题. 那么: 调参大部分用已经训练好的模型来调, 还是每次改变参数都重新训练个几天, 哪种方法对 top CVer 比较实际?</strong></p><p>A20：建议解决计算资源问题，保证快速学习，训练资源很重要，其实最优的实验周期我个人感觉在半天。 半天能出一个实验结果最好，中间可以干别的。 结果出得太快也不好，要及时总结和记录实验。</p><p><strong>Q21：之前看到有新闻说模型会用贴纸识别面包机，用肤色识别罪犯的这种过拟合的情况，还有aptos存在模型通过图片尺寸leak发现lable，有没有什么好办法避免这种情况？</strong></p><p>A21：我感觉过拟合问题其实比大家想象的更严重，之前做活体检测基本就是这么个情况，难以范化。 目前的DL还比较‘蠢’，要说办法的话，加数据算不算？</p><p><strong>Q22：问一个技术性问题，碰到一些受阈值影响的metrics时，训练的时候取最好的模型应该依据val-metrics还是val-loss呢？valid的时候如果遍历阈值，可能会极大的影响效率。不同模型/不同epoch，用不同阈值取得的metrics比较，会不会‘不公平’？</strong></p><p>A22：其实我也没有很好的答案。是我的话，最优的val-metrics和val-loss模型我都会存。其实最担心的是优化的loss和metrics不一致。</p><p><strong>Q23：还想问下对warmRestart这类的循环式的scheduler有什么看法？和传统的ReduceLROnPlateau相比有什么优劣？</strong></p><p>A23：最近发现这个真的很好用。如果用step LR的话，很可能下降的位置就不够好。循环的学习策略，我的感受是既不会有太多过拟合，也不需要很仔细调参，基本会有个不错的结果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Winning-competitions-with-deep-learning-skills-–-SeuTao&quot;&gt;&lt;a href=&quot;#Winning-competitions-with-deep-learning-skills-–-SeuTao&quot; class=&quot;h
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="kaggle days" scheme="http://a-kali.github.io/tags/kaggle-days/"/>
    
      <category term="kaggle" scheme="http://a-kali.github.io/tags/kaggle/"/>
    
      <category term="图像分类" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
    
      <category term="比赛技巧" scheme="http://a-kali.github.io/tags/%E6%AF%94%E8%B5%9B%E6%8A%80%E5%B7%A7/"/>
    
      <category term="优化器" scheme="http://a-kali.github.io/tags/%E4%BC%98%E5%8C%96%E5%99%A8/"/>
    
      <category term="学习率" scheme="http://a-kali.github.io/tags/%E5%AD%A6%E4%B9%A0%E7%8E%87/"/>
    
      <category term="数据蒸馏" scheme="http://a-kali.github.io/tags/%E6%95%B0%E6%8D%AE%E8%92%B8%E9%A6%8F/"/>
    
      <category term="伪标签" scheme="http://a-kali.github.io/tags/%E4%BC%AA%E6%A0%87%E7%AD%BE/"/>
    
  </entry>
  
  <entry>
    <title>深度学习语义分割初期（FCN、UNet、SegNet）</title>
    <link href="http://a-kali.github.io/2019/10/26/FCN%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://a-kali.github.io/2019/10/26/FCN论文解读/</id>
    <published>2019-10-26T02:06:38.000Z</published>
    <updated>2019-12-17T09:02:42.998Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FCN"><a href="#FCN" class="headerlink" title="FCN"></a>FCN</h1><p>论文地址：<a href="https://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">Fully Convolutional Networks for Semantic Segmentation</a></p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>原始的 CNN 在图像的分类和定位任务中都获得了不错的成绩，但在分割任务中表现不佳。本文提出了一种<strong>全卷积网络(Fully Convolution Network, FCN)</strong>，通过进行像素级的预测(pixelwise prediction)来实现<strong>语义分割(semantic segmentaion)</strong>。</p><p><a href="https://imgchr.com/i/MUdneI" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/11/15/MUdneI.md.png" alt="MUdneI.md.png"></a></p><p>实现全卷积网络主要基于三种技术：</p><ul><li>全卷积化（Fully Convolutional）</li><li>反卷积（Deconvolution）</li><li>跃层结构（Skip Layer）</li></ul><h2 id="全卷积化"><a href="#全卷积化" class="headerlink" title="全卷积化"></a>全卷积化</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUB4hV.png" alt="MUB4hV.png"></p><p>简单来说就是把传统CNN最后的全连接层换成了卷积层。全卷积在多篇目标检测的论文中都有提到，其能提取出样本的特征图，样本目标区域对应特征图的感兴趣区域所在位置（如上图中的猫对应heatmap中的彩色像素）。</p><h2 id="上采样（Upsampling）"><a href="#上采样（Upsampling）" class="headerlink" title="上采样（Upsampling）"></a>上采样（Upsampling）</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUBJ6e.png" alt="MUBJ6e.png"></p><p>图像(图a)在经过卷积、池化等一系列处理后，得到的特征图(图b)分辨率远小于原图像。这样一来特征图中的像素无法与原图中一一对应，无法对每个像素进行预测。于是需要对特征图进行<strong>上采样</strong>以提高特征图的分辨率。文中对比了三种上采样的方法，最终选择了<strong>反卷积</strong>。</p><h3 id="Deconvolution"><a href="#Deconvolution" class="headerlink" title="Deconvolution"></a>Deconvolution</h3><p>反卷积是文章作者最终采用的方法，下面是两种反卷积的示例，图解起来十分直观：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBwkt.gif" alt="MUBwkt.gif"></p><p><img src="https://s2.ax1x.com/2019/11/15/MUBBff.gif" alt="MUBBff.gif"></p><p>下面是另一种解释，这样一看好像确实是把卷积的操作反过来了：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUB20s.png" alt="MUB20s.png"></p><h2 id="跃层结构-Skip-Layer"><a href="#跃层结构-Skip-Layer" class="headerlink" title="跃层结构(Skip Layer)"></a>跃层结构(Skip Layer)</h2><p>FCN 通过卷积和反卷积我们基本能定位到目标区域，但是，我们会发现模型前期是通过卷积、池化、非线性激活函数等作用输出了特征权重图像，我们经过反卷积等操作输出的图像实际是很粗糙的，毕竟丢了很多细节。因此我们需要找到一种方式填补丢失的细节数据，所以就有了<strong>跃层结构</strong>。</p><p>跃层结构将浅层的位置信息和深层的语义信息结合起来，得到更佳鲁棒的结果，其过程如图：</p><p><img src="https://s2.ax1x.com/2019/11/15/MUBz9K.png" alt="MUBz9K.png"></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/15/MUDAAI.png" alt="MUDAAI.png"></p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p>训练过程分为四个阶段，也体现了作者的设计思路，值得研究。</p><h3 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdWsPg.png" alt="MdWsPg.png"></p><p>使用数据集对模型的分类backbone进行预训练，使卷积层获得提取相应特征的能力。最后两层红色的是全连接层。</p><h3 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdW5IU.png" alt="MdW5IU.png"></p><p> <strong>从特征小图（16×16×4096）预测分割小图（16×16×21），之后直接升采样为大图（300×300×21）。</strong>这里输出通道数为21的原因是：采用的PASCAL数据集中有20类，算上背景类一共21类。每个通道预测一类的像素。反卷积（橙色）的步长为32，故该网络被称为<strong>FCN-32s</strong>。</p><h3 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdosiV.png" alt="MdosiV.png"></p><p>这个阶段上采样分为两次完成（橙色×2）。 在第二次升采样前，把第4个pooling层（绿色）的预测结果（蓝色）通过跃层结构融合进来，提升精确性。 第二次反卷积步长为16，这个网络称为<strong>FCN-16s</strong>。 </p><h3 id="第四阶段"><a href="#第四阶段" class="headerlink" title="第四阶段"></a>第四阶段</h3><p><img src="https://s2.ax1x.com/2019/11/15/MdTPSS.png" alt="MdTPSS.png"></p><p>这个阶段和第三阶段差不多，相较多了一次上采样。这大概是最终得出的FCN模型，因为同样的原因被称为<strong>FCN-8s</strong>。</p><p>比较这几个阶段的输出可以看出，跃层结构利用浅层信息辅助逐步升采样，有更精细的结果。 </p><p><img src="https://s2.ax1x.com/2019/11/15/MdTHkq.png" alt="MdTHkq.png"></p><h2 id="FCN-的缺点"><a href="#FCN-的缺点" class="headerlink" title="FCN 的缺点"></a>FCN 的缺点</h2><ol><li>分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节。</li><li>因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系。</li></ol><h1 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h1><p>论文地址：<a href="https://arxiv.org/abs/1505.04597" target="_blank" rel="noopener">U-Net: Convolutional Networks for Biomedical Image Segmentation</a></p><p><strong>U-Net</strong>是医学图像领域十分常用的一种分割网络，因为跟FCN十分相似，就放这里顺便讲了。</p><h2 id="网络结构-1"><a href="#网络结构-1" class="headerlink" title="网络结构"></a>网络结构</h2><p><img src="https://s2.ax1x.com/2019/11/18/MyR5wj.png" alt="MyR5wj.png"></p><p>由于整个结构图呈”U”字型，故名”U-Net”。在知道FCN的原理后，从图中可以很明显地看出U-Net的结构和FCN没太大区别。其主要区别于以下几点：</p><ul><li><p>由于Unet的主要目标数据集为医学影像（最开始是细胞图像），只需要对每个像素点进行二值分割（有病/没病），故输出的特征图只有2个channel。(output segmentation: 388×388×2)</p></li><li><p>在上采样部分依然有大量的特征通道，使得网络可以将环境信息向更高的分辨率层传播。下采样和上采样部分几乎是对称的。</p></li><li><p>输入图像尺寸(572×572)和输出图像尺寸(388×388)不一样。这点似乎是为了配合一种名为<strong>overlap-tile</strong>的方法。如下图，使用左图蓝色区域预测右图黄色区域，滑动蓝色区域重复此操作直到预测完整张图片（这种细胞图尺寸通常都很大）。最终会导致最边上的蓝色区域没法预测，对于这部分使用<strong>镜像法(mirroring)</strong>外推。</p><p>注：关于这部分我也不太确定，想要了解详细原理可以去官网看原版的实现代码。</p><p><img src="https://s2.ax1x.com/2019/11/18/MyqBmq.png" alt="MyqBmq.png"></p></li><li><p>浅层特征和深层特征合并时，Unet使用的是拼接方法（图中白色模块，估计是为了保留更多的channel），而FCN使用的是求和。</p></li><li><p>用少量图像训练便能取得不错的效果，这点对医学领域图像数据集较少的特性十分友好。</p></li></ul><h1 id="SegNet"><a href="#SegNet" class="headerlink" title="SegNet"></a>SegNet</h1><p>论文地址：<a href="https://arxiv.org/abs/1511.00561" target="_blank" rel="noopener">SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation</a></p><p>说实话这篇文章没啥意思，就概括地科普一下吧。</p><p>首先架构还是和FCN一样，没啥变化，但文中将网络前面提取特征的部分称为<strong>编码器(Encoder)</strong>，后面上采样的部分称为<strong>解码器(Decoder)</strong>。这组词被沿用至今，可能就是在这里提出来的。</p><p><img src="https://i.loli.net/2019/12/04/OTcs6yDtWQJRoup.png" alt="U61.png"></p><p>然后整篇文章的亮点在于：解码器通过使用从相应的编码器接受的<strong>max-pooling索引</strong>来进行非线性上采样。这种方法<strong>减少了所需要训练的参数量，并且改善了边界划分效果</strong>。</p><p><img src="https://i.loli.net/2019/12/04/mPTJNcjrvlVC6qM.png" alt="E5QZ.png"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]<a href="https://blog.csdn.net/fate_fjh/article/details/52882134" target="_blank" rel="noopener">卷积神经网络CNN（1）——图像卷积与反卷积（后卷积，转置卷积）</a></p><p>[2]<a href="https://blog.csdn.net/qq_31347869/article/details/89429211" target="_blank" rel="noopener">【论文笔记】FCN</a></p><p>[3]<a href="http://www.sohu.com/a/270896638_633698" target="_blank" rel="noopener">10分钟看懂全卷积神经网络（ FCN ）：语义分割深度模型先驱 </a></p><p>[4]<a href="https://blog.csdn.net/qq_36269513/article/details/80420363" target="_blank" rel="noopener">FCN的学习及理解（Fully Convolutional Networks for Semantic Segmentation）</a></p><p>[5]<a href="https://blog.csdn.net/qq_37274615/article/details/73251503" target="_blank" rel="noopener">FCN的理解</a></p><p>[6]<a href="https://blog.csdn.net/justpsss/article/details/77170004" target="_blank" rel="noopener">FCN和U-Net</a></p><p>[7]<a href="https://blog.csdn.net/natsuka/article/details/78565229" target="_blank" rel="noopener">U-net翻译</a></p><p>[8]<a href="https://blog.csdn.net/mieleizhi0522/article/details/82025509" target="_blank" rel="noopener">U-net论文解析</a></p><p>[9]<a href="http://tech.ifeng.com/c/7kx5uizAx5u" target="_blank" rel="noopener">一文带你读懂 SegNet（语义分割）</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;FCN&quot;&gt;&lt;a href=&quot;#FCN&quot; class=&quot;headerlink&quot; title=&quot;FCN&quot;&gt;&lt;/a&gt;FCN&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/1411.4038&quot; target=&quot;_blank&quot; rel
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="FCN" scheme="http://a-kali.github.io/tags/FCN/"/>
    
      <category term="CNN" scheme="http://a-kali.github.io/tags/CNN/"/>
    
      <category term="U-Net" scheme="http://a-kali.github.io/tags/U-Net/"/>
    
      <category term="反卷积" scheme="http://a-kali.github.io/tags/%E5%8F%8D%E5%8D%B7%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN 系列论文解读</title>
    <link href="http://a-kali.github.io/2019/10/10/R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    <id>http://a-kali.github.io/2019/10/10/R-CNN-Fast-R-CNN-Faster-R-CNN-Mask-R-CNN-论文解读/</id>
    <published>2019-10-10T09:21:36.000Z</published>
    <updated>2019-12-04T08:46:09.396Z</updated>
    
    <content type="html"><![CDATA[<h1 id="R-CNN"><a href="#R-CNN" class="headerlink" title="R-CNN"></a>R-CNN</h1><p>论文地址：<a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a></p><p>发布时间：2014.10.22</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>伴随着AlexNet的横空出世，卷积神经网络开始进入人们的视线，R-CNN便是将卷积神经网络运用于目标检测和语义分割的一个成功典范，其在 VOC 2012 将最佳mAP提高了30%。其成绩对卷积神经网络在目标检测的运用产生了深远的影响。</p><p>但在这之前，需要解决两个主要的问题：</p><ol><li>与图片分类不同，目标检测需要在图片上定位目标的位置。那么如何利用深度的神经网络去做目标的定位？</li><li>如何在一个小规模的数据集上训练能力强劲的网络模型？</li></ol><p>R-CNN全称为Regions with CNN features，其名字来源于其主要使用的两项技术：卷积神经网络（CNN）和<strong>区域推荐</strong>（Region Proposals），而区域推荐正是第一个问题的解决方法。当时已有许多现成的区域推荐算法，本文作者使用的是<strong>选择性搜索(selective search)算法</strong>。</p><h2 id="选择性搜索"><a href="#选择性搜索" class="headerlink" title="选择性搜索"></a>选择性搜索</h2><p><img src="https://s2.ax1x.com/2019/10/25/KwZVHS.png" alt="KwZVHS.png"></p><p>大概就是根据临近颜色的相似度将左边的原图变成像右边由色块组成的图片，然后根据色块选出候选框。这样可以减少对一些不必要的区域进行卷积运算，比如左图左上角那个框。该算法被后续几代网络沿用，直到 Faster R-CNN 使用神经网络进行区域推荐。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>R-CNN整体过程如下：</p><ol><li>给定一张输入图片，使用selective search从图片中提取 2000 个类别独立的候选区域。</li><li>将每个候选区域缩放到227×227，输入到 CNN中抽取一个固定长度的特征向量。</li><li>使用<strong>各个类别对应的SVM对特征向量进行二分类</strong>，判断该候选区域是否包含该类别，之后对每个类别的窗口进行极大值抑制。</li></ol><p><img src="https://img-blog.csdnimg.cn/20181210155342586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2JyaWJsdWU=,size_16,color_FFFFFF,t_70" alt></p><p>对于第二个问题，作者给出的解决方法是：在大型图片分类数据集ILSVRC上预训练卷积神经网络，并微调（fine-tuning）到小型目标检测数据集PASCAL上，这使得mAP上升了8个百分点。</p><p>R-CNN高效的原因：</p><ol><li>所有类别共享CNN参数</li><li>特征维度相对较小</li></ol><h1 id="SPP-Net"><a href="#SPP-Net" class="headerlink" title="SPP-Net"></a>SPP-Net</h1><p>论文地址：<a href="https://arxiv.org/abs/1406.4729" target="_blank" rel="noopener">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</a></p><p>发布时间：2015.4.23</p><h2 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h2><p>由于 CNN 需要固定大小的输入，在将图片输入到神经网络之前需要对图片进行缩放(warp)或裁剪(crop)。缩放会造成图片几何失真，而裁剪则可能损失部分目标物像素，这将会对图片识别精确度有所影响。</p><p><img src="https://s2.ax1x.com/2019/10/22/K8nKrd.png" alt="K8nKrd.png"></p><p>CNN 只能接收固定尺寸图片的原因是其全连接层节点数目固定，而其卷积层是可以接收不同尺寸的图片的。于是作者设计了用于神经网络中的 <strong>SPP</strong> (spatial pytamid pooling, 空间金字塔池化) 模块，位于卷积层和全连接层之间，用于<strong>接收任意尺寸的图片、提取其特征并产生固定大小的输出</strong>。而且实验表明，训练时使用不同尺寸的输入，可以提高测试精度。</p><h2 id="空间金字塔池化层"><a href="#空间金字塔池化层" class="headerlink" title="空间金字塔池化层"></a>空间金字塔池化层</h2><p><img src="https://s2.ax1x.com/2019/10/22/KGQ98P.png" alt="KGQ98P.png"></p><p>作者将 CNN 中的最后一个池化层用 SPP 替代。如图所示，<strong>SPP 将最后一层卷积层输出的特征图分割成不同尺寸的网格，分别为4×4、2×2、1×1，然后对每个小格进行max pooling，再将池化后的结果连接起来，就能得到（16+4+1）× 256 的固定长度的输出</strong>（这里的256为256个channel）。</p><h2 id="SPP-在目标检测中的应用"><a href="#SPP-在目标检测中的应用" class="headerlink" title="SPP 在目标检测中的应用"></a>SPP 在目标检测中的应用</h2><p>前面提到，R-CNN 在图像中选出2000个候选窗口，并将每个窗口缩放后输入到神经网络中，这样对一张图片反复使用深度卷积网络十分耗时。测试时，特征提取是其主要的时间瓶颈。</p><p>论文中提到，特征图的ROI与原图中的目标物的位置存在一定的映射关系，如下图：</p><p><img src="https://s2.ax1x.com/2019/10/22/KG6IIA.png" alt="KG6IIA.png"></p><p>于是<strong>对于一张图片，只需要提取一次特征，然后将特征图的2000个候选区域输入 SPP 模块就能得到固定长度的表示。由于只需要进行一次卷积操作，节省了大量候选区域通过神经网络的时间。</strong></p><h1 id="Fast-R-CNN"><a href="#Fast-R-CNN" class="headerlink" title="Fast R-CNN"></a>Fast R-CNN</h1><p>论文地址：<a href="https://arxiv.org/abs/1504.08083" target="_blank" rel="noopener">Fast R-CNN</a></p><p>发布时间：2015.9.27</p><h2 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h2><p>SPPnet 虽然对R-CNN进行了一些改进，但仍然存在许多问题：</p><ul><li>需要大量产生候选框</li><li>对目标的定位只能靠候选框来粗略定位</li><li>多阶段pipeline，特征提取、模型训练、SVM分类器训练、边框回归要分别进行</li><li>特征图要存在本地磁盘，影响速度</li></ul><p>于是 Fast R-CNN 改进了在目标检测任务中的性能，其优势如下：</p><ul><li>相比 R-CNN、SPPnet 有着更高的 mAP</li><li>单阶段(single-stage)训练，使用多任务损失(multi-task loss)</li><li>训练可以更新网络每一层的参数</li><li>无需使用磁盘缓存特征</li></ul><h2 id="架构细节和模型训练"><a href="#架构细节和模型训练" class="headerlink" title="架构细节和模型训练"></a>架构细节和模型训练</h2><p><img src="https://s2.ax1x.com/2019/10/23/KtoCi6.png" alt="KtoCi6.png"></p><p>从上图直观上来看，Fast R-CNN 与 SPPnet 的结构有两个区别：</p><ol><li>SPP模块被换成了RoI池化层</li><li>网络末端有两个输出，分别用于图像分类和边框回归。分类器被换成了softmax。使用softmax的好处在于不用单独训练一个SVM分类器；缺点在于对于一个候选框最多只能分出一类物体，即使一个候选框包含了多个类别的目标（大概）。</li></ol><p>另外值得一提的是，Fast R-CNN 采用的是固定大小的输入，而不像SPPnet使用任意大小的输入。</p><h3 id="RoI-池化层"><a href="#RoI-池化层" class="headerlink" title="RoI 池化层"></a>RoI 池化层</h3><p>RoI 池化层实质上就是单层的 SPP 模块。其将一个候选窗口划分为 H×W 的网格，对每个网格内进行最大池化，最后输出一个长度为 H×W 的特征。超参数 H 和 W 视具体网络结构而定。</p><h3 id="多任务损失"><a href="#多任务损失" class="headerlink" title="多任务损失"></a>多任务损失</h3><p>多任务损失由分类任务损失和边框回归任务损失线性组合而成：<br>$$<br>L=L_{cls}(p,u)+\lambda [u\geq 1]L_{loc}(t^u,v)\<br>$$<br>其中：<br>$$<br>L_{cls}(p,u)=-\log p_u\<br>L_{loc}(t^u,v)=\sum smooth_{L_1}(t^u_i-v_i)<br>$$</p><h3 id="Mini-batch-sampling"><a href="#Mini-batch-sampling" class="headerlink" title="Mini-batch sampling"></a>Mini-batch sampling</h3><p>（其实这一段我没看太懂，以下仅作参考）</p><p>在调优(fine tuning)训练时，每个mini-batch中首先加入 N 张完整图片，从 N 张图片中选出一共 R 个 IoU&gt;0.5 的候选区域，然后将这 R 个候选区域作为训练样本放入网络训练。</p><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>论文地址：<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><p>发布时间：2016.1.6</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>性能优越的目标检测网络都依赖区域推荐(region proposal)算法来假定目标位置，比如R-CNN中的选择搜索(search selective)算法，而这些区域推荐的计算消耗正是整个网络性能的瓶颈。本文作者引入了<strong>区域推荐网络(Region Proposal Network, RPN)</strong>，尝试使用神经网络来进行区域提取。并将 RPN 和 Fast R-CNN 融合在一起，共享卷积特征，成为一个端到端的神经网络。</p><h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><p><img src="https://s2.ax1x.com/2019/10/25/KdqldS.png" alt="KdqldS.png"></p><p>Fast R-CNN 大致结构如图。可以看出，网络由四步组成：</p><ol><li>输入的图片经过卷积层输出一张特征图</li><li>将特征图输入 RPN，得到候选区域</li><li>将特征图上候选区域的对应位置输入到 RoI 池化层</li><li>输入到分类器得出分类结果</li></ol><p>那么 RPN 具体是怎样的呢？</p><h2 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h2><p><img src="https://s2.ax1x.com/2019/10/25/KdIwZQ.png" alt="KdIwZQ.png"></p><p>从上图中可以看到Faster R-CNN更具体的结构，包括左下方的RPN模块。RPN具体流程如下：</p><ol><li><p>使用<strong>滑动窗口(slide window)</strong>遍历整个特征图(feature map)，遍历过程中以每个window中心产生9个预设<strong>锚框(anchor)</strong>，9个锚框分别对应3种尺寸和3种长宽比。</p><p><img src="https://s2.ax1x.com/2019/10/25/KwJZQS.png" alt="KwJZQS.png"></p></li><li><p>将锚框分别输入到<strong>线性分类层(cls layer)</strong>和<strong>边框回归层(reg layer)</strong>中。分类层通过softmax对锚框进行二分类，初步判断该锚框是前景还是背景（锚框里是否包含目标物）；回归层通过边框回归进一步修正锚框，使锚框定位更精确。</p><p><img src="https://s2.ax1x.com/2019/10/26/K0RaZ9.png" alt="K0RaZ9.png"></p></li><li><p>将筛选、修正后的锚框映射到特征图上，输入到ROI池化层。后续操作和Fast R-CNN一样。</p></li></ol><h2 id="Faster-R-CNN-的训练步骤"><a href="#Faster-R-CNN-的训练步骤" class="headerlink" title="Faster R-CNN 的训练步骤"></a>Faster R-CNN 的训练步骤</h2><ol><li>训练一个用于分类的 CNN（用于特征提取）</li><li>使用 CNN 的特征图作为输出，端到端的fine-tune RPN + CNN。IoU&gt;0.7的作为正样本，IoU&lt;0.3的为负样本。</li><li>固定RPN的权值，训练整个网络。</li><li>固定其余部分的权值，训练RPN</li><li>固定CNN、RPN，训练其余部分</li><li>重复步骤4、5直到满意为止</li></ol><p>由于Faster R-CNN的训练步骤过于繁杂，促使了后续的SSD网络对其进行改进。</p><h2 id="R-CNN-家族的总结"><a href="#R-CNN-家族的总结" class="headerlink" title="R-CNN 家族的总结"></a>R-CNN 家族的总结</h2><p><img src="https://i.loli.net/2019/12/04/4kIh9YJQwTuZvEX.png" alt="57KD5.png"></p><p>图源：贪心学院</p><h1 id="Mask-R-CNN"><a href="#Mask-R-CNN" class="headerlink" title="Mask R-CNN"></a>Mask R-CNN</h1><p>（待更新）</p><h1 id="相关面试题"><a href="#相关面试题" class="headerlink" title="相关面试题"></a>相关面试题</h1><p><strong>Q：讲下faster-rcnn？Faster-rcnn里面的NMS的算法原理是什么？</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;R-CNN&quot;&gt;&lt;a href=&quot;#R-CNN&quot; class=&quot;headerlink&quot; title=&quot;R-CNN&quot;&gt;&lt;/a&gt;R-CNN&lt;/h1&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1311.2524.pdf&quot; target=
      
    
    </summary>
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="论文解读" scheme="http://a-kali.github.io/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/"/>
    
      <category term="深度学习" scheme="http://a-kali.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="R-CNN" scheme="http://a-kali.github.io/tags/R-CNN/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>计算机视觉算法岗面试归纳（持续解答ing）</title>
    <link href="http://a-kali.github.io/2019/10/05/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B2%97%E9%9D%A2%E8%AF%95%E5%BD%92%E7%BA%B3/"/>
    <id>http://a-kali.github.io/2019/10/05/计算机视觉算法岗面试归纳/</id>
    <published>2019-10-05T01:32:59.000Z</published>
    <updated>2019-12-12T13:57:25.353Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ML"><a href="#ML" class="headerlink" title="ML"></a>ML</h1><ul><li>介绍一下调参的经验</li><li>Softmax的公式和伪代码</li><li>分类常见的指标有什么，如何理解AUC？</li><li>介绍决策树、RF、XGBoost、GBDT和 LightGBM</li><li>XGboost的loss函数的推导（mse以及非mse形式），以及求解推导。</li><li>使用O(N)复杂度完成GBDT分裂</li><li>介绍 F1-score，AUC，交叉熵，ROC</li><li>介绍 Adboost，GBDT，XGBoost</li><li>介绍不同的聚类算法：K-Means、GMM、DBSCAN等</li><li>CCA和PCA的区别</li><li>牛顿法能用于非凸函数吗？</li><li>XGBoost里处理缺失值的方法</li><li>样本不平衡对 SVM 的影响</li><li>KNN和Kmeans的算法中K的含义，K对算法的影响，怎么选择K</li><li>LR的全过程，从train到inference，损失函数</li><li>介绍常见的集成方法</li><li>LR + softmax做多分类和LR + multiLoss 做多分类区别在哪里</li><li>LR为什么用交叉熵作为loss函数</li><li>Kmeans的缺点？如何改善？</li><li><a href="https://a-kali.github.io/2019/12/02/k-means-聚类算法/">讲一下K-means算法的过程以及原理</a></li><li>为什么Bagging降方差，Boosting降偏差？</li><li>介绍XGBoost对GBDT的提升，LightGBM对XGBoost的提升</li><li>为什么要对连续型数值进行离散化，这样做有什么优势</li><li>LR 为什么用sigmoid函数？</li><li>怎么解决样本不均衡（重点考核损失函数优化）</li><li>HMM 和 CRF的区别</li><li>XGBoost 如何处理缺失数据？</li><li>写一下 LR 和 SVM 的损失函数</li><li>正负样本不均衡时的解决方案</li><li>知道哪些降维的方法，具体讲讲</li><li>线性模型和非线性模型都有哪些？</li><li>手写AUC的计算（小矩形积分得到总面积即可）</li><li>决策树分支的原理</li><li>offerpolicy 和 onpolicy 的区别</li><li>为什么随机森林的树比 GBDT 的深一点？</li><li>逻辑回归的目标函数(损失函数)是凸函数吗？</li><li>完全二叉树的概念</li><li>朴素贝叶斯与贝叶斯有什么区别？</li><li>SVM 为什么变成对偶问题来求解？</li><li>缺失值如何处理，什么情况下均值、众数，什么情况下丢弃特征。</li><li>诸如ID类的特征如何处理，编码方式one-hot还是其他的，高维时？什么样才算高维，有没有界定？</li><li>聚类的算法有哪些？评价方法？优化算法？</li><li>解释几何间隔和函数间隔</li><li>描述决策树，如何选特征，怎么划分，怎么剪枝，介绍信息增益</li><li>K-Means 聚类这种方法一定会收敛嘛？如果不收敛，怎么办？</li><li>SVM 的目标函数，为什么能用拉格朗日乘子法讲原始最优化问题转化为极大极小问题，数学原理是什么</li><li>介绍SVM，其中的软间隔是什么意思？</li><li>使用线性回归的时候什么时候会需要用L2？</li><li>如果F1已经趋于平稳，如何在保持F1稳定的前提下提高precision，降低recall；</li><li>LR 为什么不用 MSE，SVM 为什么用hinge不用logloss</li><li>XGBoost 怎么解决过拟合？怎么剪枝？怎么选择特征？怎么处理缺失值？</li><li>XGBoost 的默认深度</li><li>各种决策树模型的优劣（从最简单的ID3到最后的LGB）</li><li>SVM 核函数哪些是高维空间维度已知，哪些是未知的？</li><li>LR介绍、LR对特征需要做什么特殊处理吗？类别特征、连续特征</li><li>损失函数正则项的本质是什么? </li><li>SVM 有哪些核函数？</li><li>L1 正则化为什么能使特征稀疏？</li><li>Stacking原理，还有怎么调优？</li><li>XGBoost怎么调参？用了多少棵树？</li><li>各种决策树模型的优劣（从最简单的ID3到最后的LGB）</li><li>ID3 C4.5 CART的区别</li><li>手推 SVM, GBDT, XGBoost</li><li>CRF 怎么训练的（传统+深度学习）</li><li>得到AUC的两种计算方法</li><li>树的分裂方式（id3,gini,gdbt,xgboost）</li><li>监督学习的概念？什么是随机森林，随机森林的优点？</li><li>LR和SVM区别（计算复杂度）</li><li>Adam优化器的迭代公式</li><li>SGD每步做什么，为什么能online learning</li><li>L1 L2正则化区别</li><li>PCA原理和执行步骤</li><li>特征工程知道吗？举几个特征归一化的例子</li><li>SVM为什么可以处理非线性问题</li><li>L1正则化的先验分布？</li><li>L1的不知道，L2的先验分布知道吧？</li><li>多标签分类问题怎么解决，从损失函数角度考虑</li></ul><h1 id="NN"><a href="#NN" class="headerlink" title="NN"></a>NN</h1><ul><li><a href="https://a-kali.github.io/2019/12/01/激活函数汇总/">激活函数除了Sigmoid tanh ReLU 还有什么介绍一下</a></li><li>BFE 和 Dropout的关系</li><li>Dropout是失活神经元还是失活连接</li><li>手推梯度反向传播</li><li>分类网络样本不均衡怎么办？</li><li>dropout层作用，如何实现有什么作用？</li><li>Dropout 前向和反向的处理</li><li>神经网络如果没有激活函数还能解决线性不可分问题吗？</li><li>Tensorflow的动态图和静态图有什么区别</li><li>GN，BN，LN，IN 它们的共性和特性</li><li>为什么BN有泛化能力的改善. 什么场景用什么normalization方法，效果如何.</li><li>Dropout为什么能防止过拟合？具体实现</li><li>dropout在训练和测试时不同，怎么保证测试结果稳定</li><li>如何计算神经网络的 FLOPS？</li><li>梯度下降陷入局部最优有什么解决办法</li></ul><h1 id="图像处理"><a href="#图像处理" class="headerlink" title="图像处理"></a>图像处理</h1><ul><li>手写灰度直方图代码</li><li>介绍一下开运算和闭运算</li><li>介绍双目相机识别目标深度的原理</li><li>单目视觉如何测量深度？</li><li>介绍常见的边缘检测算法</li><li>SIFT 特征是如何保持旋转不变性的？</li><li>如何快速判断图中有环？</li><li>介绍常见的边缘检测算子</li><li>Hough 变换原理（直线和圆检测）</li><li>为什么 Sobel 算子中间是2，两边是1</li><li>算法题：实现 OpenCV中的图像缩放，包括实现双线性插值</li><li>输入图像灰度值对模型的影响，为什么要把0-255转化成0-1？</li><li>介绍 RANSAC</li><li>介绍一阶二阶边缘检测算子一阶二阶边缘检测算子</li><li>OpenCV里面findcontour函数的原理是什么？</li><li>相机里面的标定参数有哪些？是怎么计算这些参数的？</li><li>如何求边缘，45°边缘，高斯滤波和双边滤波</li><li>代码题：手撕实现图像的resize和rotate90度</li><li>手写中值滤波</li><li>介绍一下高斯滤波，均值滤波，中值滤波</li><li>SIFT特征提取怎么做的，具备什么性质，为什么</li><li>讲一下CTC的原理</li><li>夜间拍照的多图对齐和融合</li></ul><h1 id="CV"><a href="#CV" class="headerlink" title="CV"></a>CV</h1><ul><li>介绍你读到的19年 Anchor-free 目标检测论文</li><li>简单介绍Fast RCNN -&gt; Faster RCNN -&gt; mask RCNN (这个真的好高频)</li><li>256×256×3 -&gt; 128×128×64的卷积，stride，padding和待优化的参数有多少</li><li>手撕 SoftNMS代码</li><li>CNN反向传播公式推导；参数共享指的是？</li><li>介绍熟悉的NAS网络</li><li>介绍目标检测中的多尺度训练/测试？</li><li><a href="https://a-kali.github.io/2019/09/01/ResNet-CVPR-2016/">为什么 DenseNet 比 ResNet 更耗显存？</a></li><li>为什么深度学习中的图像分割要先编码再解码？</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">1×1 卷积有什么作用？</a></li><li>如何计算语义分割的 mIoU（写伪代码）</li><li>原始图片中的 RoI 如何映射到 feature map ?</li><li>PyTorch的高效convolution实现</li><li>PyTorch 不用库函数如何实现多机多卡</li><li>哪些情况用 MaxPool比AveragePool效果好？原因</li><li>介绍Anchor based 和Anchor free目标检测网络的优缺点</li><li>YOLOv3在小缺陷检测上也很好，RPN上和two-stage的有什么区别</li><li>MobileNetV2 module的参数量和FLOPs计算</li><li>CNN 的感受野受什么影响</li><li>CNN 如何保持平移方向不变性</li><li>如果分类的数据图像每一类只有几张，你会用什么方法？</li><li>RPN怎么计算 box 的实际坐标</li><li>介绍常见的 Anchor free 目标检测算法</li><li>算法题：编程实现目标检测中的 IoU 计算</li><li>公式及讲解soft attention，hard attention，multi head attention</li><li>卷积操作是线性的吗？CNN是线性的吗？为什么？（激活函数）常用的激活函数？</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">3×3 卷积核 与 5×5 卷积核相比的优点</a></li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">InceptionV1~V4系列介绍，以及每一版的改进，优缺点介绍</a></li><li>CNN Maxpooling 怎么反向传播？</li><li>写出 YOLOv3 的损失函数</li><li>YOLOV1~V3系列介绍，以及每一版的改进，优缺点介绍</li><li>介绍金字塔池化，ASPP，深度可分，带孔卷积</li><li>VGG网络什么特点，用到了哪几种卷积核？</li><li>介绍 anchor-based和anchor-free两者的优缺点</li><li>PyTorch 多gpu训练机制的原理，优化器以及网络参数保存机制</li><li>讲下faster-rcnn？Faster-rcnn里面的NMS的算法原理是什么？</li><li>Mask R-CNN 如何提高mask的分辨率？</li><li>普通卷积、DW+PW卷积计算量推导</li><li>MobileNet V2中的Residual结构最先是哪个网络提出来的</li><li>CornerNet介绍，CornerPooling是怎么做的，怎么解决cornernet检测物体合并为一个框的问题</li><li><a href="https://a-kali.github.io/2019/09/04/Inception-v1-v4/">GoogLeNet中为什么采用小的卷积核？</a></li><li><a href="https://a-kali.github.io/2019/10/26/FCN论文解读/">说一下UNet的结构</a></li><li>熟悉deeplab吗，aspp是怎样的，与其他的state-of-art的模型对比，deeplab还可以做哪些改进？</li><li>retinanet的focal loss是解决的什么问题</li><li>CRF后处理的目的</li><li>介绍deeplabv3，画出backbone（串联和并联），论文中认为这两种哪种方式更好？如何避免friding efect、deeplabv3的损失函数</li></ul><h1 id="SLAM"><a href="#SLAM" class="headerlink" title="SLAM"></a>SLAM</h1><ul><li>PnP求解最少需要几个点？</li><li>ORBSLAM的哪个部分最耗时？</li><li>ORBSLAM怎么克服尺度漂移问题？</li><li>回环原理讲一下，要估计哪些量？</li><li>后端BA中，如何存在outlier一般怎么解决？</li><li>BA中，海塞矩阵的求逆有哪些可以加速的方法？</li><li>单应矩阵(homography)为什么只有8个自由度？</li><li>如何设计一个视觉+IMU+RTK+Lidar的定位系统？</li><li>对于光照明暗变化、动态场景，视觉SLAM如何去解决？</li><li>ROS中，node属于多进程，如何把两个node放在一个进程中？</li><li>ORBSLAM 后端H矩阵求解的算法复杂度是多少？如何去加速后端求解？</li><li>ORB-SLAM的初始化步骤</li><li>介绍 Bundle Adjustment</li><li>机器人学中表示旋转的方式有哪些？区别是什么？</li><li>检测圆的方法有哪些？</li><li>霍夫圆变换的原理是什么？</li><li>你知道哪些点云匹配的算法？原理是什么？</li><li>ROS里面的一些基本操作怎么实现？</li><li>怎么估计3D姿态？用什么表示姿态？</li><li>相机标定方法与流程，内外参矩阵求解</li><li>什么是闭环检测？常用的方法有哪些？你用的哪种方法？有没有创新？</li><li>解释一下Gauss-Netwon和LM算法。</li><li>熟悉Ceres优化库吗？说一下。</li><li>描述（扩展）卡尔曼滤波与粒子滤波，你自己在用卡尔曼滤波时遇到什么问题没有？</li><li>除了视觉传感，还用过其他传感吗？比如GPS，激光雷达。。。</li></ul><h1 id="反向面试"><a href="#反向面试" class="headerlink" title="反向面试"></a>反向面试</h1><p>再也不用担心面试官灵魂拷问：你有什么要问我的么？</p><p>下面列表里的问题对于参加技术面试的人来说很有用：</p><ul><li>我的日常工作是什么？</li><li>入职培训会是什么样的？</li><li>你们怎么使用源码控制系统？</li><li>团队内/团队间的交流通常是怎样的？</li><li>有标准的开发环境吗？是强制的吗？</li><li>我可以为开源项目做贡献吗？是否需要审批？</li><li>团队里面初级和高级工程师的比例是多少？</li><li>晋升流程是怎样的？要求/预期是怎样沟通的？</li><li>我入职的岗位是新增还是接替之前离职的同事？</li><li>入职之后在哪个项目组，项目是新成立还是已有的？b公司是否有技术分享交流活动？有的话，多久一次呢？</li><li>更多提问可以在 <a href="https://github.com/yifeikong/reverse-interview-zh" target="_blank" rel="noopener">https://github.com/yifeikong/reverse-interview-zh</a> 找到</li></ul><p>出了以上几种类型的题目，还常见编程算法题、C++语言细节、Python语言细节、英语题、数学题、项目、计算机网络和操作系统</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ML&quot;&gt;&lt;a href=&quot;#ML&quot; class=&quot;headerlink&quot; title=&quot;ML&quot;&gt;&lt;/a&gt;ML&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;介绍一下调参的经验&lt;/li&gt;
&lt;li&gt;Softmax的公式和伪代码&lt;/li&gt;
&lt;li&gt;分类常见的指标有什么，如何理解AUC？&lt;/
      
    
    </summary>
    
    
      <category term="计算机视觉" scheme="http://a-kali.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
      <category term="语义分割" scheme="http://a-kali.github.io/tags/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/"/>
    
      <category term="神经网络" scheme="http://a-kali.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="目标检测" scheme="http://a-kali.github.io/tags/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    
      <category term="机器学习" scheme="http://a-kali.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="面试" scheme="http://a-kali.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
      <category term="SLAM" scheme="http://a-kali.github.io/tags/SLAM/"/>
    
      <category term="图像处理" scheme="http://a-kali.github.io/tags/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
</feed>
